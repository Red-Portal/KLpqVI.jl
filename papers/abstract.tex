
Markovian score climbing (MSC) is a recently proposed variational inference (VI) method based on Markov-chain Monte Carlo (MCMC) for minimizing the inclusive Kullback-Leibler (KL) divergence.
This paper shows that independent Metropolis-Hastings (IMH) type kernels can automatically trade off bias and variance when used in MSC.
In addition, we also show that the variance of the conditional importance sampling kernel, which was originally proposed for MSC, may increase with an additional computational budget.
To fix this, we propose to use parallel IMH (PIMH) chains for obtaining stochastic gradients.
We find that MSC with PIMH achieves better performance compared to inclusive as well as exclusive VI methods.
%Our results motivate the use of the inclusive KL divergence for VI.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
