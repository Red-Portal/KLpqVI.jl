
Markovian score climbing (MSC) is a recently proposed variational inference (VI) method based on Markov-chain Monte Carlo (MCMC) for minimizing the inclusive Kullback-Leibler (KL) divergence.
In this paper, we show that, when combined with independent Metropolis-Hastings (IMH) type of kernels, MSC has interesting properties.
Specifically, IMH kernels can automatically trade off bias and variance when used in MSC.
Also, we show that the variance of the conditional importance sampling kernel, which was originally proposed for MSC, can increase with an additional computational budget.
To fix this, we propose to use parallel IMH (PIMH) chains for obtaining stochastic gradients.
We find that MSC with PIMH achieves better performance faster compared to other inclusive and exclusive VI methods.
%Our results motivate the use of the inclusive KL divergence for VI.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
