
\vspace{-1ex}
\section{Evaluations}\label{section:eval}
\vspace{-1.5ex}
\subsection{Experimental Setup}
\vspace{-1.ex}
%\vspace{-0.05in}
\paragraph{Implementation}
For the realistic experiments, we implemented MCSA methods on top of the Turing~\citep{ge2018t} probabilistic programming framework.%\footnote{Available at \url{https://github.com/Red-Portal/KLpqVI.jl}}.
For the variational family, we use diagonal multivariate Gaussians with the support transformation of~\citet{JMLR:v18:16-107}.
We use the ADAM optimizer by~\citet{kingma_adam_2015} with a stepsize of 0.01 in all experiments.
The budget is set to \(N=10\) for all experiments unless specified.

\begin{figure*}[t]
  \vspace{-2ex}
  \centering
%%   \includegraphics[scale=1.0]{figures/gaussian_02.pdf}
  \includegraphics[scale=1.0]{figures/gaussian_03.pdf}
  \vspace{-0.5ex}
  \caption{\textbf{Gradient variance versus iteration and computational budget (\(N\)).
      pMCSA not only achieves the least gradient variance, but its variance also scales better with \(N\).
    }
    The colors range from light (\(N=2^3\)) to dark (\(N=2^7\)) representing the computational budgets \(N \in [2^3, 2^4, 2^5, 2^6, 2^7]\).
    The target distribution is a 50-D multivariate Gaussian with \(\nu = 500\).
    The error bands are the 80\% quantiles obtained from 8 replications.
  }\label{fig:gaussian}
  \vspace{-1ex}
\end{figure*}

\begin{figure*}[t]
  \vspace{-1ex}
  \centering
  \includegraphics[scale=1.0]{figures/stepsize_01.pdf}
  %\vspace{-0.05in}
  \caption{\textbf{Optimizer stepsize (\(\gamma\)) versus final KL.
      pMCSA is the least sensitive to optimizer hyperparameters and results in stable convergence.}
      The final KL is obtained at the \(10^4\)th iteration.
      The target distribution is a 100-D Gaussian with \(\nu = 500\).
      The error bands are the 80\% quantiles, while the solid lines are the median of 20 replications.
  }\label{fig:stepsize}
  \vspace{-2ex}
\end{figure*}

\input{bnn_table.tex}

\vspace{-1.ex}
\paragraph{Baselines}
We compare
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item \textbf{pMCSA} (ours,~\cref{section:pmcsa}),
  \item \textbf{JSA}~\citep{pmlr-v124-ou20a},
  \item \textbf{MSC}~\citep{NEURIPS2020_b2070693},
  \item MSC with with Rao-Blackwellization (\textbf{MSC-RB},~\citealt{NEURIPS2020_b2070693}), and
  %\item adaptive importance sampling (IS) with the self-normalized IS estimator (\textbf{SNIS},~\citealt{robert_monte_2004}), and
  \item evidence lower-bound maximization (\textbf{ELBO},~\citealt{pmlr-v33-ranganath14, JMLR:v18:16-107}) with the path derivative estimator~\citep{NIPS2017_e91068ff}.
\end{enumerate*}


\vspace{-1ex}
\subsection{Simulations}\label{section:simulation}
\vspace{-1ex}
\paragraph{Setup}
First, we verify our theoretical analysis on multivariate Gaussians with full-rank covariances sampled from Wishart distribution with \(\nu\) degrees of freedom (values of \(\nu\) are in the figure captions).
This problem is challenging since an IMH (used by pMCSA, JSA) or CIS (used by MSC, MSC-RB) kernel with a diagonal Gaussian proposal will mix slowly due to a large \(w^*\).
% Since the exact KL divergence between the target and the varitional approximation is available, accurate evaluation is possible.


\vspace{-1ex}
\paragraph{Gradient Variance}
We evaluate our theoretical analysis of the gradient variance.
The variance is estimated from \(512\) independent Markov chains using the parameters generated by the main MCSA procedure.
The estimated variances are shown in \cref{fig:gaussian}.
We make the following observations:
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item pMCSA has the lowest variance overall, and it consistently benefits from increasing \(N\).
  \item MSC does not benefit from increasing \(N\) whatsoever.
    
  \item \textcolor{blue}{
    MSC-RB does not benefit much from increasing \(N\) until the divergence of \(q\left(\cdot; \vlambda\right)\) has become small.
    }
  \item JSA does not benefit from increasing \(N\) until \(q\left(\cdot; \vlambda\right)\) has sufficiently converged (when \(w^*\) has become small).
\end{enumerate*}
These results confirm our theoretical results in~\cref{section:comparison,section:pmcsa}.


\vspace{-1.5ex}
\paragraph{Robustness Against Optimizers}
Since the convergence of most sophisticated SGD optimizers has yet to be established for MCGD, we empirically investigate their effectiveness.
The results using SGD~\citep{robbins_stochastic_1951, bottou_optimization_2018a}, Momentum~\citep{polyak_methods_1964}, Nesterov~\citep{nesterov_method_1983}, ADAM~\citep{kingma_adam_2015}, and varying stepsizes are shown in~\cref{fig:stepsize}.
Clearly, pMCSA successfully converges for the broadest variety of optimizer settings.
Overall, most MCSA methods seem to be the most stable with ADAM, which points out that establishing the convergence of ADAM for MCGD will be a promising direction for future works.

%% \subsection{Hierarchical Logistic Regression}\label{section:logistic}
%% \vspace{-0.05in}
%% \paragraph{Experimental Setup}
%% We now perform logistic regression with the \texttt{Pima Indians} diabetes (\(\vz \in \mathbb{R}^{11}\),~\citealt{smith_using_1988}), \texttt{German credit} (\(\vz \in \mathbb{R}^{27}\)), and \texttt{heart disease} (\(\vz \in \mathbb{R}^{16}\),~\citealt{detrano_international_1989}) datasets obtained from the UCI repository~\citep{Dua:2019}.
%% 10\% of the data points were randomly selected in each of the 100 repetitions as test data.

% \input{gp_table.tex}

%% %

%%   \vspace{-0.1in}
%% \paragraph{Inclusive KL v.s. Exclusive KL}
%% While both ELBO and par.-IMH showed similar numerical performance, they chose different optimization paths in the parameter space.
%% This is shown in~\cref{fig:logistic}.
%% While the test accuracy suggests that ELBO converges quickly around \(t=2000\) (\cref{fig:german_acc}), in terms of uncertainty estimate, it takes much longer to converge (\cref{fig:german_lpd}).
%% This shows that inclusive KL minimization chooses a path that has better density coverage as expected.

\input{pgp_table.tex}

  \vspace{-0.1in}
\subsection{Bayesian Neural Network Regression}\label{section:bnn}
  \vspace{-0.07in}
\looseness=-1
\paragraph{Setup}
For realistic experiments, we train Bayesian neural networks (BNN, \citealt{neal_bayesian_1996}) for regression.
We use datasets from the UCI repository~\citep{Dua:2019} with 90\% random train-test splits and run for \(T=5\cdot10^4\) iterations.
We use the model and forward propagation method of~\citet{pmlr-v37-hernandez-lobatoc15} with a \(50\)-unit hidden layer (see \cref{section:model_bnn}).
% \looseness=-1

\vspace{-1.5ex}
\paragraph{Results}
The results are shown in~\cref{table:bnn}.
pMCSA achieves the best performance compared to all other MCSA methods.
Also, its overall performance is comparable to exclusive KL minimization methods (ELBO) unlike other MCSA methods.
Furthermore, on \textsf{airfoil} and \textsf{energy}, pMCSA improves over ELBO by 0.29 \texttt{nat} and 0.48 \texttt{nat}.
%
\begin{wrapfigure}[12]{r}{0.5\textwidth}
  \vspace{-4.ex}
  \centering
  \includegraphics[scale=0.9]{figures/pruning_01.pdf}
  \vspace{-2ex}
  \caption{\textbf{
      Distribution of the variational posterior mean of the BNN weights.
      pMCSA results in much less pruning.
    }
    The density was estimated with a Gaussian kernel and the bandwidth was selected with Silverman's rule.
  }\label{fig:pruning}
\end{wrapfigure}
%
Even on \textsf{gas} where pMCSA did not beat ELBO, its performance is comparable, and it dominates all other MCSA methods by roughly 0.4 \texttt{nat}.
Additional experimental results, \textcolor{blue}{including plots with respect to the wallclock time}, can be found in~\cref{section:bnn_additional}

\vspace{-5ex}
\textcolor{blue}{
\paragraph{Weight Pruning}
When using VI, BNNs tend to underfit data.
\citet{mackay_local_2001,pmlr-v70-hoffman17a,trippe_overpruning_2017} associate this with ``weight pruning,'' that is, the variational posterior converges to the zero-mean prior.
Furthermore,~\citet{coker_wide_2022,huix_variational_2022} have shown that this is guarenteed to happen under certain conditions.
However, these results are exclusively based on exclusive KL minimization.
\cref{fig:pruning} shows that MCSA does not suffer from weight pruning, which suggest that pruning is an artifact of using the exclusive KL.
Additional plots are shown in \cref{fig:pruning_additional} (\cref{section:bnn_additional}).
}

\vspace{-1.5ex}
\subsection{Robust Gaussian Process Regression}\label{section:bgp}
\vspace{-1.5ex}
\paragraph{Setup}
We train Gaussian processes (GP) with a Student-T likelihood for robust regression.
We use datasets from the UCI repository~\citep{Dua:2019} with 90\% random train-test splits.
We use the Mat\'ern 5/2 covariance kernel with automatic relevance determination~\citep{neal_bayesian_1996} (see \cref{section:model_rgp}).
We run all methods with \(T=2\cdot10^4\) iterations.
For prediction, we use the mode of \(q\left(\cdot; \vlambda\right)\) for the hyperparameters and marginalize the latent function over \(q\left(\cdot; \vlambda\right)\)~\citep{rasmussen_gaussian_2006}.
We consider ELBO with only \(N=1\) since differentiating through the likelihood makes its per-iteration cost comparable to MCSA methods with \(N=10\).

\vspace{-1.8ex}
\paragraph{Results}
The results are shown in~\cref{table:gp}.
Except for \textsf{gas}, pMCSA achieves better performance than all other methods.
This suggests that, overall, the exclusive KL may be less effective for GP posteriors.
Although ELBO achieves the best performance on \textsf{gas}, pMCSA dominates other MCSA methods.
Our encouraging regression results suggest that incorporating methods such as inducing points~\citep{NIPS2005_4491777b} into MCSA may lead to an important new class of GP models.
Additional experimental results, \textcolor{blue}{including plots with respect to the wallclock time}, can be found in~\cref{section:gp_additional}.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
