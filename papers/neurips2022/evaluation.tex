
%% \begin{figure*}
%%   \centering
%%   \subfloat[Optimizer Sensitivity]{
%%     \includegraphics[scale=1.0]{figures/stepsize_01.pdf}
%%   } \\
%%   \subfloat[Gradient Variance]{
%%     \includegraphics[scale=1.0]{figures/gaussian_01.pdf}
%%   }
%%   \caption{\textbf{Optimizer stepsize (\(\gamma\)) versus final KL. pMCSA is the least sensitive to optimizer hyperparameters and results in stable convergence.}
%%       The final KL is obtained at the \(10^4\)th iteration.
%%       The target distribution is a 100-D Gaussian with \(\nu = 500\).
%%       The error bands are the 80\% quantiles while the solid lines are the median of 20 replications.
%%   }
%% \end{figure*}

\begin{figure*}
  \vspace{-0.2in}
  \centering
  \includegraphics[scale=0.9]{figures/stepsize_02.pdf}
  \vspace{-0.05in}
  \caption{\textbf{Optimizer stepsize (\(\gamma\)) versus final KL.
      pMCSA is the least sensitive to optimizer hyperparameters and results in stable convergence.}
      The final KL is obtained at the \(10^4\)th iteration.
      The target distribution is a 100-D Gaussian with \(\nu = 500\).
      The error bands are the 80\% quantiles while the solid lines are the median of 20 replications.
  }\label{fig:stepsize}
\end{figure*}
\begin{figure*}
  \vspace{-0.1in}
  \centering
  \includegraphics[scale=0.9]{figures/gaussian_02.pdf}
  \vspace{-0.07in}
  \caption{\textbf{
      KL divergence and gradient variance.
      pMCSA not only achieves the least gradient variance, but its variance also scales better with \(N\).
    }
    The target distribution is a 100-D Gaussian with \(\nu = 300\).
    The error bands are the 80\% quantiles while the solid lines are the median of 20 replications.
  }\label{fig:gaussian}
  \vspace{-0.2in}
\end{figure*}

\vspace{-0.05in}
\section{Evaluations}\label{section:eval}

\vspace{-0.05in}
\subsection{Experimental Setup}
\vspace{-0.05in}
\paragraph{Implementation}
For the realistic experiments, we implemented score climbing VI on top of the Turing~\citep{ge2018t} probabilistic programming framework.
For the variational family, we use diagonal Gaussians with the support transformation of~\citet{JMLR:v18:16-107}.
We use the ADAM optimizer by~\citet{kingma_adam_2015} with a learning rate of 0.01 in all of the experiments.
The computational budget is set to \(N=10\) and \(T=2\cdot10^4\) for all experiments unless specified.
We used the mean-field Gaussian variational family for all experiments.

\vspace{-0.1in}
\paragraph{Baselines}
We compare
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item pMCSA,
  \item JSA,
  \item MSC,
  \item MSC with with Rao-Blackwellization (\textbf{MSC-RB},~\citealt{NEURIPS2020_b2070693}),
  \item adaptive importance sampling (IS) with the self-normalized IS estimator (\textbf{SNIS},~\citealt{robert_monte_2004}), and
  \item evidence lower-bound maximization (\textbf{ELBO},~\citealt{pmlr-v33-ranganath14, JMLR:v18:16-107}) with the path derivative estimator~\citep{NIPS2017_e91068ff}
\end{enumerate*}

\vspace{-0.1in}
\subsection{Simulations}\label{section:simulation}
\vspace{-0.05in}
\paragraph{Target Distribution}
First, we perform experiments on a 100 dimensional correlated multivariate Gaussian where the covariance is sampled from a Wishart distribution with varying degrees of freedom \(\nu\).
This problem is challenging since a IMH/CIS kernel with a diagonal Gaussian proposal will mix very slowly.
Since exact KL divergence between the target and the varitional approximation is available, exact evaluation is possible.

\vspace{-0.1in}
\paragraph{Convergence and Gradient Variance}
The convergence in KL divergence and gradient variance is shown in \cref{fig:gaussian}.
The gradient variance was estimated using \(512\) independent Markov chains using the parameters generated by the main MCSA procedure.
We can see that pMCSA converges the fastest, and benefits the most from increasing \(N\).
This is apparent from the fact that gradient variance dramatically improves by increasing \(N\).
Although other methods also improve slightly by increasing \(N\), the variance reduction is minimal.
This confirms our theoretical analysis in~\cref{section:comparison}.

\vspace{-0.1in}
\paragraph{Robustness Against Optimizers}
We compared the robustness of the MCSA schemes with respect to different SGD optimizers and their hyperparameters in \cref{fig:stepsize}.
Clearly, pMCSA successfully converges for a wider variety of hyperparameters.
Since most methods are the most stable when using ADAM, we use ADAM for all of our remaining experiments.

%% \subsection{Hierarchical Logistic Regression}\label{section:logistic}
%% \vspace{-0.05in}
%% \paragraph{Experimental Setup}
%% We now perform logistic regression with the \texttt{Pima Indians} diabetes (\(\vz \in \mathbb{R}^{11}\),~\citealt{smith_using_1988}), \texttt{German credit} (\(\vz \in \mathbb{R}^{27}\)), and \texttt{heart disease} (\(\vz \in \mathbb{R}^{16}\),~\citealt{detrano_international_1989}) datasets obtained from the UCI repository~\citep{Dua:2019}.
%% 10\% of the data points were randomly selected in each of the 100 repetitions as test data.

% \input{gp_table.tex}

\input{bnn_table.tex}

\input{pgp_table.tex}

%% %

%% %
%% \begin{wrapfigure}{r}{0.6\textwidth}
%%   \vspace{-0.3in}
%% %\begin{figure}[h]
%%   %\vspace{-0.1in}
%%   %\centering
%%   \subfloat[Test Accuracy]{
%%     \includegraphics[scale=0.8]{figures/german_02.pdf}\label{fig:german_acc}
%%     \vspace{-0.1in}
%%   } 
%%   %% \subfloat[\texttt{heart}]{
%%   %%   \includegraphics[scale=0.75]{figures/heart_02.pdf}
%%   %% }
%%   %% \subfloat[\texttt{german}]{
%%   %%   \includegraphics[scale=0.75]{figures/german_02.pdf}
%%   %% }
%%   \subfloat[Test LPD]{
%%     \includegraphics[scale=0.8]{figures/german_03.pdf}\label{fig:german_lpd}
%%     \vspace{-0.05in}
%%   }
%%     \vspace{-0.05in}
%%   \caption{Test accuracy and log predictive density on the \texttt{german} dataset.
%%     The solid lines and colored regions are the mean and 80\% bootstrap confidence interval computed from 100 repetitions.
%%   }\label{fig:logistic}
%%   \vspace{-0.1in}
%% %\end{figure}
%% \end{wrapfigure}
%% %
%% \vspace{-0.05in}
%% \paragraph{Results}
%% The test accuracy and test log predictive density (Test LPD) results are shown in~\cref{table:logistic}.
%% Our proposed parallel state estimator (par.-IMH) achieves the best accuracy and predictive density results.
%% Despite having access to high-quality HMC samples, single-HMC shows poor performance.
%% This supports our analysis that par.-IMH with \(N \geq 2\) superior variance reduction to the single state estimator.
%% Also, seq.-IMH showed poor performance overall due to the correlated samples.
%% Among the two CIS kernel-based methods, single-CISRB performs only marginally better than single-CIS.

%%   \vspace{-0.1in}
%% \paragraph{Inclusive KL v.s. Exclusive KL}
%% While both ELBO and par.-IMH showed similar numerical performance, they chose different optimization paths in the parameter space.
%% This is shown in~\cref{fig:logistic}.
%% While the test accuracy suggests that ELBO converges quickly around \(t=2000\) (\cref{fig:german_acc}), in terms of uncertainty estimate, it takes much longer to converge (\cref{fig:german_lpd}).
%% This shows that inclusive KL minimization chooses a path that has better density coverage as expected.

  \vspace{-0.05in}
\subsection{Gaussian Process Classification}\label{section:bgp}
  \vspace{-0.05in}
\paragraph{Experimental Setup}
For a more challenging problem, we perform classification with latent Gaussian processes~\citep{NIPS2014_8c6744c9}.
The simplified probabilistic model is shown in~\cref{section:gp_logistic} and uses the Mat\'ern 5/2 covariance kernel with automatic relevance determination~\citep{neal_bayesian_1996}.
For the datasets, we use the \texttt{sonar} (\(\vz \in \mathbb{R}^{249}\),~\citealt{gorman_analysis_1988}), \texttt{ionosphere} (\(\vz \in \mathbb{R}^{351}\),~\citealt{Sigillito1989ClassificationOR}), and \texttt{breast} (\(\vz \in \mathbb{R}^{544}\),~\citealt{wolberg_multisurface_1990}) datasets.
For \texttt{breast}, we preprocessed the input features with z-standardization.
10\% of the data points were randomly selected in each of the 100 repetitions as test data.
For this experiment, the iteration complexity of ELBO is almost two orders of magnitude larger than all inclusive KL minimization methods.

%
% \begin{wrapfigure}{r}{0.45\textwidth}
%   \vspace{-0.4in}
%   \centering
%      \includegraphics[scale=0.9]{figures/ionosphere_01.pdf}
%   %% \subfloat[\texttt{german}]{
%   %%   \includegraphics[scale=0.75]{figures/breast_01.pdf}
%   %% } \\
%      \vspace{-0.1in}
%   \caption{Test log predictive density on the \texttt{ionosphere} dataset.
%     The solid lines and colored regions are the medians and 80\% percentiles computed from 100 repetitions.
%   }\label{fig:gp}
%   \vspace{-0.2in}
% \end{wrapfigure}
%
\vspace{-0.05in}
\paragraph{Result}
The results are shown in~\cref{table:gp}.
Again, among inclusive KL minimization, our method achieved the best results.
Compared to ELBO, its accuracy was lower on \texttt{breast}, but the uncertainty estimates were much better.
This is better-shown in~\cref{fig:gp}, where ELBO quickly converges to a point with poor uncertainty calibration.
%Meanwhile, on \texttt{breast}, ELBO gives better uncertainty estimates than inclusive KL minimization methods.
%This happens when the modal estimate (preferred by the exclusive KL) gives good accuracy and uncertainty estimates.

%%   \vspace{-0.05in}
%% \subsection{Marginal Likelihood Estimation}\label{section:mll}
%%   \vspace{-0.05in}
%% \paragraph{Experimental Setup}
%% Lastly, we now estimate the marginal log-likelihood of a hierarchical regression model with partial pooling (\texttt{radon}, \(\vz \in \mathbb{R}^{175}\),~\citealt{gelman_data_2007}) for modeling radon levels in U.S homes.
%% \texttt{radon} contains multiple posterior degeneracies from the hierarchy.
%% We estimated the reference marginal likelihood using \textit{thermodynamic integration} (TI,~\citealt{gelman_simulating_1998, neal_annealed_2001, lartillot_computing_2006}) with HMC implemented by Stan~\citep{carpenter_stan_2017, betancourt_conceptual_2017}.
%% %
%% \begin{wrapfigure}{r}{0.45\textwidth}
%%   \vspace{-0.25in}
%%   \centering
%%   \begin{minipage}[b]{0.25\linewidth}
%%     \centering
%%     \includegraphics[scale=0.8]{figures/radon_03.pdf}
%%   \end{minipage}
%%   \begin{minipage}[b]{0.7\linewidth}
%%     \centering
%%     \includegraphics[scale=0.8]{figures/radon_02.pdf}
%%     %\subcaption{\texttt{radon}}
%%   \end{minipage}
%%     \vspace{-0.1in}
%%   %% \begin{minipage}[b]{0.35\linewidth}
%%   %%   \centering
%%   %%   \includegraphics[scale=0.7]{figures/sv_02.pdf}
%%   %%   \subcaption{\texttt{stock}}\label{fig:sv}
%%   %% \end{minipage}
%%   \caption{Marginal log-likelihood estimates on the \texttt{radon} dataset.
%%     The solid lines and colored regions are the medians and 80\% percentiles computed from 100 repetitions.
%%   }\label{fig:marginal_likelihood}
%%   \vspace{-0.2in}
%% \end{wrapfigure}
%
  \vspace{-0.2in}
\paragraph{Results}
The results are shown in~\cref{fig:marginal_likelihood}.
par.-IMH converges quickly and provides the most accurate estimate.
By contrast, other estimators converge much more slowly.
SNIS and ELBO, on the other hand, overestimate \(\log Z\), which can be attributed to the mode-seeking behavior of ELBO and the small sample bias of SNIS.


%%% Local Variables:
%%% TeX-master: "master"
%%% End:
