
%% \begin{figure*}
%%   \centering
%%   \subfloat[Optimizer Sensitivity]{
%%     \includegraphics[scale=1.0]{figures/stepsize_01.pdf}
%%   } \\
%%   \subfloat[Gradient Variance]{
%%     \includegraphics[scale=1.0]{figures/gaussian_01.pdf}
%%   }
%%   \caption{\textbf{Optimizer stepsize (\(\gamma\)) versus final KL. pMCSA is the least sensitive to optimizer hyperparameters and results in stable convergence.}
%%       The final KL is obtained at the \(10^4\)th iteration.
%%       The target distribution is a 100-D Gaussian with \(\nu = 500\).
%%       The error bands are the 80\% quantiles while the solid lines are the median of 20 replications.
%%   }
%% \end{figure*}

\begin{figure*}
  \vspace{-0.2in}
  \centering
  \includegraphics[scale=1.0]{figures/gaussian_02.pdf}
  \vspace{-0.07in}
  \caption{\textbf{
      KL divergence and gradient variance.
      pMCSA not only achieves the least gradient variance, but its variance also scales better with \(N\).
    }
    The target distribution is a 20-D multivariate Gaussian with \(\nu = 100\).
    The error bands are the 80\% quantiles while the solid lines are the median of 20 replications.
  }\label{fig:gaussian}
\end{figure*}
\begin{figure*}
  \vspace{-0.1in}
  \centering
  \includegraphics[scale=1.0]{figures/stepsize_02.pdf}
  \vspace{-0.05in}
  \caption{\textbf{Optimizer stepsize (\(\gamma\)) versus final KL.
      pMCSA is the least sensitive to optimizer hyperparameters and results in stable convergence.}
      The final KL is obtained at the \(10^4\)th iteration.
      The target distribution is a 100-D Gaussian with \(\nu = 500\).
      The error bands are the 80\% quantiles while the solid lines are the median of 20 replications.
  }\label{fig:stepsize}
  %\vspace{-0.2in}
\end{figure*}

\vspace{-0.05in}
\section{Evaluations}\label{section:eval}
\vspace{-0.05in}
\subsection{Experimental Setup}
\vspace{-0.05in}
\paragraph{Implementation}
For the realistic experiments, we implemented score climbing VI on top of the Turing~\citep{ge2018t} probabilistic programming framework.
For the variational family, we use diagonal Gaussians with the support transformation of~\citet{JMLR:v18:16-107}.
We use the ADAM optimizer by~\citet{kingma_adam_2015} with a stepsize of 0.01 in all of the experiments.
The computational budget is set to \(N=10\) and \(T=5\cdot10^4\) for all experiments unless specified.

\vspace{-0.1in}
\paragraph{Baselines}
We compare
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item pMCSA,
  \item JSA~\citep{pmlr-v124-ou20a},
  \item MSC~\citep{NEURIPS2020_b2070693},
  \item MSC with with Rao-Blackwellization (\textbf{MSC-RB},~\citealt{NEURIPS2020_b2070693}), and
  %\item adaptive importance sampling (IS) with the self-normalized IS estimator (\textbf{SNIS},~\citealt{robert_monte_2004}), and
  \item evidence lower-bound maximization (\textbf{ELBO},~\citealt{pmlr-v33-ranganath14, JMLR:v18:16-107}) with the path derivative estimator~\citep{NIPS2017_e91068ff}
\end{enumerate*}

\vspace{-0.1in}
\subsection{Simulations}\label{section:simulation}
\vspace{-0.05in}
\paragraph{Target Distribution}
First, we verify our theoretical analysis on multivariate Gaussian distributions with full-rank covariances sampled from a Wishart distribution with  \(\nu\) degrees of freedom.
This problem is challenging since a IMH/CIS kernel with a diagonal Gaussian proposal will mix very slowly.
% Since the exact KL divergence between the target and the varitional approximation is available, accurate evaluation is possible.

\vspace{-0.1in}
\paragraph{Gradient Variance}
We evaluate our theoretical analysis on the gradient variance.
The variance is estimated from \(512\) independent Markov chains using the parameters generated by the main MCSA procedure.
The estimated variances are shown in \cref{fig:gaussian}.
We note the following observations:
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item pMCSA has the lowest variance overall, and it consistently benefits from increasing \(N\).
  \item MSC does not benefit from increasing \(N\) whatsoever.
  \item JSA does not benefit from increasing \(N\) until the \(q_{\vlambda}\) has sufficiently converged (this suggests \(w^*\) has become small).
\end{enumerate*}
These results confirm our theoretical results in~\cref{section:comparison,section:pmcsa}.

\vspace{-0.1in}
\paragraph{Robustness Against Optimizers}
Since the properties of MCGD are very different from conventional SGD, we investigate the effectiveness of different SGD optimizers.
The performance resulting from different SGD optimizers and varying stepsizes are shown in~\cref{fig:stepsize}.
Clearly, pMCSA successfully converges for a wide variety of hyperparameters.
Furthermore, most MCSA schemes seem to be the most stable with ADAM.
Since convergence of ADAM for MCGD has yet to be established, this will be a promising direction for future works.

%% \subsection{Hierarchical Logistic Regression}\label{section:logistic}
%% \vspace{-0.05in}
%% \paragraph{Experimental Setup}
%% We now perform logistic regression with the \texttt{Pima Indians} diabetes (\(\vz \in \mathbb{R}^{11}\),~\citealt{smith_using_1988}), \texttt{German credit} (\(\vz \in \mathbb{R}^{27}\)), and \texttt{heart disease} (\(\vz \in \mathbb{R}^{16}\),~\citealt{detrano_international_1989}) datasets obtained from the UCI repository~\citep{Dua:2019}.
%% 10\% of the data points were randomly selected in each of the 100 repetitions as test data.

% \input{gp_table.tex}

\input{bnn_table.tex}

\input{pgp_table.tex}

%% %

%%   \vspace{-0.1in}
%% \paragraph{Inclusive KL v.s. Exclusive KL}
%% While both ELBO and par.-IMH showed similar numerical performance, they chose different optimization paths in the parameter space.
%% This is shown in~\cref{fig:logistic}.
%% While the test accuracy suggests that ELBO converges quickly around \(t=2000\) (\cref{fig:german_acc}), in terms of uncertainty estimate, it takes much longer to converge (\cref{fig:german_lpd}).
%% This shows that inclusive KL minimization chooses a path that has better density coverage as expected.

  \vspace{-0.05in}
\subsection{Bayesian Neural Network Regression}\label{section:bnn}
  \vspace{-0.05in}
\paragraph{Experimental Setup}
For realistic experiments, we train Bayesian neural networks (BNN) for regression.
We use datasets from the UCI repository~\cite{Dua:2019} with 90\% random train-test splits.
For the model, we use the BNN by~\citet{pmlr-v37-hernandez-lobatoc15} (see \cref{section:bnn} for more details) with a \(50\)-unit hidden layer.
For prediction, we use the student-T approximation of~\citet{pmlr-v37-hernandez-lobatoc15}.

\vspace{-0.1in}
\paragraph{Results}
The results are shown in~\cref{table:bnn}.
pMCSA achieves the best performance compared to all other MCSA methods.
Also, its performance is comparable overall to exclusive KL minimization methods (ELBO) unlike other MCSA methods.
Furthermore, on \textsf{airfoil} and \textsf{energy}, pMCSA significantly improves over ELBO by 0.29 \texttt{nat} and 0.48 \texttt{nat}.
This contrasts to the fact that the worst case performance of pMCSA is not too far from ELBO (\textsf{gas}).
Additional experimental results can be found in~\cref{section:bnn_additional}

  \vspace{-0.05in}
\subsection{Robust Gaussian Process Regression}\label{section:bgp}
  \vspace{-0.05in}
\paragraph{Experimental Setup}
This time, we train Gaussian processes (GP) with a Student-T likelihood for robust regression.
Similarly, to the BNN experiment, we use datasets from the UCI repository~\cite{Dua:2019} with 90\% random train-test splits.
We use the Mate\'rn 5/2 covariance kernel with automatic relevance determination (ARD, \citealt{neal_bayesian_1996}) and a Gamma prior on the degre of freedom (see \cref{section:bnn} for more details).
For prediction, we use the modes of \(q_{\vlambda}\) for the hyperparameters, while we integrate over \(q_{\vlambda}\) for the latent function parameters.
Note that, in this experiment, the likelihood gradients are significantly expensive, making the iteration complexity of ELBO \(N=1\) comparable to MCSA methods with \(N=10\).
Therefore, we only run ELBO with \(N=1\).

\vspace{-0.1in}
\paragraph{Results}
The results are shown in~\cref{table:gp}.
Overall, pMCSA achieves the best performance compared to other MCSA methods and ELBO.
Interestingly, ELBO performs significantly worse than all other MCSA methods on most datasets, meaning that the exclusive KL performs poorly on GP posteriors.
Our encouraging regression results suggest that incorporating scalable GP methods such as inducing points~\citep{NIPS2005_4491777b} may lead to an important new class of approximate GP model.
Additional experimental results can be found in~\cref{section:gp_additional}.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
