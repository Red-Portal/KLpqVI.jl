\vspace{-0.05in}
\section{Related Works}\label{section:related}
\vspace{-0.05in}
\paragraph{Inclusive KL minimization}
Our method directly builds on top of MSC~\citep{NEURIPS2020_b2070693}, which minimizes the inclusive KL divergence.
Concurrently, in the context of variational autoencoders with discrete latent variables,~\citet{pmlr-v124-ou20a} proposed JSA.
As shown in~\cref{section:comparison}, these methods are part of our MCSA framework.
Meanwhile,~\citet{li_approximate_2017} used a similar approach to MSC, but their method is slow to achieve stationarity, and does not fit into the MCSA framework. 
Therefore, convergence of this method is not guaranteed.
Other than using Markov chains,~\citet{DBLP:journals/corr/BornscheinB14,le_revisiting_2019} used SNIS while~\citet{pmlr-v119-wu20h} used sequential Monte Carlo (SMC) for estimating the score, but these methods are restricted to deep generative models.
On a different note,~\citet{pmlr-v161-jerfel21a} used boosting instead of SGD to minimize the inclusive KL, which results in a more flexible variational family.
%Lastly,~\citet{10.5555/2074022.2074067} introduced expectation propagation.

\vspace{-0.1in}
\paragraph{MCMC for VI, VI for MCMC}
MCMC has been widely utilized in VI, not only for inclusive KL minimization.
For example,~\citet{pmlr-v37-salimans15, pmlr-v97-ruiz19a} construct alternative divergence bounds from samples of an MCMC sampler.
On the other side of the coin, as pointed out by~\citet{pmlr-v124-ou20a}, using \(q_{\vlambda}\) within the MCMC kernel makes MCSA structurally equivalent to adaptive MCMC.
In particular,~\citet{10.1007/s11222-008-9110-y, garthwaite_adaptive_2016} discuss the use of stochastic approximation in adaptive MCMC.
Also,~\citet{andrieu_ergodicity_2006, keith_adaptive_2008, holden_adaptive_2009, giordani_adaptive_2010} specifically discuss adapting the propsosal of IMH kernels.
Most similar to score climbing VI is the work of~\citet{keith_adaptive_2008} where they propose to use cross-entropy minimization~\citep{barbakh_cross_2009}, which is mathematically identical to inclusive VI.
However, in this work, we focused on the variational approximation \(q_{\vlambda}\) (or proposal in an adaptive MCMC context) rather than the posterior samples generated intermediately.
More recently, several methods that apply modern VI techniques to adaptating MCMC kernels have been developed.
For adapting IMH kernels, \citet{habib2018auxiliary} minimize the exclusive KL divergence while~\cite{neklyudov_metropolishastings_2019} minimize the symmetric KL divergence.
And for HMC,~\citet{zhang_variational_2018, pmlr-v139-campbell21a} have proposed to use score matching, ELBO maximization, and Stein discrepancy minimization.


%%% Local Variables:
%%% TeX-master: "master"
%%% End:
