
\begin{theoremEnd}{theorem}\label{thm:pmcsa}
  pMCSA, our proposed scheme, is obtained by setting
  {\small
  \begin{align*}
    P_{\vlambda}^n\left(\veta, d\veta\prime\right)
    = 
    K_{\vlambda}^n\left(\vz^{(1)}, d\vz\prime^{(1)}\right)
    \,
    K_{\vlambda}^n\left(\vz^{(2)}, d\vz\prime^{(2)}\right)
    \cdot
    \ldots 
    \cdot
    K_{\vlambda}^n\left(\vz^{(N)}, d\vz\prime^{(N)}\right)
  \end{align*}
  }
  with \(\veta = \left[\vz^{(1)}, \vz^{(2)}, \ldots, \vz^{(N)}\right]\).
  Then, given~\cref{thm:bounded_weight,thm:bounded_score}, the mixing rate and the gradient variance bounds are
  {\small
  \begin{align*}
    \DTV{P_{\vlambda}^n\left(\veta, \cdot\right)}{\Pi}
    \leq
    C\left(N\right)\,{\rho^n}
    \quad\text{and}\quad
    \E{ \norm{ \vg\left(\vlambda, \rvveta\right) }^2_{*} \,\middle|\, \mathcal{F}_{t} }
    \leq
    L^2 \left[\; \frac{1}{N} + \frac{1}{N}\,\left(1 - \frac{1}{w^*}\right) \;\right],
  \end{align*}
  }
  where \(w^* = \sup_{\vz} \pi\left(\vz\right) / q_{\text{def.}}\left(\vz;\vlambda\right)\) and \(C\) is some positive constant depending on \(N\).
\end{theoremEnd}
\begin{proofEnd}

  Our proposed scheme, pMCSA, is described in~\cref{alg:jsa}. 
  At each iteration, our scheme performs a single MCMC transition for each of the \(N\) samples, or chains, to estimate the gradient.
  Similarly to JSA, we use the IMH kernel \(K_{\vlambda}\).

  \paragraph{Ergodicity of the Markov Chain}
  Since our kernel operates the same MCMC kernel \(K_{\vlambda}\) for each of the \(N\) parallel Markov chains, the \(n\)-step marginal kernel \(P_{\vlambda}\) can be represented as
  \begin{align*}
    P_{\vlambda}^n\left(\veta, d\veta\prime\right)
    = 
    K_{\vlambda}^n\left(\vz^{(1)}, d\vz\prime^{(1)}\right)
    \,
    K_{\vlambda}^n\left(\vz^{(2)}, d\vz\prime^{(2)}\right)
    \cdot
    \ldots 
    \cdot
    K_{\vlambda}^n\left(\vz^{(N)}, d\vz\prime^{(N)}\right).
  \end{align*}
  Then, the convergence in total variation \(d_{\mathrm{TV}}\left(\cdot, \cdot\right)\) can be shown to decrease geometrically as
  \begin{alignat}{2}
    &\DTV{K^{k}_{\vlambda}\left(\veta, \cdot\right)}{\Pi}
    \nonumber
    \\
    &\quad=
    \sup_{A}
    \abs{
      \Pi\left(A\right)
      -
      P^{n}_{\vlambda}\left(\veta, A\right)
    }
    &&\quad\text{\textit{Definition of \(d_{\text{TV}}\)}}
    \nonumber
    \\
    &\quad\leq
    \sup_{A}
    \big|\;
    \int_{A}
      \pi\left(d\vz\prime_1\right) \cdot \ldots \cdot \pi\left(d\vz\prime_N\right)
    \nonumber
      \\
      &\qquad\qquad\qquad-
      K^n_{\vlambda}\left(\vz_1, d\vz\prime_1\right) \cdot \ldots \cdot K^n_{\vlambda}\left(\vz_N, d\vz\prime_N\right)
    \;\big|
    \nonumber
    \\
    &\quad\leq
    \sup_{A}
    \sum_{n=1}^N
    \abs{
    \int_{A}
      \pi\left(d\vz\prime_k\right) - K^{n}_{\vlambda}\left(\vz_n, d\vz\prime_n\right) 
    }
    &&\quad\text{\textit{\cref{thm:product_measure_bound}}}
    \nonumber
    \\
    &\quad=
    \sum_{n=1}^N
    \DTV{K^n_{\vlambda}\left(\vz_n, \cdot\right)}{\pi}
    &&\quad\text{\textit{Definition of TV distance}}
    \nonumber
    \\
    &\quad\leq
    \sum_{n=1}^N
    \rho^{n}
    &&\quad\text{\textit{Geometric ergodicity}}
    \nonumber
    \\
    &\quad=
    N\,\rho^{k}
    &&\quad\text{\textit{Solved sum}}.
    \nonumber
  \end{alignat}

  \paragraph{\textbf{Bound on the Gradient Variance}}
  The bound on the gradient variance can be derived in similar manner to JSA as
  \begin{alignat}{2}
    &\E{ \norm{ \vg\left(\vlambda, \rvveta\right) }^2_{*} \,\middle|\, \mathcal{F}_{t} }
    \nonumber
    \\
    &\quad=
    \E{ \norm{ \vg\left(\vlambda, \rvveta\right) }^2 \,\middle|\, \vz_{t-1}^{(1:N)},\, \vlambda_{t-1} }
    \nonumber
    \\
    &\quad=
    \E{ \norm{ \frac{1}{N}\sum^{N}_{n=1} \vs\left(\vlambda; \rvvz_{n}\right) }^2_{*} \,\middle|\, \vz_{t-1}^{(1:N)},\, \vlambda_{t-1} }
    \nonumber
    \\
    &\quad\leq
    \E{  \frac{1}{N^2} \sum^{N}_{n=1} \norm{\vs\left(\vlambda; \rvvz_{n}\right) }^2_{*} \,\middle|\, \vz_{t-1}^{(1:N)},\, \vlambda_{t-1} }
    &&\quad\text{\textit{Triangle inequality}}
    \nonumber
    \\
    &\quad=
    \frac{1}{N^2}\sum^{N}_{n=1} \Esub{\rvvz_{n} \sim K_{\vlambda_{t-1}}\left(\vz_{t-1}^{(n)}, \cdot\right)}{ \norm{\vs\left(\vlambda; \rvvz_{n}\right) }^2_{*} \,\middle|\,  \vz_{t-1}^{(1:N)},\, \vlambda_{t-1} }
    &&\quad\text{\textit{Linearity of expectation}}
    \nonumber
    \\
    &\quad\leq
    \frac{1}{N^2}\sum^{N}_{n=1}
      \Esub{\rvvz_n \sim q_{\text{def.}}\left(\cdot;\vlambda_{t-1}\right)}{ \norm{\vs\left(\vlambda; \rvvz_n \right)}_{*}^2 }
      +
      {\left(1 - \frac{1}{w^*}\right)} \, \norm{\vs\left(\vlambda; \vz_{t-1}^{(n)} \right)}_{*}^2
    &&\quad\text{\textit{\cref{thm:imh_expecation}}}
    \nonumber
    \\
    &\quad\leq
    \frac{1}{N^2}\sum^{N}_{n=1}
        L^2 + {\left(1 - \frac{1}{w^*}\right)} \, L^2
    &&\quad\text{\textit{\cref{thm:bounded_score}}}
    \nonumber
    \\
    &\quad=
    \frac{L^2}{N^2} \sum^{N}_{n=1}
      1 + {\left(1 - \frac{1}{w^*}\right)}
    \nonumber
    &&\quad\text{\textit{Moved constant forward}}
    \\
    &\quad=
    L^2 \left[ \frac{1}{N} + \frac{1}{N}\,{\left(1 - \frac{1}{w^*}\right)} \right].
    \nonumber
    &&\quad\text{\textit{Solved sum}}
  \end{alignat}
  %Therefore, the gradient decreases at a solid \(\mathcal{O}\left(1/N\right)\) rate.
\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
