
\begin{theoremEnd}{theorem}\label{thm:pmcsa}
  pMCSA, our proposed scheme, is obtained by setting
  {%\small
  \begin{align*}
    P_{\vlambda}^k\left(\veta, d\veta^{\prime}\right)
    = 
    K_{\vlambda}^k\left(\vz^{(1)}, d\vz^{\prime\; (1)}\right)
    \,
    K_{\vlambda}^k\left(\vz^{(2)}, d\vz^{\prime\; (2)}\right)
    \cdot
    \ldots 
    \cdot
    K_{\vlambda}^k\left(\vz^{(N)}, d\vz^{\prime\; (N)}\right)
  \end{align*}
  }
  with \(\veta = \left(\vz^{(1)}, \vz^{(2)}, \ldots, \vz^{(N)}\right)\).
  Then, given~\cref{thm:bounded_weight,thm:bounded_score}, the mixing rate and the gradient variance bounds are
  {\small
  \begin{align*}
    \textstyle
    \DTV{P_{\vlambda}^k\left(\veta, \cdot\right)}{\Pi}
    \leq
    C\left(N\right)\,{r^k}
    \;\;\text{and}\;\;
    \E{ {\lVert\rvvg_{t,\mathrm{pMCSA}}\rVert}^2_2 \,\middle|\, \mathcal{F}_{t-1} }
    \leq
    L^2 \left[\; \frac{1}{N} + \frac{1}{N}\,\left(1 - \frac{1}{w^*}\right) \;\right]
    +
    \mathcal{O}\left(r^{t}\right)
    +
    {\lVert\vmu\rVert}^2_2,
  \end{align*}
  }%
  where
  \(\vmu = \mathbb{E}_{\pi}\vs\left(\vlambda;\rvvz\right)\), 
  \(w^* = \sup_{\vz} \pi\left(\vz\right) / q_{\text{def.}}\left(\vz;\vlambda\right)\) and \(C\left(N\right) > 0\) is a finite constant.
\end{theoremEnd}
\begin{proofEnd}

  Our proposed scheme, pMCSA, is described in~\cref{alg:jsa}. 
  At each iteration, our scheme performs a single MCMC transition for each of the \(N\) samples, or chains, to estimate the gradient.
  That is,
  \begin{align*}
    \rvvz^{(1)}_{t} &\mid \rvvz_{t-1}^{(1)},\, \rvvlambda_{t-1} \sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t-1}^{(1)}, \cdot\right) \\
    \rvvz^{(2)}_{t} &\mid \rvvz_{t-1}^{(2)},\, \rvvlambda_{t-1}  \sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t-1}^{(2)}, \cdot\right) \\
    &\qquad\qquad\vdots
    \\
    \rvvz^{(N)}_{t} &\mid \rvvz^{(N)}_{t-1},\, \rvvlambda_{t-1}  \sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t-1}^{(N)}, \cdot\right)
    \\
    \rvvg_{t,\text{pMCSA}} &= -\frac{1}{N}\,\sum^{N}_{n=1} \, \vs\left(\vlambda, \rvvz^{(n)}_t\right),
  \end{align*}
  where \(K_{\rvvlambda_{t-1}}^n\) is an \(n\)-transition IMH kernel using \(q_{\text{def.}}\left(\cdot; \rvvlambda_{t-1}\right)\).

  \paragraph{Ergodicity of the Markov Chain}
  Since our kernel operates the same MCMC kernel \(K_{\vlambda}\) for each of the \(N\) parallel Markov chains, the \(n\)-step marginal kernel \(P_{\vlambda}\) can be represented as
  \begin{align*}
    P_{\vlambda}^k\left(\veta, d\veta^{\prime}\right)
    = 
    K_{\vlambda}^k\left(\vz^{(1)}, d\vz^{\prime\; (1)}\right)
    \,
    K_{\vlambda}^k\left(\vz^{(2)}, d\vz^{\prime\; (2)}\right)
    \cdot
    \ldots 
    \cdot
    K_{\vlambda}^k\left(\vz^{(N)}, d\vz^{\prime\; (N)}\right).
  \end{align*}
  Then, the convergence in total variation \(d_{\mathrm{TV}}\left(\cdot, \cdot\right)\) can be shown to decrease geometrically as
  \begin{alignat}{2}
    &\DTV{K^{k}_{\vlambda}\left(\veta, \cdot\right)}{\Pi}
    \nonumber
    \\
    &\quad=
    \sup_{A}
    \abs{
      \Pi\left(A\right)
      -
      P^{k}_{\vlambda}\left(\veta, A\right)
    }
    &&\quad\text{\textit{Definition of \(d_{\text{TV}}\)}}
    \nonumber
    \\
    &\quad\leq
    \sup_{A}
    \big|\;
    \int_{A}
      \pi\left(d\vz^{\prime}_1\right) \cdot \ldots \cdot \pi\left(d\vz^{\prime}_N\right)
    \nonumber
      \\
      &\qquad\qquad\qquad-
      K^k_{\vlambda}\left(\vz_1, d\vz^{\prime}_1\right) \cdot \ldots \cdot K^k_{\vlambda}\left(\vz_N, d\vz^{\prime}_N\right)
    \;\big|
    \nonumber
    \\
    &\quad\leq
    \sup_{A}
    \sum_{n=1}^N
    \abs{
    \int_{A}
      \pi\left(d\vz^{\prime}_k\right) - K^{k}_{\vlambda}\left(\vz_n, d\vz^{\prime}_n\right) 
    }
    &&\quad\text{\textit{\cref{thm:product_measure_bound}}}
    \nonumber
    \\
    &\quad=
    \sum_{n=1}^N
    \DTV{K^k_{\vlambda}\left(\vz_n, \cdot\right)}{\pi}
    &&\quad\text{\textit{\cref{eq:imh_ergodicity}}}
    \nonumber
    \\
    &\quad\leq
    \sum_{n=1}^N
    r^{k}
    &&\quad\text{\textit{Geometric ergodicity}}
    \nonumber
    \\
    &\quad=
    N\,r^{k}.
    \nonumber
  \end{alignat}

  \paragraph{\textbf{Bound on the Gradient Variance}}
  By \cref{thm:second_moment_bound}, the second moment of the gradient is bounded as
  \begin{alignat}{2}
    \E{ {\lVert \rvvg_{t,\text{pMCSA}} \rVert}_2^2 \,\middle|\, \mathcal{F}_{t-1} } 
    &=
    \V{\rvvg_{t,\text{pMCSA}} \mid \mathcal{F}_{t-1}} + {\mathsf{Bias}\left[\rvvg_{t,\text{pMCSA}} \mid \mathcal{F}_{t-1}\right]}^2 + 2\, \mathsf{Bias}\left[\rvvg_{t,\text{pMCSA}} \mid \mathcal{F}_{t-1}\right] \norm{\vmu}_2 + \norm{\vmu}^2_2
    \nonumber
    \\
    &\leq
    \V{ \rvvg_{t,\text{pMCSA}} \mid \mathcal{F}_{t-1}} + {\mathsf{Bias}\left[\rvvg_{t,\text{pMCSA}} \mid \mathcal{F}_{t-1}\right]}^2 + 2\,L\,\mathsf{Bias}\left[\rvvg_{t,\text{pMCSA}} \mid \mathcal{F}_{t-1}\right] + \norm{\vmu}^2_2,
    \nonumber
  \end{alignat}
  where \(\vmu = \mathbb{E}_{\pi}\vs\left(\vlambda; \rvvz\right)\).
  As shown in \cref{thm:mcmc_bias}, the bias terms decreases in a rate of \(r^{t}\).
  Therefore, 
  \begin{alignat}{2}
    \E{ {\lVert \rvvg_{t,\text{pMCSA}} \rVert}_2^2 \,\middle|\, \mathcal{F}_{t-1} } 
    &\leq
    \V{ \rvvg_{t,\text{pMCSA}} \mid \mathcal{F}_{t-1}} + \norm{\vmu}^2_2 + \mathcal{O}\left(r^{t}\right).
    &&\quad\text{\textit{\cref{thm:mcmc_bias}}}
    \nonumber
  \end{alignat}
  As noted in the proof of \cref{thm:jsa}, it is possible to obtain a tighter bound on the bias terms such that \(\mathcal{O}\left(r^{t}/N\right)\).

  The variance term is bounded as
  \begin{alignat}{2}
    &\V{ \rvvg_{t,\text{pMCSA}} \mid \mathcal{F}_{t-1}}
    \nonumber
    \\
    &\quad=
    \V{ \frac{1}{N}\sum^{N}_{n=1} \vs\left(\vlambda; \rvvz^{(n)}_t\right) \,\middle|\, \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }
    \nonumber
    \\
    &\quad=
    \frac{1}{N^2}\sum^{N}_{n=1} \V{ \vs\left(\vlambda; \rvvz^{(n)}_t\right) \,\middle|\, \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }
    &&\quad\text{\textit{\(\rvvz^{(i)}_t \,\bot\, \rvvz^{(j)}_t\) for \(i \neq j\)}}
    \nonumber
    \\
    &\quad\leq
    \frac{1}{N^2}\sum^{N}_{n=1} \E{ \norm{\vs\left(\vlambda; \rvvz^{(n)}_t\right) }^2_2 \,\middle|\, \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }_2
    \nonumber
    \\
    &\quad=
    \frac{1}{N^2}\sum^{N}_{n=1} \Esub{\rvvz^{(n)}_t \sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t-1}^{(n)}, \cdot\right)}{ \norm{\vs\left(\vlambda; \rvvz^{(n)}_t\right) }^2_2 \,\middle|\,  \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }
    \nonumber
    \\
    &\quad\leq
    \frac{1}{N^2}\sum^{N}_{n=1} \left[\,
      \Esub{\rvvz \sim q_{\text{def.}}\left(\cdot;\rvvlambda_{t-1}\right)}{ \norm{\vs\left(\vlambda; \rvvz \right)}^2_2 }
      +
      {r} \, \norm{\vs\left(\vlambda; \rvvz_{t-1}^{(n)} \right)}^2_2
      \,\right]
    &&\quad\text{\textit{\cref{thm:imh_expecation}}}
    \nonumber
    \\
    &\quad\leq
    \frac{1}{N^2}\sum^{N}_{n=1} \left[\,
        L^2 + {r} \, L^2 \,\right]
    &&\quad\text{\textit{\cref{thm:bounded_score}}}
    \nonumber
    \\
    &\quad=
    \frac{L^2}{N^2} \sum^{N}_{n=1} \left[\,
      1 + {r} \,\right]
    \nonumber
    \\
    &\quad=
    L^2 \left[\, \frac{1}{N} + \frac{1}{N}\,{\left(1 - \frac{1}{w^*}\right)} \,\right].
    \nonumber
  \end{alignat}
\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
