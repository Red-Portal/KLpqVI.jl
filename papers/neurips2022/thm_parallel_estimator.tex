
\begin{theoremEnd}{theorem}\label{thm:pmcsa}
  pMCSA, our proposed scheme, is obtained by setting
  {%\small
  \begin{align*}
    P_{\vlambda}^k\left(\veta, d\veta^{\prime}\right)
    = 
    K_{\vlambda}^k\left(\vz^{(1)}, d\vz^{\prime\; (1)}\right)
    \,
    K_{\vlambda}^k\left(\vz^{(2)}, d\vz^{\prime\; (2)}\right)
    \cdot
    \ldots 
    \cdot
    K_{\vlambda}^k\left(\vz^{(N)}, d\vz^{\prime\; (N)}\right)
  \end{align*}
  }
  with \(\veta = \left(\vz^{(1)}, \vz^{(2)}, \ldots, \vz^{(N)}\right)\).
  Then, given~\cref{thm:bounded_weight,thm:bounded_score}, the mixing rate and the gradient variance bounds are
  {%\small
  \begin{align*}
    \DTV{P_{\vlambda}^k\left(\veta, \cdot\right)}{\Pi}
    \leq
    C\left(N\right)\,{\rho^k}
    \quad\text{and}\quad
    \V{ \vg_{\mathrm{pMCSA}} \,\middle|\, \mathcal{F}_{t-1} }
    \leq
    L^2 \left[\; \frac{1}{N} + \frac{1}{N}\,\left(1 - \frac{1}{w^*}\right) \;\right],
  \end{align*}
  }
  where \(w^* = \sup_{\vz} \pi\left(\vz\right) / q_{\text{def.}}\left(\vz;\vlambda\right)\) and \(C\) is some positive constant depending on \(N\).
\end{theoremEnd}
\begin{proofEnd}

  Our proposed scheme, pMCSA, is described in~\cref{alg:jsa}. 
  At each iteration, our scheme performs a single MCMC transition for each of the \(N\) samples, or chains, to estimate the gradient.
  That is,
  \begin{align*}
    \rvvz^{(1)}_{t} \mid \rvvz_{t-1}^{(1)}, \rvvlambda_{t-1} &\sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t-1}^{(1)}, \cdot\right) \\
    \rvvz^{(2)}_{t} \mid \rvvz_{t-1}^{(2)}, \rvvlambda_{t-1}  &\sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t-1}^{(2)}, \cdot\right) \\
    &\vdots
    \\
    \rvvz^{(N)}_{t} \mid \rvvz^{(N)}_{t-1}, \rvvlambda_{t-1}  &\sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t-1}^{(N)}, \cdot\right)
    \\
    \rvvg_{\text{pMCSA}} &= -\frac{1}{N}\,\sum^{N}_{n=1} \, \vs\left(\vlambda, \rvvz^{(n)}_t\right),
  \end{align*}
  where \(K_{\rvvlambda_{t-1}}^n\) is an \(n\)-transition IMH kernel using \(q_{\text{def.}}\left(\cdot; \rvvlambda_{t-1}\right)\).

  \paragraph{Ergodicity of the Markov Chain}
  Since our kernel operates the same MCMC kernel \(K_{\vlambda}\) for each of the \(N\) parallel Markov chains, the \(n\)-step marginal kernel \(P_{\vlambda}\) can be represented as
  \begin{align*}
    P_{\vlambda}^k\left(\veta, d\veta^{\prime}\right)
    = 
    K_{\vlambda}^k\left(\vz^{(1)}, d\vz^{\prime\; (1)}\right)
    \,
    K_{\vlambda}^k\left(\vz^{(2)}, d\vz^{\prime\; (2)}\right)
    \cdot
    \ldots 
    \cdot
    K_{\vlambda}^k\left(\vz^{(N)}, d\vz^{\prime\; (N)}\right).
  \end{align*}
  Then, the convergence in total variation \(d_{\mathrm{TV}}\left(\cdot, \cdot\right)\) can be shown to decrease geometrically as
  \begin{alignat}{2}
    &\DTV{K^{k}_{\vlambda}\left(\veta, \cdot\right)}{\Pi}
    \nonumber
    \\
    &\quad=
    \sup_{A}
    \abs{
      \Pi\left(A\right)
      -
      P^{k}_{\vlambda}\left(\veta, A\right)
    }
    &&\quad\text{\textit{Definition of \(d_{\text{TV}}\)}}
    \nonumber
    \\
    &\quad\leq
    \sup_{A}
    \big|\;
    \int_{A}
      \pi\left(d\vz^{\prime}_1\right) \cdot \ldots \cdot \pi\left(d\vz^{\prime}_N\right)
    \nonumber
      \\
      &\qquad\qquad\qquad-
      K^k_{\vlambda}\left(\vz_1, d\vz^{\prime}_1\right) \cdot \ldots \cdot K^k_{\vlambda}\left(\vz_N, d\vz^{\prime}_N\right)
    \;\big|
    \nonumber
    \\
    &\quad\leq
    \sup_{A}
    \sum_{n=1}^N
    \abs{
    \int_{A}
      \pi\left(d\vz^{\prime}_k\right) - K^{k}_{\vlambda}\left(\vz_n, d\vz^{\prime}_n\right) 
    }
    &&\quad\text{\textit{\cref{thm:product_measure_bound}}}
    \nonumber
    \\
    &\quad=
    \sum_{n=1}^N
    \DTV{K^k_{\vlambda}\left(\vz_n, \cdot\right)}{\pi}
    &&\quad\text{\textit{Definition of TV distance}}
    \nonumber
    \\
    &\quad\leq
    \sum_{n=1}^N
    \rho^{k}
    &&\quad\text{\textit{Geometric ergodicity}}
    \nonumber
    \\
    &\quad=
    N\,\rho^{k}
    &&\quad\text{\textit{Solved sum}}.
    \nonumber
  \end{alignat}

  \paragraph{\textbf{Bound on the Gradient Variance}}
  The bound on the gradient variance can be derived in similar manner to JSA as
  \begin{alignat}{2}
    &\V{ \vg_{\mathrm{pMCSA}} \,\middle|\, \mathcal{F}_{t-1} }
    \nonumber
    \\
    &\quad=
    \V{ \vg_{\mathrm{pMCSA}} \,\middle|\, \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }
    \nonumber
    \\
    &\quad=
    \V{ \frac{1}{N}\sum^{N}_{n=1} \vs\left(\vlambda; \rvvz^{(n)}\right) \,\middle|\, \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }
    \nonumber
    \\
    &\quad=
    \frac{1}{N}\sum^{N}_{n=1} \V{ \vs\left(\vlambda; \rvvz^{(n)}\right) \,\middle|\, \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }
    &&\quad\text{\textit{\(\rvvz^{(i)} \,\bot\, \rvvz^{(j)}\) for \(i \neq j\)}}
    \nonumber
    \\
    &\quad\leq
    \frac{1}{N^2}\sum^{N}_{n=1} \E{ \norm{\vs\left(\vlambda; \rvvz^{(n)}\right) }^2 \,\middle|\, \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }
    \nonumber
    \\
    &\quad=
    \frac{1}{N^2}\sum^{N}_{n=1} \Esub{\rvvz^{(n)} \sim K_{\rvvlambda_{t-1}}\left(\vz_{t-1}^{(n)}, \cdot\right)}{ \norm{\vs\left(\vlambda; \rvvz^{(n)}\right) }^2 \,\middle|\,  \rvvz_{t-1}^{(1:N)},\, \rvvlambda_{t-1} }
    \nonumber
    \\
    &\quad\leq
    \frac{1}{N^2}\sum^{N}_{n=1}
      \Esub{\rvvz^{(n)} \sim q_{\text{def.}}\left(\cdot;\rvvlambda_{t-1}\right)}{ \norm{\vs\left(\vlambda; \rvvz^{(n)} \right)}^2 }
      +
      {\rho} \, \norm{\vs\left(\vlambda; \rvvz_{t-1}^{(n)} \right)}^2
    &&\quad\text{\textit{\cref{thm:imh_expecation}}}
    \nonumber
    \\
    &\quad\leq
    \frac{1}{N^2}\sum^{N}_{n=1}
        L^2 + {\left(1 - \frac{1}{w^*}\right)} \, L^2
    &&\quad\text{\textit{\cref{thm:bounded_score}}}
    \nonumber
    \\
    &\quad=
    \frac{L^2}{N^2} \sum^{N}_{n=1}
      1 + {\left(1 - \frac{1}{w^*}\right)}
    \nonumber
    \\
    &\quad=
    L^2 \left[ \frac{1}{N} + \frac{1}{N}\,{\left(1 - \frac{1}{w^*}\right)} \right].
    \nonumber
  \end{alignat}
  %Therefore, the gradient decreases at a solid \(\mathcal{O}\left(1/N\right)\) rate.
\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
