
\begin{theoremEnd}{theorem}\label{thm:jsa}
  JSA~\citep{pmlr-v124-ou20a} is obtained by defining 
  {%\small
  \begin{align*}
  P_{\vlambda}^k\left(\veta, d\veta^{\prime}\right)
  = 
  K_{\vlambda}^{N\,\left(k-1\right) + 1}\left(\vz^{(1)}, d\vz^{\prime\;(1)}\right)
  \,
  K_{\vlambda}^{N\,\left(k-1\right) + 2}\left(\vz^{(2)}, d\vz^{\prime\;(2)}\right)
  \cdot
  \ldots 
  \cdot
  K_{\vlambda}^{N\,\left(k-1\right) + N}\left(\vz^{(N)}, d\vz^{\prime\;(N)}\right)
  \end{align*}
  }%
  with \(\veta_t = \big(\vz_t^{(1)}, \vz_t^{(2)}, \ldots, \vz_t^{(N)}\big)\).
  Then, given~\cref{thm:bounded_weight,thm:bounded_score}, the mixing rate and the gradient variance bounds are
  {\small
  \begin{align*}
   % \textstyle
    \DTV{P_{\vlambda}^k\left(\veta, \cdot\right)}{\Pi}
    \leq
    C\left(r, N\right)\,{r}^{k\,N}
    \;\;\text{and}\;\;
   % 
    \E{ {\lVert\vg_{t,\mathrm{JSA}}\rVert}^2_2 \,\middle|\, \mathcal{F}_{t-1} }
    \leq
    L^2 \,
    \left[\,
    \frac{1}{2} + \frac{3}{2}\,\frac{1}{N}
    + \mathcal{O}\left(\nicefrac{1}{w^* + r^{t\,N}}\right)
    \,\right]
    +
    C_{\text{cov}}
    +
    {\lVert\vmu\rVert}^2_2,
    \nonumber
  \end{align*}
  }%
  where
  \(\vmu = \mathbb{E}_{\pi}\vs\left(\vlambda;\rvvz\right)\), 
  \(\small\textstyle
  C_{\text{cov}} = \frac{2}{N^2} \sum^{N}_{n=2} \sum^{n-1}_{m = 1} \Cov{\vs\left(\vlambda; \rvvz^{(n)}_t\right), \vs\left(\vlambda; \rvvz^{(m)}_t\right) \,\middle|\, \mathcal{F}_{t-1} }
  \) is the sum of the covariance between the samples, \(w^* = \sup_{\vz} \pi\left(\vz\right) / q_{\text{def.}}\left(\vz;\vlambda\right)\), and \(C\left(r, N\right) > 0\) is a finite constant.
\end{theoremEnd}
\begin{proofEnd}

  JSA is described in~\cref{alg:jsa}. 
  At each iteration, it performs \(N\) MCMC transitions and uses the \(N\) intermediate states to estimate the gradient.
  That is,
  \begin{align*}
    \rvvz^{(1)}_{t} &\mid \rvvz_{t-1}^{(N)},\, \rvvlambda_{t-1} \sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t-1}^{(N)}, \cdot\right) \\
    \rvvz^{(2)}_{t} &\mid \rvvz^{(1)}_{t},\, \rvvlambda_{t-1}  \sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t}^{(1)}, \cdot\right) \\
    &\qquad\qquad\vdots
    \\
    \rvvz^{(N)}_{t} &\mid \rvvz^{(N-1)}_{t},\, \rvvlambda_{t-1}  \sim K_{\rvvlambda_{t-1}}\left(\rvvz_{t}^{(N-1)}, \cdot\right)
    \\
    \rvvg_{t,\text{JSA}}  &= -\frac{1}{N}\,\sum^{N}_{n=1} \, \vs\left(\vlambda, \rvvz^{(n)}_t\right),
  \end{align*}
  where \(K_{\rvvlambda_{t-1}}^n\) is an \(n\)-transition IMH kernel using \(q_{\text{def.}}\left(\cdot; \rvvlambda_{t-1}\right)\).
  Under \cref{thm:bounded_weight}, an IMH kernel is uniformly geometrically ergodic~\citep{10.2307/2242610, wang_exact_2022} as
  \begin{align}
    \DTV{K_{\vlambda}^k\left(\vz, \cdot\right)}{\pi} \leq r^{k}\label{eq:imh_ergodicity}
  \end{align}
  for any \(\vz \in \mathcal{Z}\).

  \paragraph{Ergodicity of the Markov Chain}
  The state transitions of the Markov chain samples \(\vz^{(1:N)}\) are visualized as 
  {\small
  \begin{center}
  \bgroup
  \setlength{\tabcolsep}{3pt}
  \def\arraystretch{1.8}
  \begin{tabular}{c|ccccc}
   & \(\vz^{(1)}_t\) & \(\vz^{(2)}_t\) & \(\vz^{(3)}_t\) & \(\ldots\) &  \(\vz^{(N)}_t\) \\ \midrule
   \(t=1\) & \(K_{\vlambda_1}\left(\vz_0, d\vz_1^{(1)}\right)\) & \(K_{\vlambda_1}^2\left(\vz_0, d\vz_1^{(2)}\right)\) & \(K_{\vlambda_1}^3\left(\vz_0, d\vz_1^{(3)}\right)\) & \(\ldots\) & \(K_{\vlambda_1}^N\left(\vz_0, d\vz_1^{(N)}\right)\) \\
   \(t=2\) & \(K_{\vlambda_2}^{N + 1}\left(\vz_0, d\vz_2^{(1)}\right)\) & \(K_{\vlambda_2}^{N + 2}\left(\vz_0, d\vz_2^{(2)}\right)\) & \(K_{\vlambda_2}^{N + 3}\left(\vz_0, d\vz_2^{(3)}\right)\) & \(\ldots\) & \(K_{\vlambda_2}^{2\,N}\left(\vz_0, d\vz_2^{(N)}\right)\) \\
   \(\vdots\) & & & \(\vdots\) & & \\
   \(t=k\) & \(K_{\vlambda_k}^{\left(k-1\right)\,N + 1}\left(\vz_0, d\vz_k^{(1)}\right)\) & \(K_{\vlambda_k}^{\left(k-1\right)\,N + 2}\left(\vz_0, d\vz_k^{(2)}\right)\) & \(K_{\vlambda_k}^{\left(k-1\right)\,N + 3}\left(\vz_0, d\vz_k^{(3)}\right)\) & \(\ldots\) & \(K_{\vlambda_k}^{\left(k-1\right)\,N + N}\left(\vz_0, d\vz_k^{(N)}\right)\) \\
  \end{tabular}
  \egroup
  \end{center}
  }
  where \(K_{\vlambda}\left(\vz, \cdot\right)\) is an IMH kernel.
  Therefore, the \(n\)-step transition kernel for the vector of the Markov-chain samples \(\veta = \vz^{(1:N)}\) is represented as
  \begin{align*}
  P_{\vlambda}^k\left(\veta, d\veta^{\prime}\right)
  = 
  K_{\vlambda}^{N\,\left(k-1\right) + 1}\left(\vz_1, d\vz^{\prime}_1\right)
  \,
  K_{\vlambda}^{N\,\left(k-1\right) + 2}\left(\vz_2, d\vz^{\prime}_2\right)
  \cdot
  \ldots 
  \cdot
  K_{\vlambda}^{N\,\left(k-1\right) + N}\left(\vz_N, d\vz^{\prime}_N\right).
  \end{align*}

  Now, the convergence in total variation \(d_{\mathrm{TV}}\left(\cdot, \cdot\right)\) can be shown to decrease geometrically as
  \begin{alignat}{2}
    &\DTV{P_{\vlambda}^{k}\left(\veta, \cdot\right)}{\Pi}
    \nonumber
    \\
    &\quad=
    \sup_{A}
    \abs{
      \Pi\left(A\right)
      -
      P^{k}\left(\veta, A\right)
    }
    \nonumber
    \\
    &\quad\leq
    \sup_{A}
    \Bigg|
    \int_{A}
      \pi\left(d\vz^{\prime\;(1)}\right) \times \ldots \times \pi\left(d\vz^{\prime\;(N)}\right)
    \nonumber
    &&\quad\text{\textit{Definition of \(d_{\text{TV}}\)}}
      \\
      &\qquad\qquad\qquad-
      K^{(k-1)\,N\,+1}_{\vlambda}\left(\vz^{(1)}, d\vz^{\prime\;(1)}\right) \times \ldots \times K^{k\,N}_{\vlambda}\left(\vz^{(N)}, d\vz^{\prime\;(N)}\right)
    \,\Bigg|
    \nonumber
    \\
    &\quad\leq
    \sup_{A}
    \sum_{n=1}^N
    \abs{
    \int_{A}
      \pi\left(d\vz^{(n)}\right) - K^{(k-1)\,N + n}_{\vlambda}\left(\vz^{(n)}, d\vz^{\prime\;(n)}\right) 
    }
    &&\quad\text{\textit{\cref{thm:product_measure_bound}}}
    \nonumber
    \\
    &\quad=
    \sum_{n=1}^N
    \DTV{K^{(k-1)\,N + n}_{\vlambda}\left(\vz^{(n)}, \cdot\right)}{\pi}
    &&\quad\text{\textit{Definition of \(d_{\text{TV}}\)}}
    \nonumber
    \\
    &\quad\leq
    \sum_{n=1}^N
    r^{(k-1)\,N + n}
    &&\quad\text{\textit{\cref{eq:imh_ergodicity}}}
    \nonumber
    %\label{eq:used_ergodicity}
    \\
    &\quad=
    r^{k\,N}
    \,
    r^{-N}
    \,
    \frac{r - r^{N+1}}{1 - r}
    \nonumber
    \\
    &\quad=
    \frac{r \, \left(1 - r^N\right)}{r^N \left(1 - r\right)}
    \,
    {\left( r^{N} \right)}^k.
    \nonumber
  \end{alignat}
  Although the constant depends on \(r\) and \(N\), the kernel \(P\) is geometrically ergodic and converges \(N\) times faster than the base kernel \(K\).

  \paragraph{\textbf{Bound on the Gradient Variance}}
  To analyze the variance of the gradient, we require detailed information about the \(n\)-step marginal transition kernel, which is unavailable for most MCMC kernels.
  Fortunately, specifically for the IMH kernel,~\citet{Smith96exacttransition} have shown that the \(n\)-step marginal IMH kernel is given as~\cref{eq:imh_exact_kernel}.

  Furthermore, by \cref{thm:second_moment_bound}, the second moment of the gradient is bounded as
  \begin{alignat}{2}
    \E{ {\lVert \rvvg_{t,\text{JSA}} \rVert}_2^2 \,\middle|\, \mathcal{F}_{t-1} } 
    &=
    \V{\rvvg_{t,\text{JSA}} \mid \mathcal{F}_{t-1}} + {\mathsf{Bias}\left[\rvvg_{t,\text{JSA}} \mid \mathcal{F}_{t-1}\right]}^2 + 2\, \mathsf{Bias}\left[\rvvg_{t,\text{JSA}} \mid \mathcal{F}_{t-1}\right] \norm{\vmu}_2 + \norm{\vmu}^2_2
    \nonumber
    \\
    &\leq
    \V{ \rvvg_{t,\text{JSA}} \mid \mathcal{F}_{t-1}} + {\mathsf{Bias}\left[\rvvg_{t,\text{JSA}} \mid \mathcal{F}_{t-1}\right]}^2 + 2\,L\,\mathsf{Bias}\left[\rvvg_{t,\text{JSA}} \mid \mathcal{F}_{t-1}\right] + \norm{\vmu}^2_2,
    \nonumber
  \end{alignat}
  where \(\vmu = \mathbb{E}_{\pi}\vs\left(\vlambda; \rvvz\right)\).
  As shown in \cref{thm:mcmc_bias}, the bias terms decreases in a rate of \(r^{t\,N}\).
  Therefore, 
  \begin{alignat}{2}
    \E{ {\lVert \rvvg_{t,\text{JSA}} \rVert}_2^2 \,\middle|\, \mathcal{F}_{t-1} } 
    &\leq
    \V{ \rvvg_{t,\text{JSA}} \mid \mathcal{F}_{t-1}} + \norm{\vmu}^2_2 + \mathcal{O}\left(r^{t\,N}\right).
    &&\quad\text{\textit{\cref{thm:mcmc_bias}}}
    \nonumber
  \end{alignat}
  Note that it is possible to obtain a tighter bound on the bias terms such that \(\mathcal{O}\left(r^{t\,N}/N\right)\), if we directly use \(\left(K, \rvvz\right)\) to bound the bias instead of the higher-level \(\left(P, \rvveta\right)\) abstraction.
  The extra looseness comes from the use of \cref{thm:product_measure_bound}.

  For the variance term, we show that
  \begin{alignat}{2}
    &\V{ \rvvg_{t,\mathrm{JSA}} \,\middle|\, \mathcal{F}_{t-1} }
    \nonumber
    \\
    &\;=
    \V{\frac{1}{N}\,\sum^{N}_{n=1}\vs\left(\vlambda; \rvvz^{(n)}_t\right) \,\middle|\, \mathcal{F}_{t-1} }
    \nonumber
    \\
    &\;=
      \frac{1}{N^2}\,
      \sum^{N}_{n=1}
      \V{
        \vs\left(\vlambda; \rvvz^{(n)}_t\right)
        \,\middle|\,
        \mathcal{F}_{t-1}
      }
      +
      \frac{2}{N^2}
      \sum^{N}_{n=2}
      \sum^{n-1}_{m=1}
      \Cov{
        \vs\left(\vlambda; \rvvz^{(n)}_t\right),
        \vs\left(\vlambda; \rvvz^{(m)}_t\right)
        \,\middle|\,
        \mathcal{F}_{t-1}
      }
    \nonumber
    \\
    &\;\leq
    \frac{1}{N^2}\sum^{N}_{n=1} \E{ \norm{ \vs\left(\vlambda; \rvvz^{(n)}_t\right) }^2_2 \,\middle|\, \mathcal{F}_{t-1} } + C_{\text{cov}}
    \nonumber
    \\
    &\;=
    \frac{1}{N^2}\sum^{N}_{n=1} \E{ \norm{ \vs\left(\vlambda; \rvvz^{(n)}_t\right) }^2_2 \,\middle|\, \rvvz_{t-1}^{(N)},\, \rvvlambda_{t-1} } + C_{\text{cov}}
    \nonumber
    \\
    &\;=
    \frac{1}{N^2}\sum^{N}_{n=1} \Esub{\rvvz^{(n)}_t \sim K^n_{\rvvlambda_{t-1}}\left(\vz_{t-1}, \cdot\right)}{ \norm{\vs\left(\vlambda; \rvvz^{(n)}_t\right) }^2_2 \,\middle|\,  \rvvz_{t-1}^{(N)},\, \rvvlambda_{t-1} }
    + 
    C_{\text{cov}}
    \nonumber
    \\
    &\;\leq
    \frac{1}{N^2}\sum^{N}_{n=1}
    \left[\,
      n \, {r}^{n-1}
      \Esub{\rvvz \sim q_{\text{def.}}\left(\cdot; \rvvlambda_{t-1}\right)}{ \norm{\vs\left(\vlambda; \rvvz \right)}^2_2 }
      + 
      {r}^{n} \, \norm{\vs\left(\vlambda; \rvvz_{t-1}^{(N)} \right)}^2_2
      \,\right]
      +
    C_{\text{cov}}
    &&\quad\text{\textit{\cref{thm:imh_expecation}}}
    \nonumber
    \\
    &\;\leq
    \frac{1}{N^2}\sum^{N}_{n=1}
    \left[\,
      n \, {r}^{n-1}  \, L^2
      +
      {r}^{n} \, L^2
      \,\right]
      +
    C_{\text{cov}}
    &&\quad\text{\textit{\cref{thm:bounded_score}}}
    \nonumber
    \\
    &\;=
    \frac{L^2}{N^2}\sum^{N}_{n=1}
    \left[\,
      n \, {\left(1 - \frac{1}{w^*}\right)}^{n-1}
      +
      {\left(1 - \frac{1}{w^*}\right)}^{n} 
      \,\right]
      +
    C_{\text{cov}}
    \nonumber
    \\
    &\;=
    \frac{L^2}{N^2} \,
    \left[\,
      {\left(w^*\right)}^2 + w^*
      -
      {\left(1 - \frac{1}{w^*}\right)}^N
      \left(
        {\left(w^*\right)}^2 + w^* + N\,w^*
      \right)
    \,\right]
    +
    C_{\text{cov}}
    \nonumber
    \\
    &\;=
    \frac{L^2}{N^2} \,
    \left[\,
    \frac{1}{2} N^2 + \frac{3}{2}\,N 
    + \mathcal{O}\left(1/w^*\right)
    \,\right]
    +
    C_{\text{cov}}
    \nonumber
    &&\quad\text{\textit{Laurent series expansion at \(w^* \rightarrow \infty\)}}
    \\
    &\;=
    L^2 \,
    \left[\,
    \frac{1}{2} + \frac{3}{2}\,\frac{1}{N}
    + \mathcal{O}\left(1/w^*\right)
    \,\right] + C_{\text{cov}},
    \nonumber
  \end{alignat}
  where \[
  C_\text{cov} = {\frac{2}{N^2} \sum^{N}_{n=2} \sum^{n-1}_{m = 1} \Cov{\vs\left(\vlambda; \rvvz^{(n)}_t\right), \vs\left(\vlambda; \rvvz^{(m)}_t\right) \,\middle|\, \rvvz_{t-1}^{(N)},\, \rvvlambda_{t-1} }}.\]
  The Laurent approximation becomes exact as \(w^* \rightarrow \infty\), which is useful considering \cref{thm:wstar}.
\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
