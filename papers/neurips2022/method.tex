
\vspace{-0.05in}
\section{Markov Chain Score Ascent}
\vspace{-0.05in}

In this section, we provide a formal analysis of the two existing approaches to inclusive KL minimization, MSC~\citet{NEURIPS2020_b2070693} and JSA~\citet{pmlr-v124-ou20a}.
To achieve this, we first develop Markov Chain Score Ascent (MCSA), a framework for inclusive KL minimization with MCGD.
Then, we show that MSC and JSA are special cases of this framework.
By analyzing the ergodic convergence rate and gradient variance for MSC and JSA, we then analyze the non-asymptotic convergence of these methods.

\vspace{-0.05in}
\subsection{Markov Chain Score Ascent as a special case of MCGD}\label{section:convergence}

\vspace{-0.05in}
As shown in~\cref{eq:mcgd}, the basic ingredients of MCGD are the target function \(f\left(\vlambda, \eta\right)\), the gradient estimator \(\vg\left(\vlambda, \eta\right)\), and the Markov chain kernel \(P_{\vlambda}\left(\eta, \cdot\right)\).
Obtaining MCSA through MCGD boils down to designing \(\vg\) and \(P_{\vlambda}\) such that \(f\left(\vlambda\right) = \DKL{\pi}{q\left(\cdot; \vlambda\right)} \).
In the following theorem, we provide practical conditions for setting \(g\) and \(P_{\vlambda}\) such that the MCGD steps in~\cref{eq:mcgd} results in MCSA.

\input{thm_product_kernel}

As we will show, this framework is general enough to include both JSA and MSC.
We will later propose a third novel scheme that conforms to~\cref{thm:product_kernel}.
Note that \(N\) here can be regarded as the computational budget of each MCGD iteration since the cost of
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item generating the Markov chain samples \(\vz^{(1)}, \ldots, \vz^{(N)}\) and
  \item computing the gradient \(\vg\)
\end{enumerate*}
will linearly increase with \(N\).

In addition, the MCGD framework often assumes \(P\) to be geometrically ergodic.
An exception is~\citet{debavelaere_convergence_2021} where they operate with polynomially ergodic kernels.
\begin{assumption}{(Markov chain kernel)}\label{thm:kernel_conditions}
\vspace{-0.05in}
  The Markov chain kernel \(P\) is geometrically ergodic as
  {%\small
  \[
  \DTV{P_{\vlambda}^{n}\left(\veta, \cdot\right)}{ \Pi } \leq C \, \rho^{n}
  \]
  }
  for some positive constant \(C\).
\end{assumption}
\vspace{-0.05in}

\vspace{-0.1in}
\paragraph{Non-Asymptotic Convergence Rates}
Based on the conclusion of \cref{thm:product_kernel,thm:kernel_conditions} and additional technical assumptions on the objective function, we can apply the existing convergence results of MCGD to MCSA.
\cref{table:convergence} provides a brief list of some relevant results.
We omitted terms resulting from assumptions on the objective function, such as Lipschitz smoothness.
Instead, we focus on the terms including the gradient variance (\(G\)) and mixing rate (\(\rho\)) since they are closely related to the algorithmic design of MCSA algorithms.

\vspace{-0.1in}
\paragraph{Convergence and Mixing Rate}
\citet{duchi_ergodic_2012} was first to provide an analysis on the general MCGD setting.
Their convergence rate is dependent on the mixing rate through the \(1 / \log \rho^{-1}\) term.
For MCSA, this results in an overly conservative rate since, on challenging problems, mixing can be slow such that \(\rho \approx 1\).
Fortunately, \citet{doan_convergence_2020,doan_finitetime_2020} have recently shown that it is possible to obtain a rate independent of the mixing rate \(\rho\).
For example, in the result of~\citet{doan_finitetime_2020}, the influence of \(\rho\) decreases in a rate of \(\mathcal{O}\left(\nicefrac{1}{T^2}\right)\).
%The fact that the convergence rate can be independent of the mixing rate is critical.
This obervation is important since it means trading gradient variance and mixing rate could be profitable.

\begin{table*}
\vspace{-0.2in}
\centering
\caption{Convergence Rates of MCGD Algorithms}\label{table:convergence}
\setlength{\tabcolsep}{3pt}
\begin{threeparttable}
  \begin{tabular}{lllcc}\toprule
    \multicolumn{1}{c}{\footnotesize\textbf{Algorithm}} & \multicolumn{1}{c}{\footnotesize\textbf{Stepsize Rule}} & \multicolumn{1}{c}{\footnotesize\textbf{Gradient Assumption}} & {\footnotesize\textbf{Rate}} & {\footnotesize\textbf{Reference}} \\\midrule
    \multirow{2}{*}{\small Mirror Descent\tnote{1}}
    & \multirow{2}{*}{\small\(\gamma_t = \gamma / \sqrt{t}\)}
    & \multirow{2}{*}{\small\(\E{ {\|\, \vg\left(\vlambda, \veta\right) \,\|}_*^2 \mid \mathcal{F}_t } < G^2\)}
    & \multirow{2}{*}{\small\(\mathcal{O}\left(\frac{G^2 \log T}{ \log \rho^{-1} \sqrt{T}}\right)\)}
    & {\footnotesize\citet{duchi_ergodic_2012}}
    \\
    &&&& {\footnotesize{Corollary 3.5}}
    \\\cdashlinelr{1-5}
    \multirow{2}{*}{\small SGD-Nesterov\tnote{2}}
    & {\small\(\gamma_t = 2/(t + 1)\)}
    & \multirow{2}{*}{\footnotesize\( {\|\vg\left(\vlambda, \veta\right)\|}_2 < G \)}
    & \multirow{2}{*}{\small\(\mathcal{O}\left(\frac{G^2 \log T}{ \sqrt{T}}\right)\)}
    & {\footnotesize\citet{doan_convergence_2020}}
    \\
    & {\footnotesize\(\beta_t = \frac{1}{2 \, L \sqrt{t + 1}}\)}
    &&& {\footnotesize{Theorem 2}}
    \\\cdashlinelr{1-5}
    \multirow{2}{*}{\small SGD\tnote{3}}
    & {\footnotesize\(\gamma_t = \gamma/t\)}
    & \multirow{2}{*}{\footnotesize\( {\|\,\vg\left(\vlambda, \veta\right)\|}_* < G \left( \norm{\vlambda}_2 + 1 \right) \)}
    & \multirow{2}{*}{\small\(\mathcal{O}\left(\frac{G^2 \log T}{ T}\right)\)}
    & {\footnotesize\citet{doan_finitetime_2020}}
    \\ 
    & {\footnotesize\(\gamma = \min\{\nicefrac{1}{2\,L}, \nicefrac{2 L}{\mu}\}\)}
    &&& {\footnotesize{Theorem 1,2}}
    \\ \bottomrule
  \end{tabular}
  \begin{tablenotes}[flushleft]
  \item[]{%
    \footnotesize\textit{\textbf{Notation}}: \(^1\)\(\mathcal{F}_t\) is the \(\sigma\)-field formed by all the iterates \(\veta_t\), \(\vlambda_t\) up to the \(t\)th SGD iteration and \(\norm{\vx}_*\) is the dual norm such that \(\norm{\vx}_* = \sup_{\norm{\vz} \leq 1} \iprod{\vx}{\vz}\).
    \(^2\)\(\beta_t\) is the stepsize of the momentum.
    \(^2\)\(^3\)\(L\) is the Lipschitz smoothness constant.
    \(^3\)\(\mu\) is the strong convexity constant.
  }
  \end{tablenotes}
\end{threeparttable}
\vspace{-0.2in}
\end{table*}

\vspace{-0.1in}
\paragraph{Gradient Bound Assumption}
Except for \citet{doan_finitetime_2020}, most results assume that the gradient is bounded for \(\forall\veta,\vlambda\) as {\footnotesize\( {\| \vg\left(\vlambda, \veta\right) \|} < G \)}.
Admidttedly, this condition is strong for MCSA, but it is similar to the bounded variance assumption {\footnotesize\(\mathbb{E}\,[\norm{\vg}^2]  < G^2\)} used in vanilla SGD, which is also known to be strong since it does not hold for strongly convex objectives~\citep{pmlr-v80-nguyen18c}.
Nonetheless, assuming the existence of \(G\) can lead to useful analysis with practical benefits.
For example, it can be used to compare the performance of different algorithms as done by~\citet{pmlr-v108-geffner20a}.
In a similar spirit, we will obtain the gradient bound \(G\) of different MCSA algorithms and compare them.

\vspace{-0.05in}
\subsection{Analysis of Prior Markov Chain Score Ascent Methods}\label{section:comparison}
\vspace{-0.05in}
In this section, we will show that MSC and JSA both qualify as MCSA through~\cref{thm:product_kernel}.
Furthermore, we establish 
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item the geometric convergence rate of their implicitly defined kernel \(P\) and
  \item the upper bound on their gradient variance.
\end{enumerate*}
To do this, we use the following assumptions.
\begin{assumption}{(Bounded importance weight)}\label{thm:bounded_weight}
  The importance weight ratio \(w\left(\vz\right) = \pi\left(\vz\right) / q\left(\vz; \vlambda\right)\) is bounded by some finite constant as \(w^* < \infty\) for all \(\vlambda \in \Lambda\) such that \(\rho = \left(1 - 1/w^*\right) < 1\).
\end{assumption}
\vspace{-0.05in}
The fact that \(w^*\) exists for all \(\vlambda \in \Lambda\) is restrictive and not relevant in practice.
It does help excaping slow mixing regions in the initial MCGD steps, but this benefit vanishes quickly as we converge.
However, ensuring boundedness is necessary for theoretically analyzing MCGD through~\cref{thm:kernel_conditions}.
Although previous MCSA methods did not take specific measures to ensure~\cref{thm:bounded_weight}, this can be done by using a variational family with heavy tails~\citep{NEURIPS2018_25db67c5} or using a defensive mixture~\citep{hesterberg_weighted_1995, holden_adaptive_2009} as
\begin{align*}
  q_{\text{def.}}\left(\vz; \vlambda \right) = w \, q\left(\vz; \vlambda\right) + (1 - w) \, \nu\left(\vz\right)
\end{align*}
where \(0 < w < 1\) and \(\nu\left(\cdot\right)\) is a heavy tailed distribution that satisfies \(\sup_{\vz \in \mathcal{Z}} \pi\left(\vz\right) / \nu\left(\vz\right) < \infty\).
Note that \(q_{\text{def.}}\) is only used in the Markov chain kernels, which does not restrict our choice of the variational family.

\begin{assumption}{(Bounded Score)}\label{thm:bounded_score}
  The score gradient is bounded for \(\forall \vlambda \in \Lambda\) and \(\forall \vz \in \mathcal{Z}\) such that \(\norm{\vs\left(\vlambda; \vz\right)}_* \leq L \) for some finite constant \(L > 0\).
\end{assumption}
\vspace{-0.05in}
This assumption enables us to compare the gradient variance of different MCSA variants.

\vspace{-0.1in}
\paragraph{Markovian Score Climbing}
MSC is a simple instance of MCSA where \(\eta_t = \vz_t\) and \(P_{\vlambda_t} = K_{\vlambda_t}\), \(K_{\vlambda_t}\) is the conditional importance sampling (CIS) kernel (originally proposed as the iterated sequential importance resampling kernel by~\citet{andrieu_uniform_2018}).
See \cref{alg:msc} for a detailed pseudocode.
Although MSC uses only a single sample such that \(N=1\), the CIS kernel internally uses \(N\) proposals to generate a single sample.
Therefore, \(N\) in MSC has a different meaning, but it still indicates the computational budget.

\input{lemmas_previous_mcsa}
\input{thm_previous_msc}

\vspace{-0.1in}
\paragraph{Joint Stochastic Approximation}
JSA~\citep{pmlr-v124-ou20a} was originally proposed for deep generative models where the likelihood can be factorized by each datapoint.
In this setting,~\citeauthor{pmlr-v124-ou20a} perform minibatching by using a random-scan verion of the independent Metropolis-Hastings (IMH, \citealt{hastings_monte_1970,robert_monte_2004}) kernel.
We use the general verion of JSA with the vanilla IMH kernel which can be applied to non-factorizable likelihoods.
At each MCGD step, JSA performs multiple MCMC transitions and estimates the gradient by averaging all the intermediate samples.
See \cref{alg:jsa} for a detailed pseudocode.

\vspace{-0.1in}
\paragraph{Independent Metropolis-Hastings}
%A key element of JSA~\citep{pmlr-v124-ou20a} is that it uses the IMH kernel.
The IMH kernel used in JSA generates the proposals from the variational approximation \(q\left(\cdot; \vlambda_t\right)\).
To show geometric ergodicity of the implicit kernel \(P\), we utilize the geometric convergence rate of IMH kernels provided by~\citet[Theorem 2.1]{10.2307/2242610} and~\citet{wang_exact_2020}.
Furthermore, to derive an upper bound on the gradient variance, we use the exact \(n\)-step marginal IMH kernel derived by~\citet{Smith96exacttransition} as
{%\small
  \begin{align}
  K^n_{\vlambda}\left(\vz, d\vz\prime\right) 
  = T_n\left(\, w\left(\vz\right) \vee w\left(\vz\prime\right)\,\right) \, \pi\left(\vz\prime\right) \, d\vz\prime
  + \lambda^n\left(w\left(\vz\right)\right) \, \delta_{\vz}\left(d\vz\prime\right)
  \label{eq:imh_exact_kernel}
  \end{align}
}%
where {\(w\left(\vz\right) = \pi\left(\vz\right)/q_{\text{def.}}\left(\vz; \vlambda\right)\), \(x \vee y = \max\left(x, y\right)\)},
{\small
  \begin{align}
    T_n\left(w\right)      = \int_w^{\infty}
    \frac{n}{v^2}
    %\left(n / v^2\right)
    \, \lambda^{n-1}\left(v\right)\,dv,
    \quad\text{and}\quad
    \lambda\left(w\right) =
    \int_{R\left(w\right)}
    \left( 1 - \frac{w\left(\vz\prime\right)}{w}  \right)
    %\left( 1 - w\left(\vz\prime\right)/w  \right)
    \pi\left(d\vz\prime\right)\label{eq:T_lambda}
  \end{align}
}
for {\(R\left(w\right) = \{\, \vz\prime \mid w\,\left(\vz\prime\right) \leq w \,\}\)}.
%
\input{lemmas_imh}
\input{thm_previous_jsa}
%

\section{Parallel Markov Chain Score Ascent}
In this section, we turn to leveraging our new understanding of MCSA methods to develop a novel scheme with improved theoretical performance.

\subsection{Parallel Markov Chain Score Ascent}

\jrg{can we motivate this more strongly? Specifically, can we say that, by tying MCSA theory to MCGD theory, we can observe that the mixing rate isn't as important, so we develop a new scheme to exploit this previously not understood property.} In partiuclar, our JSA gradient bound suggests that increasing \(N\) does not improve variance, which significantly limits its applicability.
To fix this limitation, we propose a new MCSA scheme we call \textit{parallel MCSA} (pMCSA) that always achieves \(\mathcal{O}\left(\nicefrac{1}{N}\right)\) variance reduction.
Instead of using \(N\) \textit{sequential} Markov chain states, pMCSA operates \(N\) parallel Markov chains with.
At each step, pMCSA performs only a single Markov-chain transition for each chain.
This results in a per-SGD-iteration cost comparable to JSA.
We will later discuss the computational costs in detail.
See~\cref{alg:pmcsa} for a detailed pseudocode.

\input{thm_parallel_estimator}

%% This suggests that our proposed scheme achieves clear \(\mathcal{O}\left(1/N\right)\) variance reduction with a slower mixing rate.
%% The mixing rate independent bounds in \cref{table:convergence} suggest that this would provide faster convergence regardless.
%% Our empirical results in \cref{section:eval} show that this is indeed true in practice.

\vspace{-0.1in}
\paragraph{Theoretical Performance of MSC, JSA, and pMCSA}
By combining~\cref{thm:msc,thm:jsa,thm:pmcsa} with the convergence rates in~\cref{table:convergence}, we can compare the theoretical convergence of the considered algorithms.
For MSC, when \(w^*\) is large, the gradient variance and the mixing rate are worse than JSA and pMCSA and cannot be improved by increasing \(N\).
Therefore, we will not discuss it further.
On the other hand, JSA and pMCSA can be seen as trading-off bias (mixing rate) for variance.
However, when \(w^*\) is large, we can expect JSA to perform worse than pMCSA due to the constant \(1/2\).

\vspace{-0.1in}
\paragraph{Effect of increasing \(N\)}
If we consider the convergence rate of~\citet{duchi_ergodic_2012}, the mixing rate affects the convergence rate through the \(\log \rho^{-1}\) term.
For JSA, the combined rate appears to worsen as we increase \(N\), while for pMCSA, the combined rate stays constant with respect to \(N\).
In practice, however, we observe that increasing \(N\) accelerates convergence in general (quite dramatically for pMCSA).
Therefore, the mixing rate independent convergence rates by~\citet{doan_finitetime_2020, doan_convergence_2020} appears to better reflect practical performance.
This is especially true in MCSA since
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item the mixing rate \(\rho\) is a conservative \textit{global} bound and 
  \item the mixing rate will improve as MCSA converges.
\end{enumerate*}
Unfortunately, it is challenging to incorporate the changes in the mixing rate into the convergence rate.

%% \paragraph{Bias v.s. Variance}
%% While our proposed scheme achievs superior variance reduction, the mixing rate is worse.
%% In a MCMC estimation perspective, this translates into higher bias.
%% However, we note that
%% \begin{enumerate*}[label=\textbf{(\roman*)}]
%%   \item the constant \(C\left(\rho, N\right)\) depends on \(\rho\), 
%%   \item all of the ergodic convergence rate are close to 1 as \(w^* \rightarrow \infty\), and
%%   \item the mixing rate is a conservative global bound with respect to \(\vlambda\).
%% \end{enumerate*}
%% Therefore, in general, the superior ergodic convergence rate of JSA does not translate into faster convergence of MCSA.
%% In fact, as MCSA converges, \(w^*\) also decreases, dramatically improving the mixing rate.
%% In contrast, the relative variance does not improve too much with \(w^*\).
%% Therefore, reducing the variance is much more effective for accelerating convergence.
%% We empirically show this fact on the bias and variance in~\cref{section:simulation}.

\begin{wraptable}{r}{0.55\textwidth}
  \centering
  \vspace{-0.5in}
  \input{table_cost}
  \vspace{-0.2in}
\end{wraptable}
%
\vspace{-0.05in}
\subsection{Computational Cost}
\vspace{-0.05in}
The three schemes using the CIS kernel and the IMH kernel can have different computational costs depending on the parameter \(N\).
The computational costs of each scheme are organized in~\cref{table:cost}.

\vspace{-0.1in}
\paragraph{Cost of Sampling Proposals}
For the CIS kernel used by MSC, \(N\) controls the number of internal proposals sampled from \(q\,(\vz; \vlambda)\).
For JSA and pMCSA, the IMH kernel only uses a single sample from \(q\,(\vz; \vlambda)\), but applies the kernel \(N\) times.
Assuming caching is done as much as possible, pMCSA needs twice more evaluations of \(q\,(\vz; \vlambda)\) compared to other methods.
However, in general, this added cost should be minimal compared to the cost of evaluating \(p\,(\vz,\vx)\).

\vspace{-0.1in}
\paragraph{Cost of Estimating the Score}
When estimating the score, MSC computes \(\nabla_{\vlambda} \log q\,(\vz; \vlambda)\) only once, while JSA and our proposed scheme compute it \(N\) times.
However,~\cite{NEURIPS2020_b2070693} also discuss a Rao-Blackwellized version of the CIS kernel, which also computes the gradient \(N\) times.
Lastly, notice that MCSA methods do not need to differentiate through the likelihood, unlike ELBO maximization, making its per-iteration cost significantly cheaper.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
