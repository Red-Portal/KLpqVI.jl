
\vspace{-1.5ex}
\section{Markov Chain Score Ascent}\label{section:mcsa}
\vspace{-1.0ex}

First, we develop Markov chain score ascent (MCSA), a framework for inclusive KL minimization with MCGD.
This framework will establish the connection between MSC/JSA and MCGD.

\vspace{-1.ex}
\subsection{Markov Chain Score Ascent as a Special Case of Markov Chain Gradient Descent}\label{section:convergence}
\vspace{-1.ex}
As shown in~\cref{eq:mcgd}, the basic ingredients of MCGD are the target function \(f\left(\vlambda, \eta\right)\), the gradient estimator \(\vg\left(\vlambda, \eta\right)\), and the Markov chain kernel \(P_{\vlambda}\left(\eta, \cdot\right)\).
Obtaining MCSA from MCGD boils down to designing \(\vg\) and \(P_{\vlambda}\) such that \(f\left(\vlambda\right) = \DKL{\pi}{q\left(\cdot; \vlambda\right)} \).
The following proposition provides sufficient conditions on \(\vg\) and \(P_{\vlambda}\) to achieve this goal.

\vspace{0.07in}
\input{thm_product_kernel}

\begin{table*}[t]
\vspace{-1ex}
\centering
\caption{Convergence Rates of MCGD Algorithms}\label{table:convergence}
\vspace{-0.05in}
\setlength{\tabcolsep}{3pt}
\begin{threeparttable}
  \begin{tabular}{lllcc}\toprule
    \multicolumn{1}{c}{\footnotesize\textbf{Algorithm}} & \multicolumn{1}{c}{\footnotesize\textbf{Stepsize Rule}} & \multicolumn{1}{c}{\footnotesize\textbf{Gradient Assumption}} & {\footnotesize\textbf{Rate}} & {\footnotesize\textbf{Reference}} \\\midrule
    \multirow{2}{*}{\small Mirror Descent\tnote{1}}
    & \multirow{2}{*}{\small\(\gamma_t = \gamma / \sqrt{t}\)}
    & \multirow{2}{*}{\small\(\E{ {\|\, \vg\left(\rvvlambda, \rvveta\right) \,\|}_*^2 \mid \mathcal{F}_{t-1} } < G^2\)}
    & \multirow{2}{*}{\small\(\mathcal{O}\left(\frac{G^2 \log T}{ \log \rho^{-1} \sqrt{T}}\right)\)}
    & {\footnotesize\citet{duchi_ergodic_2012}}
    \\
    &&&& {\footnotesize{Corollary 3.5}}
    \\\cdashlinelr{1-5}
    \multirow{2}{*}{\small SGD-Nesterov\tnote{2}}
    & {\small\(\gamma_t = 2/(t + 1)\)}
    & \multirow{2}{*}{\footnotesize\( {\|\vg\left(\rvvlambda, \rvveta\right)\|}_2 < G \)}
    & \multirow{2}{*}{\small\(\mathcal{O}\left(\frac{G^2 \log T}{ \sqrt{T}}\right)\)}
    & {\footnotesize\citet{doan_convergence_2020}}
    \\
    & {\footnotesize\(\beta_t = \frac{1}{2 \, L \sqrt{t + 1}}\)}
    &&& {\footnotesize{Theorem 2}}
    \\\cdashlinelr{1-5}
    \multirow{2}{*}{\small SGD\tnote{3}}
    & {\footnotesize\(\gamma_t = \gamma/t\)}
    & \multirow{2}{*}{\footnotesize\( {\|\,\vg\left(\rvvlambda, \rvveta\right)\|}_* < G \left( \norm{\vlambda}_2 + 1 \right) \)}
    & \multirow{2}{*}{\small\(\mathcal{O}\left(\frac{G^2 \log T}{ T}\right)\)}
    & {\footnotesize\citet{doan_finitetime_2020}}
    \\ 
    & {\footnotesize\(\gamma = \min\{\nicefrac{1}{2\,L}, \nicefrac{2 L}{\mu}\}\)}
    &&& {\footnotesize{Theorem 1,2}}
    \\ \bottomrule
  \end{tabular}
  \begin{tablenotes}[flushleft]
  \item[]{%
    \footnotesize\textbf{\textit{Notation}:}
    \(^1\)\(\mathcal{F}_t\) is the \(\sigma\)-field formed by the iterates \(\veta_t\) up to the \(t\)th MCGD iteration, \(\norm{\vx}_* \) is the dual norm of \(\vx\);
    \(^2\)\(\beta_t\) is the stepsize of the momentum;
    \(^2\)\(^3\)\(L\) is the Lipschitz smoothness constant;
    \(^3\)\(\mu\) is the strong convexity constant.
  }
  \end{tablenotes}
\end{threeparttable}
\vspace{-2ex}
\end{table*}

This simple connection between MCGD and VI paves the way toward the non-asymptotic analysis of JSA and MSC.
%We will later propose a third novel scheme that conforms to~\cref{thm:product_kernel}.
Note that \(N\) here can be regarded as the computational budget of each MCGD iteration since the cost of
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item generating the Markov chain samples \(\vz^{(1)}, \ldots, \vz^{(N)}\) and
  \item computing the gradient \(\vg\)
\end{enumerate*}
will linearly increase with \(N\).

In addition, the MCGD framework often assumes \(P\) to be geometrically ergodic.
An exception is the analysis of~\citet{debavelaere_convergence_2021} where they work with polynomially ergodic kernels.
\begin{assumption}{(Markov chain kernel)}\label{thm:kernel_conditions}
\vspace{-0.05in}
  The Markov chain kernel \(P\) is geometrically ergodic as
  {%\small
  \[
  \DTV{P_{\vlambda}^{n}\left(\veta, \cdot\right)}{ \Pi } \leq C \, \rho^{n}
  \]
  }
  for some positive constant \(C\).
\end{assumption}

\vspace{-2ex}
\subsection{Non-Asymptotic Convergence of Markov Chain Score Ascent}%\label{}
\vspace{-1.5ex}
\paragraph{Non-Asymptotic Convergence}
Through~\cref{thm:product_kernel},~\cref{thm:kernel_conditions} and some technical assumptions on the objective function, we can apply the existing convergence results of MCGD to MCSA.
\cref{table:convergence} provides a list of relevant results.
Apart from properties of the objective function (such as Lipschitz smoothness), the convergence rates are stated in terms of the gradient bound \(G\), kernel mixing rate \(\rho\), and the number of MCGD iterations \(T\).
We focus on \(G\) and \(\rho\) as they are closely related to the design choices of different MCSA algorithms.

\vspace{-0.1in}
\paragraph{Convergence and the Mixing Rate \(\rho\)}
\citet{duchi_ergodic_2012} was the first to provide an analysis of the general MCGD setting.
Their convergence rate is dependent on the mixing rate through the \(1 / \log \rho^{-1}\) term.
For MCSA, this result is overly conservative since, on challenging problems, mixing can be slow such that \(\rho \approx 1\).
Fortunately, \citet{doan_convergence_2020,doan_finitetime_2020} have recently shown that it is possible to obtain a rate independent of the mixing rate \(\rho\).
For example, in the result of~\citet{doan_finitetime_2020}, the influence of \(\rho\) decreases in a rate of \(\mathcal{O}\left(\nicefrac{1}{T^2}\right)\).
%The fact that the convergence rate can be independent of the mixing rate is critical.
This observation is critical since it implies that \textbf{trading a ``slower mixing rate'' for ``lower gradient variance'' could be profitable}.
We exploit this observation in our novel MCSA scheme in~\cref{section:pmcsa}.

\vspace{-0.1in}
\paragraph{Gradient Bound \(G\)}
Except for \citet{doan_finitetime_2020}, most results assume that the gradient is bounded for \(\forall\veta,\vlambda\) as {\footnotesize\( {\| \vg\left(\vlambda, \veta\right) \|} < G \)}.
Admittedly, this condition is strong, but it is similar to the bounded variance assumption {\footnotesize\(\mathbb{E}\,[\norm{\vg}^2]  < G^2\)} used in vanilla SGD, which is also known to be strong as it contradicts strong convexity~\citep{pmlr-v80-nguyen18c}.
Nonetheless, assuming \(G\) can have practical benefits beyond theoretical settings.
For example,~\citet{pmlr-v108-geffner20a} use \(G\) to compare the performance different VI gradient estimators.
In a similar spirit, we will obtain the gradient bound \(G\) of different MCSA algorithms and compare their theoretical performance.

\vspace{-0.1in}
\section{Demystifying Prior Markov Chain Score Ascent Methods}\label{section:comparison}
\vspace{-0.1in}
In this section, we will show that MSC and JSA both qualify as MCSA methods.
Furthermore, we establish 
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item the mixing rate of their implicitly defined kernel \(P\) and
  \item the upper bound on their gradient variance.
\end{enumerate*}
This will provide insight into their practical non-asymptotic performance.

\vspace{-1ex}
\subsection{Technical Assumptions}\label{section:assumption}
\vspace{-1ex}
To cast previous methods into MCSA, we need some technical assumptions.

\begin{assumption}{(Bounded importance weight)}\label{thm:bounded_weight}
  The importance weight ratio \(w\left(\vz\right) = \pi\left(\vz\right) / q\left(\vz; \vlambda\right)\) is bounded by some finite constant as \(w^* < \infty\) for all \(\vlambda \in \Lambda\) such that \(\rho = \left(1 - 1/w^*\right) < 1\).
\end{assumption}
\vspace{-0.05in}
This assumption is necessary to ensure~\cref{thm:kernel_conditions}, and can be practically ensured by using a variational family with heavy tails~\citep{NEURIPS2018_25db67c5} or using a defensive mixture~\citep{hesterberg_weighted_1995, holden_adaptive_2009} as
%\vspace{-0.07in}
{\[
  q_{\text{def.}}\left(\vz; \vlambda \right) = \alpha \, q\left(\vz; \vlambda\right) + (1 - \alpha) \, \nu\left(\vz\right),
\]}%
%\vspace{-0.01in}}%
where \(0 < \alpha < 1\) and \(\nu\left(\cdot\right)\) is a heavy tailed distribution such that \(\sup_{\vz \in \mathcal{Z}} \pi\left(\vz\right) / \nu\left(\vz\right) < \infty\).
Note that \(q_{\text{def.}}\) is only used in the Markov chain kernels and \(q\left(\cdot;\vlambda\right)\) is still the output of the VI procedure.
While these tricks help escape slowly mixing regions, this benefit quickly vanishes as \(\lambda\) converges.
Therefore, ensuring \cref{thm:bounded_weight} seems unnecessary in practice unless we absolutely care about ergodicity (\textit{e.g.} adaptive MCMC,~\citealt{holden_adaptive_2009, pmlr-v151-brofos22a}).

\vspace{-0.1in}
\paragraph{\textbf{Model (Variational Family) Misspecification and \(w^*\)}}
Note that \(w^*\) is bounded below exponentially by the inclusive KL as shown in~\cref{thm:wstar}.
Therefore, \(w^*\) will be large 
\begin{enumerate*}[label=\textbf{(\roman*)}]
    \item in the initial steps of VI and
    \item under model (variational family) misspecification.
\end{enumerate*}
%In practice, for practical it is fair to expect \(w^*\) will be large.

\begin{assumption}{(Bounded Score)}\label{thm:bounded_score}
  The score gradient is bounded for \(\forall \vlambda \in \Lambda\) and \(\forall \vz \in \mathcal{Z}\) such that \(\norm{\vs\left(\vlambda; \vz\right)} \leq L \) for some finite constant \(L > 0\).
\end{assumption}
%
Although this assumption is strong, it enables us to compare the gradient variance of MCSA methods.
We empirically justify the bounds obtained using~\cref{thm:bounded_score} in \cref{section:simulation}.

%\pagebreak
\vspace{-0.07in}
\subsection{Markovian Score Climbing}
\vspace{-0.07in}

MSC (\cref{alg:msc} in \cref{section:pseudocode}) is a simple instance of MCSA where \(\eta_t = \vz_t\) and \(P_{\vlambda_t} = K_{\vlambda_t}\) is the conditional importance sampling (CIS) kernel (originally proposed by~\citet{andrieu_uniform_2018}) where the proposals are generated from \(q\left(\cdot; \vlambda_t\right)\).
Although MSC uses only a single sample for the Markov chain, the CIS kernel internally operates \(N-1\) proposals.
Therefore, \(N\) in MSC has a different meaning, but it still indicates the computational budget.

\input{lemmas_previous_mcsa}
\vspace{0.04in}
\input{thm_previous_msc}

\vspace{-0.1in}
\paragraph{Discussion}
\cref{thm:msc} shows that the gradient variance of MSC is insensitive to \(N\).
Although the mixing rate does improve with \(N\), when \(w^*\) is large due to model misspecification and lack of convergence (see the discussion in \cref{section:assumption}), this will be marginal.
Overall, \textit{the performance of MSC cannot be improved by increasing the computational budget \(N\)}.

\vspace{-0.1in}
\paragraph{Rao-Blackwellization}
\textcolor{blue}{
Meanwhile, \citeauthor{NEURIPS2020_b2070693} also provide a Rao-Blackwellized version of MSC we denote as MSC-RB.
Instead of selecting a single \(\vz_t\) by resampling over the \(N\) internal proposals, they suggest to form an importance-weighted estimate~\citep{robert_monte_2004}.
The theoretical properties of this estimator have been concurrently analyzed by~\citet{cardoso_brsnis_2022}.
}

\input{thm_previous_mscrb}

\textcolor{blue}{
The variance of MSC-RB decreases as \(\mathcal{O}\left(\nicefrac{1}{N-1}\right)\), which is more encouraging than vanilla MSC.
However, the first term depends on the \(\chi^2\) divergence, which is bounded below exponentially by the KL divergence~\citep{10.1214/17-STS611}, and the second term depends quadratically on \(w^*\).
Therefore, \textit{the variance of MSC-RB will be large on challenging problems such that \(w^*\) and the \(\chi^2\) divergence are large}, although linear variance reduction is possible.
}

\vspace{-0.1in}
\subsection{Joint Stochastic Approximation}
\vspace{-0.1in}
JSA (\cref{alg:jsa} in \cref{section:pseudocode}) was proposed for deep generative models where the likelihood factorizes into each datapoint.
Then, subsampling can be used through a random-scan version of the independent Metropolis-Hastings (IMH, \citealt{hastings_monte_1970}) kernel.
Instead, we consider the general version of JSA with a vanilla IMH kernel since it can be used for any type of likelihood.
At each MCGD step, JSA performs multiple Markov chain transitions and estimates the gradient by averaging all the intermediate states, which is closer to traditional MCMC.

\vspace{-0.1in}
\paragraph{Independent Metropolis-Hastings}
%A key element of JSA~\citep{pmlr-v124-ou20a} is that it uses the IMH kernel.
Similarly to MSC, the IMH kernel in JSA generates proposals from \(q\left(\cdot; \vlambda_t\right)\).
To show the geometric ergodicity of the implicit kernel \(P\), we utilize the geometric convergence rate of IMH kernels provided by~\citet[Theorem 2.1]{10.2307/2242610} and~\citet{wang_exact_2020}.
\textcolor{blue}{
The gradient variance, on the other hand, is difficult to analyze, especially the covariance between the samples.
However, we show that, even if we ignore the covariance terms, the variance reduction with respect to \(N\) is severly limited in the large \(w^*\) regime.
}
To do this, we use the exact \(n\)-step marginal IMH kernel derived by~\citet{Smith96exacttransition} as
{
  \begin{align}
  K^n_{\vlambda}\left(\vz, d\vz^{\prime}\right) 
  = T_n\left(\, w\left(\vz\right) \vee w\left(\vz^{\prime}\right)\,\right) \, \pi\left(\vz^{\prime}\right) \, d\vz^{\prime}
  + \lambda^n\left(w\left(\vz\right)\right) \, \delta_{\vz}\left(d\vz^{\prime}\right)
  \label{eq:imh_exact_kernel}
  \end{align}
}%
where {\(w\left(\vz\right) = \pi\left(\vz\right)/q_{\text{def.}}\left(\vz; \vlambda\right)\), \(x \vee y = \max\left(x, y\right)\)}, and for {\(R\left(v\right) = \{\, \vz^{\prime} \mid w\,\left(\vz^{\prime}\right) \leq v \,\}\)}, 
{%\small
  \begin{align}
    %\textstyle
    T_n\left(w\right)      = \int_w^{\infty}
    \frac{n}{v^2}
    %\left(n / v^2\right)
    \, \lambda^{n-1}\left(v\right)\,dv,
    \quad\text{and}\quad
    \lambda\left(w\right) =
    \int_{R\left(w\right)}
    \left( 1 - \frac{w\left(\vz^{\prime}\right)}{w}  \right)
    %\left( 1 - w\left(\vz^{\prime}\right)/w  \right)
    \pi\left(d\vz^{\prime}\right).\label{eq:T_lambda}
  \end{align}
}%
\input{lemmas_imh}
\input{thm_previous_jsa}
%
\vspace{-1.5ex}
\paragraph{Discussion}
As shown in~\cref{thm:jsa}, JSA benefits from increasing \(N\) in terms of a faster mixing rate.
\textcolor{blue}{
However,  under lack of convergence and model misspecification (large \(w^*\)), the variance improvement becomes marginal.
Specifically, in the large \(w^*\) regime, the variance reduction is limited by the constant \(1/2\) term.
This is true even ideally when the covariance between the samples is ignorable such that \(C_{\text{cov}} \approx 0\).
In practice, however, the covariance term \(C_{\text{cov}}\) will be positive, only increasing variance.
Therefore, \textit{in the large \(w^*\) regime, JSA will perform poorly, and the variance reduction by increasing \(N\) is fundamentally limited}.
}
%Furthermore, the inefficiency persists even under stationarity unless \(\pi\left(\cdot\right) \in \mathcal{Q}\) such that the rejection rate becomes exactly 0.

%Intuitively, the inefficiency comes from the rejections persisting even under stationarity.
%On challenging problems with a large \(w^*\), the performance of JSA will also be poor.

%
\begin{wrapfigure}[12]{r}{0.43\textwidth}
\vspace{-7ex}
\begin{minipage}[c]{0.43\textwidth}
  \begin{algorithm2e}[H]
    \DontPrintSemicolon
    \SetAlgoLined
    \KwIn{initial samples \(\vz_0^{(1)},\, \ldots,\, \vz_0^{(N)}\),\linebreak
      initial parameter \(\vlambda_0\), \linebreak
      number of iterations \(T\), \linebreak
      stepsize schedule \(\gamma_t\)
    }
    \For{\(t = 1, 2, \ldots, T\)}{
        \For{\(n = 1, 2, \ldots, N\)}{
          \(\vz^{(n)}_{t} \sim K_{\vlambda_{t-1}}(\vz^{(n)}_{t-1}, \cdot)\)\;
      }
      \( \vg\left(\vlambda\right) = -\frac{1}{N} \sum^{N}_{n=1} \vs\,(\vlambda; \vz_{t}^{(n)}) \)\;
      \( \vlambda_{t} = \vlambda_{t-1} - \gamma_t\, \vg\left(\vlambda_{t-1}\right) \)\;
    }
    \caption{pMCSA}\label{alg:pmcsa}
  \end{algorithm2e}
\end{minipage}
\end{wrapfigure}
%
\vspace{-1.ex}
\section{Parallel Markov Chain Score Ascent}\label{section:pmcsa}
\vspace{-1.5ex}
%\vspace{-0.12in}
Our analysis in~\cref{section:comparison} suggests that the statistical performance of MSC, MSC-RB, and JSA are heavily affected by model specification and the state of convergence through \(w^*\).
%This affects both the mixing rate \(\rho\) and the gradient variance \(G\).
Furthermore, for JSA, a large \(w^*\) abolishes our ability to counterbalance the inefficiency by increasing the computational budget \(N\).
However, \(\rho\) and \(G\) do not equally impact convergence; recent results on MCGD suggest that gradient variance is more critical than the mixing rate (see \cref{section:convergence}).
We turn to leverage this understanding to overcome the limitations of previous methods.

%\vspace{-0.1in}
\vspace{-1.ex}
\subsection{Parallel Markov Chain Score Ascent}
\vspace{-1ex}
We propose a novel scheme, \textit{parallel Markov chain score ascent} (pMCSA,~\cref{alg:pmcsa}), that embraces a slower mixing rate in order to consistently achieve an \(\mathcal{O}\left(\nicefrac{1}{N}\right)\) variance reduction, even on challenging problems with a large \(w^*\),  

%\jrg{can we motivate this more strongly? Specifically, can we say that, by tying MCSA theory to MCGD theory, we can observe that the mixing rate isn't as important, so we develop a new scheme to exploit this previously not understood property.} 

\vspace{-1.5ex}
\paragraph{Algorithm Description}
Unlike JSA that uses \(N\) \textit{sequential} Markov chain states, pMCSA operates \(N\) \textit{parallel} Markov chains.
To maintain a similar per-iteration cost with JSA, it performs only a single Markov chain transition for each chain.
Since the chains are independent, the Metropolis-Hastings rejections do not affect the variance of pMCSA.

\input{thm_parallel_estimator}

\vspace{-1.5ex}
\paragraph{Discussion}
Unlike JSA and MSC, the variance reduction rate of pMCSA is independent of \(w^*\).
Therefore, it should perform significantly better on challenging practical problems.
If we consider the rate of~\citet{duchi_ergodic_2012}, the combined rate is constant with respect to \(N\) since it cancels out.
In practice, however, we observe that increasing \(N\) accelerates convergence quite dramatically.
Therefore, the mixing rate independent convergence rates by~\citet{doan_finitetime_2020, doan_convergence_2020} appears to better reflect practical performance.
This is because
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item the mixing rate \(\rho\) is a conservative \textit{global} bound and 
  \item the mixing rate will improve naturally as MCSA converges.
\end{enumerate*}

\begin{wraptable}[11]{r}{0.55\textwidth}
  \vspace{-5ex}
  \centering
  \input{table_cost}
\end{wraptable}
%
\vspace{-1.5ex}
\subsection{Computational Cost Comparison}
\vspace{-1.5ex}
The three schemes using the CIS and IMH kernels have different computational costs depending on \(N\) as organized in~\cref{table:cost}.

\vspace{-1.5ex}
\paragraph{Cost of Sampling Proposals}
For the CIS kernel used by MSC, \(N\) controls the number of internal proposals sampled from \(q\,(\cdot; \vlambda)\).
For JSA and pMCSA, the IMH kernel only uses a single sample from \(q\,(\cdot; \vlambda)\), but applies the kernel \(N\) times.
On the other hand, pMCSA needs twice more evaluations of \(q\,(\cdot; \vlambda)\).
However, this added cost is minimal since it is dominated by that of evaluating \(p\,(\vz,\vx)\).

\vspace{-1.5ex}
\paragraph{Cost of Estimating the Score}
When estimating the score, MSC computes \(\nabla_{\vlambda} \log q\,(\vz; \vlambda)\) only once, while JSA and our proposed scheme compute it \(N\) times.
However,~\cite{NEURIPS2020_b2070693} also discuss a Rao-Blackwellized version of the CIS kernel, which also computes the score \(N\) times.
Lastly, notice that MCSA methods do not need to differentiate through the likelihood \(p\,(\vz,\vx)\), unlike ELBO maximization, making its per-iteration cost significantly cheaper.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
