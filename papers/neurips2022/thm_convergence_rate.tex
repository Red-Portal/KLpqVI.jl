
\begin{theoremEnd}[all end]{lemma}\label{thm:mixing_time}
  Assuming the kernel \(P_{\vlambda}\left(\veta, \cdot\right)\) satisfies \cref{thm:ergodicity}, the Hellinger mixing time \(\tau_{\text{Hel.}}\) is bounded as
  \begin{align*}
    \tau_{\text{Hel.}}\left(K_{\vlambda}, \epsilon\right) \leq  \frac{2}{\log \rho^{-1}} \log \frac{1}{\epsilon}
  \end{align*}
\end{theoremEnd}
\begin{proofEnd}
  The following two inequalities are equivalent.
  \begin{align*}
    d_{\text{Hel.}}\left(\, P^t_{\vlambda}\left(\veta, \cdot \right), \Pi \,\right)
    &\leq \sqrt{\DTV{ P^t_{\vlambda}\left(\veta, \cdot \right)}{\Pi}}
    \leq \rho^{t / 2}
    \\
    \log d_{\text{Hel.}}\left(\, P^t_{\vlambda}\left(\veta, \cdot \right), \Pi \,\right)
    &\leq \frac{t}{2} \log \rho
  \end{align*}

  The Hellinger mixing time \(\tau_{\text{Hel.}}\left(K_{\vlambda}, \epsilon\right)\) is the smallest \(t\) that statisfies the inequality
  \begin{align*}
    d_{\text{Hel.}}\left(\, P^t_{\vlambda}\left(\veta, \cdot \right), \Pi \,\right)
    &\leq 
    \epsilon.
  \end{align*}
  Instead, we can find the \(t\prime > t\) that satisfies
  \begin{align*}
    \log d_{\text{Hel.}}\left(\, P^t_{\vlambda}\left(\veta, \cdot \right), \Pi \,\right)
    \leq 
    \frac{t\prime}{2} \log \rho
    \leq 
    \log \epsilon
 \end{align*}
  by solving the inequalities
  \begin{alignat*}{2}
    \frac{t\prime}{2} \log \rho
    &\leq 
    \log \epsilon
    \\
    t\prime 
    &\geq 
    \frac{2}{\log \rho} \log \epsilon
    &&\quad\text{\textit{Inequality flipped since \(\log \rho \leq 1\)}}
    \\
    t\prime 
    &\geq 
    \frac{2}{\log \rho^{-1}} \log \frac{1}{\epsilon}
 \end{alignat*}
\end{proofEnd}

\begin{theoremEnd}{theorem}{(\textbf{Convergence rate})}\label{thm:convergence_rate}
  Assuming~\cref{thm:kernel_conditions,thm:gradient_estimator,thm:logconcave,thm:compact,thm:bounded_variance} hold, with a diminishing stepsize of \(\alpha_t = \alpha /  \sqrt{T} \), the average iterates {\small\(\overline{\vlambda}_T = \sum^{T}_{t=1} \vlambda_{t} / T\)} of Markov chain score ascent achieve a convergence rate of
  {%\small
  \begin{align*}
    \E{ \DKL{\pi}{q\,(\cdot; {\overline{\vlambda}_{T}})} - \DKL{\pi}{q\left(\cdot; {\vlambda^*}_{T}\right)}}
    =
    \mathcal{O}\left(
    \frac{
      \alpha \, G^2 \, \sqrt{\log T}
    }{
      \log \rho^{-1} \, \sqrt{T}
    } \right)  
  \end{align*}
  }
\end{theoremEnd}
\begin{proofEnd}
  \citet[Corollary 3.5]{duchi_ergodic_2012} provide a non-asymptotic convergence rate for the \textit{ergodic mirror descent} algorithm which computes the parameter update as
  \begin{align}
    \vlambda_{t+1} &= \argmin_{\vlambda \in \Lambda} \left\{\,
    \iprod{\,\vg\left(\vlambda, \veta_t\right)}{\vlambda\,} 
    +
    \frac{1}{\alpha_t} D_{\psi}\left(\vlambda, \vlambda_{t}\right)
    \,\right\}\label{eq:ergodic_mirror_descent}
    \\
    \rvveta_{t+1} &\sim P_{\vlambda_{t}}\left(\veta_t, \cdot\right)
    \nonumber
  \end{align}
  where \(D_{\psi}\) is the Bregman divergence defined as
  \begin{align*}
    D_{\psi}\left(\vlambda, \vlambda\prime\right)
    \triangleq
    \psi\left(\vlambda\right)
    - \psi\left(\vlambda\prime\right)
    - \iprod{ \nabla \psi\left(\vlambda\prime\right)}{\vlambda - \vlambda\prime}
  \end{align*}
  for some convex function \(\psi\).
  Our result is based on the fact that MCGD is a special case of the ergodic mirror descent algorithm.
  Specifically, by choosing \(\psi\left(\vlambda\right) = \frac{1}{2} \norm{\vlambda}^2_2 \), we obtain
  \begin{alignat}{2}
    D_{\psi}\left(\vlambda, \vlambda\prime\right) = \frac{1}{2} \norm{\vlambda - \vlambda\prime}_2^2 \leq \frac{1}{2} \, R^2.\label{eq:bregman}
    &&\quad\text{\textit{\cref{thm:compact}}}
  \end{alignat}
  This reduces the update in \cref{eq:ergodic_mirror_descent} into projected gradient descent which is the form used for Markov chain score climbing.

  Under our assumptions, \citet[Corollary 3.5]{duchi_ergodic_2012} show that, by assuming that the Hellinger mixing time is bounded as
  \begin{align}
    \tau_{\text{Hel.}}\left(K_{\vlambda}, \epsilon\right) \leq \kappa_1 \log\left( \kappa_2 /\epsilon \right) \label{eq:hellinger_mixing}
  \end{align}
  for any \(\epsilon > 0\), and setting a decreasing stepsize \(\alpha_t = \alpha / \sqrt{t}\),
  it follows that
  \begin{align}
    &\E{ \DKL{\pi}{q\,(\cdot; {\overline{\vlambda}_{T}})} - \DKL{\pi}{q\left(\cdot; {\vlambda^*}_{T}\right)}}
    \nonumber
    \\
    &\;\leq
    \frac{R^2}{2 \, \alpha \, \sqrt{T}}
    +
    \frac{2 \, \alpha \, G^2}{\sqrt{T}}\left( \kappa_1 \, \log \frac{\kappa_2}{\epsilon} \right)
    +
    3 \, \epsilon \, G \, R
    +
    \frac{R \, G \, \kappa_1 \, \log \frac{\kappa_2}{\epsilon}}{T}.
    \label{eq:duchi_base}
  \end{align}

  From \cref{thm:mixing_time}, we retrieve the constants of \cref{eq:hellinger_mixing} as
  \(
  \kappa_1=\frac{2}{\log \rho^{-1} },\;  \kappa_2 = 1
  \), and set \(\epsilon = 1/\sqrt{T}\).
  Then, using
  \begin{alignat}{2}
    \kappa_1 \, \log \frac{\kappa_2}{\epsilon}
    =
    \frac{1}{\log \rho^{-1} } \, \log T,\label{eq:kappa}
  \end{alignat}
  it follows that
  \begin{alignat*}{2}
    &\E{ \DKL{\pi}{q\,(\cdot; {\overline{\vlambda}_{T}})} - \DKL{\pi}{q\left(\cdot; {\vlambda^*}_{T}\right)}}
    \\
    &\;\leq
    \frac{R^2}{2 \, \alpha \, \sqrt{T}}
    +
    \frac{2 \, \alpha \, G^2}{\sqrt{T}}\left( \kappa_1 \, \log \frac{\kappa_2}{\epsilon} \right)
    +
    3 \, \epsilon \, G \, R
    +
    \frac{R \, G \, \kappa_1 \, \log \frac{\kappa_2}{\epsilon}}{T}.
    \nonumber
    &&\quad\text{\textit{\cref{eq:duchi_base}}}
    \\
    &\;=
    \frac{R^2}{2 \, \alpha \, \sqrt{T}}
    +
    \frac{2 \, \alpha \, G^2 \, \log T }{\log \rho^{-1} \, \sqrt{T}}
    +
    3 \, \epsilon \, G \, R
    +
    \frac{R \, G \, \log T }{\log \rho^{-1} \, T}.
    \nonumber
    &&\quad\text{\textit{\cref{eq:kappa}}}
  \end{alignat*}
\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
