
\vspace{-0.12in}
\section{Background}
\vspace{-0.07in}
\subsection{Inclusive Kullback-Leibler Minimization with Stochastic Gradients}\label{section:ivi_previous}
\vspace{-0.05in}
\paragraph{VI with SGD}
The goal of VI is to find the optimal variational parameter \(\vlambda\) identifying \(q\left(\cdot; \vlambda\right) \in \mathcal{Q}\) that minimizes some discrepancy measure \(D\left(\pi, q\left(\cdot; \vlambda\right)\right)\).
A typical way to perform VI is to use stochastic gradient descent (SGD,~\citealt{robbins_stochastic_1951}), provided that \textit{unbiased} gradient estimates of the optimization target \(\vg\,(\vlambda)\) are available such that we can repeat the update
%\vspace{-0.05in}
{%\small
\begin{align*}
  \vlambda_{t} = \vlambda_{t-1} - \gamma_{t} \, \vg\left(\vlambda_{t-1}\right)
\end{align*}
}%
where \(\gamma_1, \ldots, \gamma_T\) is a step-size schedule.

\paragraph{Inclusive KL Minimization with SGD}
For inclusive KL minimization, \(\vg\) should be set as
%
{%\small
\begin{align*}
  \vg\left(\vlambda\right)
  = \nabla_{\vlambda} \DKL{\pi}{q\left(\cdot; \vlambda\right)}
  = \nabla_{\vlambda} \mathbb{H}\left[\pi, q\left(\cdot; \vlambda\right)\right]
  = -\Esub{\rvvz \sim \pi\left(\cdot\right)}{ \vs\,(\vlambda; \rvvz) } 
\end{align*}
}%
%\vspace{-0.05in}
%
\looseness=-1
where \(\mathbb{H}\left[\pi, q\left(\cdot; \vlambda\right)\right]\) is the cross-entropy between \(\pi\) and \(q\left(\cdot; \vlambda\right)\), which shows the connection with cross-entropy methods~\citep{deboer_tutorial_2005}, and \(\vs\,(\vlambda;\vz) = \nabla_{\vlambda} \log q\,(\vz; \vlambda)\) is known as the \textit{score gradient}.
Since inclusive KL minimization with SGD is equivalent to ascending towards the direction of the score, \citet{NEURIPS2020_b2070693} coined the term score climbing.
To better conform with the optimization literature, we instead call this approach \textit{score ascent} as in gradient ascent.

%\vspace{-0.05in}
%% \subsection{Estimating the Score with Importance Sampling}\label{section:is}
%% When it is easy to sample from the variational approximation \(q_{\lambda}(\vz)\), one can use importance sampling (IS, \citealt{robert_monte_2004}) for estimating the score following
%% %\vspace{-0.05in}
%% {%\small
%% \begin{align*}
%%   \Esub{\pi}{ \vs\,(\vlambda;\rvvz) } 
%%   \propto Z\,\Esub{\pi}{ \vs\,(\vlambda; \rvvz) } %\label{eq:is_est} \\
%%   \approx \frac{1}{N} \sum^{N}_{i=1} \widetilde{w}\,(\vz^{(i)}) \, \vs\,(\vlambda; \vz^{(i)}) 
%%   \triangleq \vg_{IS}(\vlambda)
%% \end{align*}
%% }%
%% %\vspace{-0.05in}
%% where \(\widetilde{w}\,(\vz) = p\,(\vz,\vx) / q_{\vlambda}(\vz)\) is known as the unnormalized importance weight, \(Z\) is the marginal \(p\left(\vx\right) = \int p\left(\vx, \vz\right) \, d\vz\), and \(\vz^{(1)},\, \ldots,\, \vz^{(N)}\) are \(N\) independent samples from \(q_{\vlambda}(\vz)\).
%% This scheme is equivalent to adaptive IS methods~\citep{bugallo_adaptive_2017} since the IS proposal \(q_{\vlambda}(\vz)\) is iteratively optimized based on the current samples.
%% Although IS is unbiased, it is numerically unstable because of \(Z\). %in \cref{eq:is_est}.
%% A more stable alternative is to use the normalized weight, which results in the self-normalized IS (SNIS) approximation.
%% Unfortunately, SNIS still fails to converge even on moderate dimensional objectives and unlike IS, it is no longer unbiased~\citep{robert_monte_2004}.

\vspace{-0.05in}
\subsection{Markov Chain Gradient Descent}\label{section:mcgd}
\vspace{-0.05in}
\paragraph{\textbf{Overview of MCGD}}
Markov chain gradient descent (MCGD, \citealt{duchi_ergodic_2012, NEURIPS2018_1371bcce}) is a family of algorithms that optimize a function \(f\) defined as \(f\left(\vlambda\right) = \int f\left(\vlambda, \veta\right) \, \Pi\left(d\veta\right)\) where \(\Pi\left(d\veta\right)\) is a probability measure.
MCGD repeats the steps 
{%\small
\begin{align}
  \vlambda_{t+1}    = \vlambda_{t} - \gamma_t \, \vg\left(\vlambda_t, \veta_{t}\right),\quad 
  \rvveta_{t}  \sim P_{\vlambda_{t-1}}\left( \veta_{t-1}, \cdot \right)\label{eq:mcgd}
\end{align}
}%
where \(P_{\vlambda_{t-1}}\) is a \(\Pi\)-invariant Markov chain kernel that may depend on \(\vlambda_{t-1}\).
The noise of the gradient is Markovian and non-asymptotically biased, departing from vanilla SGD.
%Unlike vanilla SGD where the gradient noise is assumed to be \textit{i.i.d.} and unbiased, it is now .
Non-asymptotic convergence of this general algorithm has recently started to gather attention as by~\citet{duchi_ergodic_2012, NEURIPS2018_1371bcce, pmlr-v99-karimi19a, doan_finitetime_2020, doan_convergence_2020, debavelaere_convergence_2021}.

\vspace{-0.1in}
\paragraph{\textbf{Applications of MCGD}}
MCGD encompasses an extensive range of problems, including distributed optimization~\citep{ram_incremental_2009}, reinforcement learning~\citep{tadic_asymptotic_2017, doan_convergence_2020, Xiong_Xu_Liang_Zhang_2021}, and expectation-minimization~\citep{pmlr-v99-karimi19a}, to name a few.
This paper extends this list with inclusive KL VI through the MCSA framework.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
