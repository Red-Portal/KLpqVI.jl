
\section{Introduction}
Given an observed data \(\vx\) and a latent variable \(\vz\), Bayesian inference aims to analyze the posterior distribution  given an unnormalized joint density \(p\,(\vz,\,\vx)\) where the relationship is given by Bayes' rule such that \(\pi\,(\vz) = {p\,(\vz,\,\vx)}/{p\,(\vx)} \propto p\,(\vz,\,\vx)\).
Instead of working directly with the target distribution \(\pi\), variational inference (VI,~\citealt{blei_variational_2017}) methods seek a \textit{variational approximation} \(q(\vz; \vlambda)\) that is the most similar to \(\pi\) according to a discrepancy measure \(D\,(\pi,\, q(\cdot; \vlambda))\).

Naturally, choosing a discrepancy measure is critical to the problem.
This fact had led to a quest for suitable divergence measures~\citep{pmlr-v37-salimans15, NIPS2016_7750ca35, NIPS2017_35464c84, NEURIPS2018_1cd138d0, pmlr-v97-ruiz19a}.
So far, the exclusive (or reverse, backward)  KL divergence \(\DKL{q\left(\cdot; \vlambda\right)}{\pi}\) has been used ``exclusively'' among various discrepancy measures.
This is partly because the exclusive KL is defined as an average over \(q_{\lambda}(\vz)\), which can be estimated efficiently.
By contrast, the inclusive KL is defined as
%
{\small
\vspace{-0.05in}
\begin{align}
  %% \DKL{p}{q_{\lambda}} = \int p\,(\vz\mid\vx) \log\big(\, p\,(\vz\mid\vx)/q_{\lambda}(\vz) \,\big)\,d\vz
  %% = \Esub{p(\vz\mid\vx)}{\log\big(\, p\,(\vz\mid\vx)/q_{\lambda}(\vz) \,\big) } \label{eq:klpq}
  \DKL{\pi}{q\left(\cdot; \vlambda\right)}
  = \int \pi\,(\vz) \log \frac{\pi\left(\vz\right)}{\,q(\vz; \vlambda)} \,d\vz
  = \Esub{\pi}{\log \frac{\pi\left(\rvvz\right)}{\,q(\rvvz; \vlambda)} } \label{eq:klpq}
\end{align}
%\vspace{-0.05in}
}%
%
where the average is taken over \(\pi\). 
This is a chicken-and-egg problem as our goal is to obtain \(\pi\) in the first place.
Despite this challenge, minimizing~\eqref{eq:klpq} has drawn the attention of researchers because it is believed to result in favorable properties~\citep{minka2005divergence, mackay_local_2001}.

For minimizing the inclusive KL divergence,~\citet{NEURIPS2020_b2070693} have recently introduced Markovian score climbing (MSC) while~\citet{pmlr-v124-ou20a} have introduced joint stochastic approximation (JSA).
Both methods use stochastic gradient descent (SGD,~\citealt{robbins_stochastic_1951}) where the stochastic gradients are estimated using Markov chain Monte Carlo (MCMC).
Also, they are designed such that the MCMC kernel \(K_{\vlambda_t}\) can directly take advantage of the current variational approximation \(q\left(\cdot; \vlambda_t\right)\).

In this paper, we cast MSC and JSA into the same framework we call Markov chain score ascent (MCSA).
MCSA is radically different from vanilla SGD as the gradient noise is Markovian instead of being independently, identically distributed (\textit{i.i.d.}).
While convergence of MCSA has been shown by~\citet{NEURIPS2020_b2070693} through the work of~\citet{gu_stochastic_1998}, this result is asymptotic, and does not provide insight into the practical performance of MCSA.
We instead utilize the Markov chain gradient descent framework~\citet{duchi_ergodic_2012, NEURIPS2018_1371bcce, pmlr-v99-karimi19a, doan_convergence_2020}.
Although most of the current results in MCGD require strong assumptions such as boundedness of the score function, they provide a practical relationship between the convergence rate, the mixing rate of the MCMC kernel, and the gradient variance.
In particular, we obtain non-asymptotic convergence rates based on the result of~\citet{duchi_ergodic_2012}.
%% The general SGD setting where noise is Markovian have been previously considered by.
%% These Markov chain gradient descent (MCGD) methods encompasses an extremely wide range of problems including distributed optimization~\citep{ram_incremental_2009}, reinforcement learning~\citep{tadic_asymptotic_2017, doan_convergence_2020, Xiong_Xu_Liang_Zhang_2021}, and expectation-minimization~\citep{pmlr-v99-karimi19a}, to name a few.
%% Despite the apparent resemblance between MCGD and MCSA, MCSA has yet to be framed into the MCGD framework.
%% Furthermore, the design of JSA~\citep[Algorithm 3]{pmlr-v124-ou20a} is slightly more complicated to directly apply MCGD.
%% In this work, we provide a general condition for MCSA to become a special case of MCGD.
%% Under the discussed assumptions, we establish the non-asymptotic convergence of both JSA and MSC as a special case of ergodic mirror descent by~\citet{duchi_ergodic_2012}.
Our result enables comparing the practical performance of JSA and MSC through their gradient variance.
Furthermore, we provide a new simple variant of MCSA that achieves lower variance than both MCGD and MCSA.

We verify our theoretical analysis through numerical simulations.
Furthermore, we compare MSC, JSA, and our proposed method on general Bayesian inference problems.
Our experiments show that our proposed method is superior than all previous approaches.
Furthermore, within our experiments, MSCA is competitive against evidence lower-bound (ELBO) maximization.
We further discuss this result in relation with the conclusions of~\citet{dhaka_challenges_2021} in~\cref{section:discussion}.

%\vspace{-0.05in}
%\paragraph{Contribution Summary}
%\vspace{-0.2in}
\begin{itemize}[noitemsep]
\item[\ding{228}] We established non-asymptotic convergence of MCSA based on previous results of MCGD and providing general design conditions (\textbf{\cref{section:convergence}}).
\item[\ding{228}] We compare the practical performance of MCSA algorithms (\textbf{\cref{section:comparison}}).
\item[\ding{228}] We propose a new MCSA scheme with a tighter gradient variance bound (\textbf{\cref{section:comparison}}).
\item[\ding{228}] We compare the newly proposed MSCA scheme against previous algorithms on general Bayesian inference benchmarks (\textbf{\cref{section:eval}}).
\end{itemize}
\vspace{-0.05in}
%\item We discuss connections with adaptive IMH methods (\textbf{\cref{section:related}}).
%\end{enumerate*}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
