
\section{Introduction}\label{section:intro}
Bayesian inference aims to analyze the posterior distribution of an unknown latent variable \(\vz\) from which data \(\vx\) is observed.
By assuming a model \(p\,(\vx\,|\,\vz)\), the posterior \(\pi\,(\vz)\) is given by Bayes' rule such that \(\pi\,(\vz) \propto {p\,(\vx\,|\,\vz)\,p\,(\vz)}\) where \(p\,(\vz)\) represents our prior belief on \(\vz\).
Instead of working directly with \(\pi\), variational inference (VI,~\citealt{blei_variational_2017}) seeks a \textit{variational approximation} \(q\,(\vz; \vlambda)\) that is the most similar to \(\pi\) according to a discrepancy measure \(d\,(\pi,\, q\,(\cdot; \vlambda))\).

%% Naturally, choosing an appropriate discrepancy measure is key, which has led to a quest for suitable divergence measures~\citep{ NIPS2016_7750ca35, NIPS2017_35464c84, NEURIPS2018_1cd138d0, NEURIPS2020_c928d86f, regli_alphabeta_2018, pmlr-v48-hernandez-lobatob16, NIPS2016_7750ca35 }.
\looseness=-1
The apparent importance of choosing the right discrepancy measure has led to a quest spanning a decade~\citep{NIPS2017_35464c84, NEURIPS2018_1cd138d0, NEURIPS2020_c928d86f, regli_alphabeta_2018, pmlr-v48-hernandez-lobatob16,NIPS2016_7750ca35,pmlr-v37-salimans15,pmlr-v97-ruiz19a,NEURIPS2021_05f971b5,NEURIPS2021_a1a609f1, bamler_perturbative_2017}.
So far, the exclusive (or reverse, backward)  Kullback-Leibler (KL) divergence \(\DKL{q\left(\cdot; \vlambda\right)}{\pi}\) has seen ``exclusive'' use, partly because it is defined as an integral over \(q\,(\vz; \vlambda)\), which can be approximated efficiently.
In contrast, the \textit{inclusive} (or forward) KL is defined as an integral over \(\pi\) as
%
{%\small
\vspace{-0.05in}
\begin{align*}
  \DKL{\pi}{q\left(\cdot; \vlambda\right)}
  = \int \pi\,(\vz) \log \frac{\pi\left(\vz\right)}{q\,(\vz; \vlambda)} \,d\vz
  = \Esub{\rvvz \sim \pi\left(\cdot\right)}{\log \frac{\pi\left(\rvvz\right)}{\,q\,(\rvvz; \vlambda)} }. %\label{eq:klpq}
\end{align*}
%\vspace{-0.05in}
}%
Since our goal is to approximate \(\pi\) with \(q\left(\cdot;\vlambda\right)\) but the inclusive KL involves an integral over \(\pi\), we end up facing a chicken-and-egg problem.
Despite this challenge, the inclusive KL has consistently drawn attention due to its statistical properties, such as better uncertainty estimates due to its mass covering property~\citep{minka2005divergence, mackay_local_2001, trippe_overpruning_2017}.

Recently,~\citet{NEURIPS2020_b2070693,pmlr-v124-ou20a} have respectively proposed Markovian score climbing (MSC) and joint stochastic approximation (JSA).
These methods minimize the inclusive KL using stochastic gradient descent (SGD,~\citealt{robbins_stochastic_1951}), where the gradients are estimated using a Markov chain.
The Markov chain kernel \(K_{\vlambda_t}\left(\vz_t, \cdot\right)\) is \(\pi\)-invariant~\citep{robert_monte_2004} and is chosen such that it directly takes advantage of the current variational approximation \(q\left(\cdot; \vlambda_t\right)\).
Thus, the quality of the gradients improves over time as the KL divergence decreases.
Still, the gradients are non-asymptotically biased and Markovian across adjacent iterations, which sharply contrasts MSC and JSA from classical black-box VI~\citep{pmlr-v33-ranganath14, JMLR:v18:16-107}, where the gradients are unbiased and independent.
While~\citet{NEURIPS2020_b2070693} has shown the convergence of MSC through the work of~\citet{gu_stochastic_1998}, this result is only asymptotic and does not provide practical insight into the performance of MSC.

%

In this paper, we address these theoretical gaps by casting MSC and JSA into a general framework we call Markov chain score ascent (MCSA), which we show is a special case of Markov chain gradient descent (MCGD,~\citealt{duchi_ergodic_2012}).
This enables the application of the non-asymptotic convergence results of MCGD~\citep{duchi_ergodic_2012, NEURIPS2018_1371bcce, pmlr-v99-karimi19a, doan_finitetime_2020, doan_convergence_2020, Xiong_Xu_Liang_Zhang_2021, debavelaere_convergence_2021}.
For MCGD methods, the fundamental properties affecting the convergence rate are the ergodic convergence rate (\(\rho\)) of the MCMC kernel and the gradient variance (\(G\)).
We analyze \(\rho\) and \(G\) of MSC and JSA, enabling their practical comparison given a fixed computational budget (\(N\)).
Furthermore, based on the recent insight that the mixing rate does not affect the convergence rate of MCGD~\citet{doan_convergence_2020,doan_finitetime_2020}, we propose a novel scheme, parallel MCSA (pMCSA), which achieves lower variance by trading off the mixing rate.
We verify our theoretical analysis through numerical simulations and compare MSC, JSA, and pMCSA on general Bayesian inference problems.
Our experiments show that our proposed method outperforms previous MCSA approaches.
%Furthermore, MSCA is competitive against evidence lower-bound (ELBO) maximization within our experiments.
%We further discuss this result in relation with the conclusions of~\citet{dhaka_challenges_2021} in~\cref{section:discussion}.

\vspace{-0.1in}
\paragraph{Contribution Summary}
%\jrg{Item one should be something along the lines of we provide the first non-asymptotic analysis of MSC and JSA. Item two should be we accomplish this by showing it's a special case (basically point 1 below). I've sketched this out -- maybe we add back in the appropriate sec/thm references?}
%\vspace{-0.2in}
%% \begin{enumerate*}[label=\textbf{(\roman*)}]
\setlength\itemsep{0.01in}
\begin{enumerate}[leftmargin=7mm,listparindent=\parindent]
    \vspace{-0.05in}
    \item \textbf{\cref{section:comparison}:} We provide the first non-asymptotic theoretical analysis of two recently proposed inclusive KL minimization methods, MSC~\textbf{(\cref{thm:msc})} and JSA~\textbf{(\cref{thm:jsa})}.
    \item \textbf{\cref{section:mcsa}:} To do this, we show that both methods can be viewed as what we call ``Markov chain score ascent'' (MCSA) methods, which are a special case of MCGD~\textbf{(\cref{thm:product_kernel})}.
    \item \textbf{\cref{section:pmcsa}:} In light of this, we develop a novel MCSA method which we call parallel MCSA (pMCSA) that achieves lower gradient variance \textbf{(\cref{thm:pmcsa})}.
    \item \textbf{\cref{section:eval}:} We demonstrate that the improved theoretical performance of pMCSA translates to superior empirical performance across a variety of Bayesian inference tasks.
\end{enumerate}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
