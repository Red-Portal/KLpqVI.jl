
\begin{theoremEnd}[all end]{lemma}\label{thm:iw_variance}
  Let the importance weight be defined as \(w\left(\vz\right) = \pi\left(\vz\right)/q\left(\vz\right)\).
  The variance of the importance weights is related to the \(\chi^2\) divergence as
  \begin{align*}
    \mathbb{V}_{q}{w\left(\rvvz\right)}
    =
    \DChi{\pi}{q}.
  \end{align*}
\end{theoremEnd}
\begin{proofEnd}
  {
  \begin{alignat}{2}
    \mathbb{V}_{q}{w\left(\rvvz\right)}
    =
    \Esub{q}{
      {\left(
        w\left(\rvvz\right)
      -
      \Esub{q}{
        w\left(\rvvz\right)
      }
      \right)}^2
    }
    =
    \Esub{q}{
      {\left(
      w\left(\rvvz\right)
      -
      1
      \right)}^2
    }
    =
    \int {\left( \frac{\pi\left(\vz\right)}{q\left(\vz\right)}  - 1 \right)}^2 q\left(d\vz\right)
    =
    \DChi{\pi}{q}
    \nonumber
  \end{alignat}
  }
\end{proofEnd}

\begin{theoremEnd}{theorem}{\citep{cardoso_brsnis_2022}}\label{thm:mscrb}
  The gradient variance of MSC-RB is bounded as
  {%\small
  \begin{align*}
  {\textstyle
    \E{ {\lVert \rvvg_{t,\mathrm{MSC\text{-}RB}} \rVert}_2^2  \,\middle|\, \mathcal{F}_{t-1} }
    \leq
    4 \, L^2 \,\left[
    \frac{1}{N-1}\,\DChi{\pi}{q\left(\cdot; \rvvlambda_{t-1}\right)} 
    +
    \mathcal{O}\left(N^{-3/2} + \nicefrac{\gamma^{t-1}}{N-1}\right)
    \right]
    +
    {\lVert\vmu\rVert}^2_2
  }
  \end{align*}
  }%
  where \(\vmu = \mathbb{E}_{\pi}\vs\left(\vlambda;\rvvz\right)\) and \(\gamma = 2 w^* / \left(2 w^* + N - 2\right)\) is the mixing rate of the Rao-Blackwellized CIS kernel.
\end{theoremEnd}
\begin{proofEnd}
  Rao-Blackwellization of the CIS kernel is to reuse the importance weights \(w\left(\vz\right) = \pi\left(\vz\right) / q_{\text{def.}}\left(\vz\right)\) internally used by the kernel when forming the estimator.
  That is, the gradient is estimated as
  \begin{align*}
    &\rvvz^{(n)}_t \mid \rvvlambda_{t-1} \sim q\left(\cdot; \rvvlambda_{t-1}\right) \\
    &\rvvg_{t,\text{MSC-RB}} = -\sum^{N}_{n=2} \frac{w\left(\rvvz^{(n)}_t\right)}{\sum^{N}_{m=2}w\left(\rvvz^{(n)}_t\right) + w\left(\rvvz_{t-1}\right)} \vs\left(\rvvz^{(n)}_t\right) + \frac{w\left(\rvvz_{t-1}\right)}{\sum^{N}_{m=2}w\left(\rvvz^{(n)}_t\right) + w\left(\rvvz_{t-1}\right)} \vs\left(\rvvz_{t-1}\right).
  \end{align*}

  By \cref{thm:second_moment_bound}, the second moment of the gradient is bounded as
  \begin{alignat}{2}
    \E{ {\parallel \rvvg_{t,\text{MSC-RB}} \parallel}_2^2 \,\middle|\, \mathcal{F}_{t-1} } 
    &=
    \mathsf{MSE}\left[ \rvvg_{t, \text{MSC-RB}} \,\middle|\, \mathcal{F}_{t-1} \right]
    +
    2 \, {\mathsf{Bias}\left[ \rvvg_{t, \text{MSC-RB}} \,\middle|\, \mathcal{F}_{t-1} \right]}^{\top} \norm{\vmu}_2
    + \norm{\vmu}^2_2
    \nonumber
    \\
    &\leq
    \mathsf{MSE}\left[ \rvvg_{t, \text{MSC-RB}} \,\middle|\, \mathcal{F}_{t-1} \right]
    +
    2 \, L \, {\mathsf{Bias}\left[ \rvvg_{t, \text{MSC-RB}} \,\middle|\, \mathcal{F}_{t-1} \right]}
    + \norm{\vmu}^2_2.
    \label{eq:mscrb_decomp}
  \end{alignat}
  
  \citet[Theorem 3]{cardoso_brsnis_2022} show that the mean-squared error of this estimator, which they call bias reduced self-normalized importance sampling, is bounded as
  \begin{align*}
    \mathsf{MSE}\left[ \rvvg_{t,\text{MSC-RB}} \,\middle|\, \mathcal{F}_{t-1} \right] \leq
    4\,L^2 \, \Big[\,&
     \left( 1 + \epsilon^2 \right)\, \frac{1}{N-1}\,\Vsub{\rvvz \sim q_{\text{def.}}\left(\cdot; \rvvlambda_{t-1}\right)}{w\left(\rvvz\right) \mid \rvvlambda_{t-1}} 
     \\
     &+
     \left(1 + \epsilon^{-2} \right)\, \frac{1}{N^2} {\left(1 + w^*\right)}^2 
    \,\Big],
  \end{align*}
  for some arbitrary constant \(\epsilon^2\).
  The first term is identical to the variance of an \(N-1\)-sample self-normalized importance sampling estimator~\citep{10.1214/17-STS611}, while the second term is the added variance due to ``rejections.''

  Since the variance of the importance weights is well known to be related to the \(\chi^2\) divergence,
  \begin{alignat}{2}
    \mathsf{MSE}\left[ \rvvg_{t,\text{MSC-RB}} \,\middle|\, \mathcal{F}_{t-1} \right] \leq
    4\,L^2 \, \Big[\,&
     \left( 1 + \epsilon^2 \right)\, \frac{1}{N-1}\,\DChi{\pi}{q\left(\cdot; \rvvlambda_{t-1}\right)}
    \nonumber
     \\
     &+
     \left(1 + \epsilon^{-2} \right)\, \frac{1}{N^2} {\left(1 + w^*\right)}^2 
    \,\Big].
    &&\quad\text{\textit{\cref{thm:iw_variance}}}
    \nonumber
  \end{alignat}

  For \(\epsilon^2\), \citeauthor{cardoso_brsnis_2022} choose \(\epsilon^2 = {\left(N-1\right)}^{-1/2}\), which results in their stated bound
  \begin{align*}
    \mathsf{MSE}\left[ \rvvg_{t,\text{MSC-RB}} \,\middle|\, \mathcal{F}_{t-1} \right] \leq
    4\,L^2 \, \Big[\,
      \frac{1}{N-1}\,\DChi{\pi}{q\left(\cdot; \rvvlambda_{t-1}\right)}
      +
      \mathcal{O}\left(N^{-3/2}\right)
    \,\Big].
  \end{align*}

  Furthermore, they show that the bias term is bounded as
  \begin{align*}
    \mathsf{Bias}\left[ \rvvg_{t,\text{MSC-RB}} \right]
    \leq
    \frac{4 \, L}{N-1} \left( \DChi{\pi}{q\left(\cdot; \rvvlambda_{t-1}\right)} + 1 + w^*\right) {\left( \frac{2 w^*}{ 2 w^* + N - 2 } \right)}^{t-1}.
  \end{align*}
  Combining both the bias and the mean-squared error to \cref{eq:mscrb_decomp}, we obtain the bound
  \begin{align*}
    &\E{ {\lVert \rvvg_{t,\text{MSC-RB}} \rVert}_2^2 \,\middle|\, \mathcal{F}_{t-1} }
    \\
    &\;\leq 
    4\,L^2\,\bigg[
    \frac{1}{N-1}\,\DChi{\pi}{q\left(\cdot; \rvvlambda_{t-1}\right)}
    \\
    &\qquad+
    \frac{1}{N-1} \left( \DChi{\pi}{q\left(\cdot; \rvvlambda_{t-1}\right)} + 1 + w^*\right) {\left( \frac{2 w^*}{ 2 w^* + N - 2 } \right)}^{t-1}
    +
    \mathcal{O}\left(N^{-3/2}\right)
    \bigg]
    \\
    &=
    4 \, L^2 \,\bigg[
    \frac{1 + \gamma^{t-1}}{N-1}\,\DChi{\pi}{q\left(\cdot; \rvvlambda_{t-1}\right)} + \frac{\gamma^{t-1}}{N-1} + \frac{\gamma^{t-1} w^*}{N-1}
    +
    \mathcal{O}\left(N^{-3/2}\right)
    \bigg].
  \end{align*}
\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
