
\vspace{-1.5ex}
\section{Discussions}\label{section:discussion}
\vspace{-1.5ex}
This paper presented a new theoretical framework for analyzing inclusive KL divergence minimization methods based on running SGD with Markov chains.
Furthermore, we proposed pMCSA, a new MCSA method that enjoys substantially low variance.
We have shown that this theoretical improvement translates into better empirical performance.

\vspace{-1.5ex}
\paragraph{Limitations}
Our work has three main limitations.
Firstly, since our work aims to understand existing MCSA methods, it inherits their current limitations.
For example, minibatch subsampling is challenging for models with non-factorizable likelihoods~\citep{NEURIPS2020_b2070693}.
Secondly, our theoretical analysis in~\cref{section:comparison} requires~\cref{thm:bounded_score}, which is strong, but required to connect with MCGD.
An important future direction would be to relax the assumptions needed by MCGD.
\textcolor{blue}{%
Lastly, our MCSA framework does not include models with parameterized posteriors such as variational autoencoders.
}

\vspace{-4ex}
\textcolor{blue}{
\vspace{-1.5ex}
\paragraph{Parameterized Posteriors}
On problems with parameterized posteriors, the target posterior moves around.
Therefore, quickly chasing the moving posterior with fast converging MCMC kernels is as important as achieving low variance.
Because of this, trading bias and variance is less straightforward compared to the ``static'' setting we consider.
Furthermore, the usage of expensive MCMC kernels could be beneficial as suggested by \citet{zhang_transport_2022}.
}

\vspace{-1.5ex}
\paragraph{Towards Alternative Divergences}
In \cref{section:eval}, we have shown that minimizing the \textit{inclusive} KL is competitive against  minimizing the \textit{exclusive} KL on general Bayesian inference problems.
Although~\citet{dhaka_challenges_2021} has shown that the inclusive KL fails on high-dimensional problems, this is only the case under the presence of strong correlations.
Before entirely ditching the inclusive KL, it is essential to ask, ``how correlated posteriors really are in practice?''
Furthermore, the true performance of alternative divergences is often masked by the limitations of the inference procedure~\citep{geffner2021empirical, pmlr-v139-geffner21a}.
Given that pMCSA significantly advances the best-known performance of inclusive KL minimization, it is possible that similar improvements could be extracted from other divergences.
To conclude, our results motivate further development of better inference algorithms for alternative divergence measures.

%An interesting direction for future research would be to invstigate whether such result 
%This is against the conclusions of~\citet{dhaka_challenges_2021} that exclusive VI should outperform inclusive VI in high-dimensions. 

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
