
\section{Probabilistic Models Used in the Experiments}
\subsection{Bayesian Neural Network Regression}\label{section:model_bnn}
We use the BNN model of~\citet{pmlr-v37-hernandez-lobatoc15} defined as
\begin{align*}
  \lambda^{-1} &\sim \mathsf{inverse\text{-}gamma}\left(\alpha=6, \beta=6\right) \\
  \gamma^{-1}  &\sim \mathsf{inverse\text{-}gamma}\left(\alpha=6, \beta=6\right) \\
  \mW_1       &\sim \mathcal{N}\left(\mathbf{0}, \lambda^{-1} \mI \right) \\
  \vz         &= \mathsf{ReLU}\left(\mW_1 \vx_i\right) \\
  \mW_2       &\sim \mathcal{N}\left(\mathbf{0}, \lambda^{-1} \mI \right) \\
  \widehat{y} &= \mathsf{ReLU}\left(\mW_2 \vz\right) \\
  y_i         &\sim \mathcal{N}\left(\widehat{y}, \gamma^{-1}\right),
\end{align*}
where \(\vx_i\) and \(y_i\) are the feature vector and target value of the \(i\)th datapoint.
Given the variational distribution of \(\lambda^{-1}, \gamma^{-1}, \mW_1, \mW_2\), we use the same posterior predictive approximation of~\citet{pmlr-v37-hernandez-lobatoc15}.
We apply z-standardization (whitening) to the features \(\vx_i\) and the target values \(y_i\), and unwhiten the predictive distribution.

\subsection{Robust Gaussian Process Logistic Regression}\label{section:model_rgp}
We perform robust Gaussian process regression by using a student-t prior with a latent Gaussian process prior.
The model is defined as
\begin{align*}
   \log \sigma_f &\sim \mathcal{N}(0, 4) \\
   \log \epsilon &\sim \mathcal{N}(0, 4) \\
   \log \ell_i   &\sim \mathcal{N}(0, 0.2) \\
   f   &\sim \mathcal{GP}\left(\mathbf{0}, \mSigma_{\sigma_f, \mathbf{\ell}} + \left(\delta + \epsilon^2\right)\,\mI \right) \\
   \nu &\sim \mathsf{gamma}\left(\alpha=4, \beta=1/10\right) \\
   \log \sigma_y &\sim \mathcal{N}(0, 4) \\
   y_i &\sim \mathsf{student\text{-}t}\left(  f\left( \vx_i \right), \sigma_y, \nu  \right).
\end{align*}
The covariance \(\mSigma\) is computed using a kernel \(k\left(\cdot, \cdot\right)\) such that \({[\mSigma]}_{i,j} = k\left( \vx_{i}, \vx_{j} \right) \) where \(\vx_i\) and \(\vx_j\) are data points in the dataset.
For the kernel, we use the Matern 5/2 kernel with automatic relevance determination~\citep{neal_bayesian_1996} defined as
\begin{align*}
  &k\left(\vx, \vx' ;\; \sigma^2, \ell^2_1, \ldots, \ell^2_D \right) =
  \sigma_f \left( 1 + \sqrt{5} r + \frac{5}{3} r^2 \right) \exp\left( - \sqrt{5} r \right), \quad
  \text{where}\;\; r = \sum^{D}_{i=1} \frac{ {\left(\vx_i - \vx'_i\right)}^2 }{\ell^2_i}
\end{align*}
and \(D\) is the number of dimensions.
The jitter term \(\delta\) is used for numerical stability.
We set a small value of \(\delta = 1\times10^{-6}\).

%% \subsection{Radon Hierarchical Regression}
%% The partially pooled linear regression model used in~\cref{section:mll} is
%% \begin{align*}
%%   \sigma_{a_1} &\sim \mathrm{Gamma}\left( \alpha = 1, \beta = 0.02 \right) \\
%%   \sigma_{a_2} &\sim \mathrm{Gamma}\left( \alpha = 1, \beta = 0.02 \right) \\
%%   \sigma_{y}  &\sim \mathrm{Gamma}\left( \alpha = 1, \beta = 0.02 \right) \\
%%   \mu_{a_1}    &\sim \mathcal{N}\left( 0, 1 \right) \\
%%   \mu_{a_2}    &\sim \mathcal{N}\left( 0, 1 \right) \\
%%   a_{1,\, c}     &\sim \mathcal{N}\left( \mu_{a_1}, \sigma_{a_1}^2 \right) \\
%%   a_{2,\, c}     &\sim \mathcal{N}\left( \mu_{a_2}, \sigma_{a_2}^2 \right) \\
%%   y_i         &\sim \mathcal{N}\left( a_{1,\, c_i} + a_{2,\, c_i}\,x_i,\, \sigma_y^2 \right)
%% \end{align*}
%% where \(a_{1,\,c}\) is the intercept at the county \(c\), \(a_{2,\,c}\) is the slope at the county \(c\), \(c_i\) is the county of the \(i\)th datapoint, \(x_i\) and \(y_i\) are the floor predictor of the measurement and the measured radon level of the \(i\)the datapoint, respectively.
%% The model pools the datapoints into their respective counties, which complicates the posterior geometry~\citep{betancourt_hierarchical_2020}.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
