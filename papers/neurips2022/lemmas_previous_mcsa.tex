
%% \begin{theoremEnd}{lemma}\label{thm:}
%%   Assuming \cref{thm:location_scale,thm:lipschitz,thm:solution_space} and that the base density \(\phi\left(\cdot\right)\) is standardized, the expectation of the score function over the the variational approximation \(q\left(\cdot; \vlambda\right)\) is bivariate uniformly Lipschitz continuous as
%%   \begin{align*}
%%     \Esub{\rvvz \sim q\left(\cdot; \vlambda\right)}{
%%       \norm{ \nabla \log q\left(\rvvz; \vlambda\right) }_2
%%     }
%%     \leq
%%     2 \, D \, L \, M \, \left(\norm{\vlambda}_2 + 1 \right).
%%   \end{align*}
%% \end{theoremEnd}
%% \begin{proofEnd}
%%   \begin{alignat}{2}
%%     &\Esub{\rvvz \sim q\left(\cdot; \vlambda\right)}{
%%       \norm{ \nabla \log q\left(\rvvz; \vlambda\right) }_2
%%     }
%%     \nonumber
%%     \\
%%     &\;\leq
%%     \Esub{\rvvz \sim q\left(\cdot; \vlambda\right)}{
%%       D \, L \, M \, \left( \norm{\rvvz}_2 + \norm{\vlambda}_2 + 1 \right)
%%     }
%%     \nonumber
%%     &&\quad\text{\textit{\cref{thm:score_bound}}}
%%     \\
%%     &\;\leq
%%     D \, L \, M \, \left(
%%     \Esub{\rvvz \sim q\left(\cdot; \vlambda\right)}{
%%       \norm{\rvvz}_2
%%     }
%%     + \norm{\vlambda} + 1 \right)
%%     &&\quad\text{\textit{Pushed in expectation}}
%%     \nonumber
%%     \\
%%     &\;\leq
%%     D \, L \, M \, \left(
%%     \sqrt{
%%       \Esub{\rvvz \sim q\left(\cdot; \vlambda\right)}{
%%         \norm{\rvvz}_2^2
%%       }
%%     }
%%     + \norm{\vlambda}_2 + 1 \right)
%%     &&\quad\text{\textit{Jensen's inequality}}
%%     \nonumber
%%     \\
%%     &\;=
%%     D \, L \, M \, \left(
%%     \sqrt{
%%       \Esub{\rvvu \sim \phi\left(\cdot\right)}{
%%         \norm{ \mC_{\vlambda} \rvvu + \vm_{\vlambda} }_2^2
%%       }
%%     }
%%     + \norm{\vlambda}_2 + 1 \right)
%%     \nonumber
%%     &&\quad\text{\textit{\cref{thm:location_scale}}}
%%     \\
%%     &\;=
%%     D \, L \, M \, \left(
%%     \sqrt{
%%       \Esub{\rvvu \sim \phi\left(\cdot\right)}{
%%         \rvvu^{\top} \mC_{\vlambda}^{\top} \mC_{\vlambda} \rvvu +  \rvvu^{\top} \mC_{\vlambda}^{\top}  \vm_{\vlambda}  + \vm_{\vlambda}^{\top} \vm_{\vlambda}
%%       }
%%     }
%%     + \norm{\vlambda}_2 + 1 \right)
%%     \nonumber
%%     &&\quad\text{\textit{Expanded norm}}
%%     \\
%%     &\;=
%%     D \, L \, M \, \left(
%%     \sqrt{
%%       \Esub{\rvvu \sim \phi\left(\cdot\right)}{
%%         \mathrm{tr}\left( \mC_{\vlambda}^{\top} \mC_{\vlambda} \rvvu \rvvu^{\top} \right)
%%         +  \rvvu^{\top} \mC_{\vlambda}^{\top}  \vm_{\vlambda}  + \vm_{\vlambda}^{\top} \vm_{\vlambda}
%%       }
%%     }
%%     + \norm{\vlambda}_2 + 1 \right)
%%     \nonumber
%%     &&\quad\text{\textit{Matrix trace trick}}
%%     \\
%%     &\;=
%%     D \, L \, M \, \left(
%%     \sqrt{
%%       \mathrm{tr}\left( \mC_{\vlambda}^{\top} \mC_{\vlambda} \right)
%%       +
%%       \vm_{\vlambda}^{\top} \vm_{\vlambda}
%%     }
%%     + \norm{\vlambda}_2 + 1 \right)
%%     \nonumber
%%     &&\quad\text{\textit{Since \(\rvvu \sim \phi\left(\cdot\right)\) is standardized}}
%%     \\
%%     &\;=
%%     D \, L \, M \, \left(
%%     \sqrt{
%%       \norm{C}_F^2
%%       +
%%       \norm{\vm}^2_2
%%     }
%%     + \norm{\vlambda}_2 + 1 \right)
%%     \nonumber
%%     &&\quad\text{\textit{Definition of \(L^2\)-norm}}
%%     \\
%%     &\;=
%%     D \, L \, M \, \left(
%%     \norm{\vlambda}_2
%%     + \norm{\vlambda}_2 + 1 \right)
%%     \nonumber
%%     &&\quad\text{\textit{\cref{eq:parameter_norm}}}
%%     \\
%%     &\;\leq
%%     2 \, D \, L \, M \, \left(\norm{\vlambda}_2 + 1 \right)
%%     \nonumber
%%   \end{alignat}
%% \end{proofEnd}

\begin{theoremEnd}[all end]{proposition}\label{thm:wstar}
The maximum importance weight
\(w^* = \sup_{\vz} w\left(\vz\right) = \sup_{\vz} \pi\left(\vz\right)/q\left(\vz;\vlambda\right)\)
is bounded below exponentially by the KL divergence as
\[
    \exp\left( \DKL{\pi}{q\left(\cdot; \vlambda\right)} \right) < w^*.
\]
\end{theoremEnd}
\begin{proofEnd}
  \begin{alignat*}{2}
    \DKL{\pi}{q\left(\cdot; \vlambda\right)}
    &=
    \Esub{\rvvz \sim \pi\left(\cdot\right)}{ \log \frac{\pi\left(\rvvz\right)}{q\left(\rvvz; \vlambda\right)} }
    &&\quad\text{\textit{Definition of \(d_{\text{KL}}\)}}
    \\
    &\leq
    \log \Esub{\rvvz \sim \pi\left(\cdot\right)}{ \frac{\pi\left(\rvvz\right)}{q\left(\rvvz; \vlambda\right)} }
    &&\quad\text{\textit{Jensen's inequality}}
    \\
    &\leq
    \log \Esub{\rvvz \sim \pi\left(\cdot\right)}{ w^* }
    \\
    &=
    \log w^*.
  \end{alignat*}
\end{proofEnd}

\begin{theoremEnd}[all end]{lemma}\label{thm:product_measure_bound}
  For the probability measures \(p_1, \ldots, p_N\) and \(q_1, \ldots, q_N\) defined on a measurable space \((\mathsf{X}, \mathcal{A})\) and an arbitrary set \(A \in \mathcal{A}\),
  \begin{align*}
    &\abs{
    \int_{A^N}
    p_1\left(dx_1\right)
    p_2\left(dx_2\right)
    \times
    \ldots
    \times
    p_N\left(dx_N\right)
    -
    q_1\left(dx_1\right)
    q_2\left(dx_2\right)
    \times
    \ldots
    \times
    q_N\left(dx_N\right)
  }
    \\
  &\qquad\leq
  \sum_{n=1}^N
  \abs{
    \int_{A}
    p_n\left(dx_n\right)
    -
    q_n\left(dx_n\right)
  }
  \end{align*}
\end{theoremEnd}
\begin{proofEnd}
  By using the following shorthand notations
  \begin{alignat*}{2}
    p_{(1:N)}\left(dx_{(1:N)}\right)
    &= 
    p_1\left(dx_1\right)
    p_2\left(dx_2\right)
    \times
    \ldots
    \times
    p_N\left(dx_N\right)
    \\
    q_{(1:N)}\left(dx_{(1:N)}\right)
    &= 
    q_1\left(dx_1\right)
    q_2\left(dx_2\right)
    \times
    \ldots
    \times
    q_N\left(dx_N\right),
  \end{alignat*}
  the result follows from induction as
  \begin{alignat}{2}
    &\abs{
      \int_{A^N}
      p_{(1:N)}\left(dx_{(1:N)}\right)
      -
      q_{(1:N)}\left(dx_{(1:N)}\right)
    }
    \nonumber
    \\
    &\quad=
    \Bigg|\;
    \left( \int_{A} p_1\left(dx_1\right) - q_1\left(dx_1\right) \right) \,
    \int_{A^{N-1}} p_{(2:N)}\left(dx_{(2:N)}\right)
    \nonumber
    \\
    &\qquad\quad+
    \int_{A} q_1\left(dx_1\right) \,
    {\left(
      \int_{A^{N-1}}
      p_{(2:N)}\left(dx_{(2:N)}\right)
      -
      q_{(2:N)}\left(dx_{(2:N)}\right)
    \right)}
    \;\Bigg|
    \nonumber
    \\
    &\quad\leq
    \Bigg|
    \int_{A} p_1\left(dx_1\right) - q_1\left(dx_1\right)
    \Bigg|\;
    \int_{A^{N-1}} p_{(2:N)}\left(dx_{(2:N)}\right)
    \nonumber
    \\
    &\qquad\quad+
    \int_{A} q_1\left(dx_1\right) \,
    {
    \Bigg|\;
      \int_{A^{N-1}}
      p_{(2:N)}\left(dx_{(2:N)}\right)
      -
      q_{(2:N)}\left(dx_{(2:N)}\right)
    }
    \;\Bigg|
    &&\quad\text{\textit{Triangle inequality}}
    \nonumber
    \\
    &\quad\leq
    \Bigg|
    \int_{A} p_1\left(dx_1\right) - q_1\left(dx_1\right)
    \Bigg|\;
    \nonumber
    \\
    &\qquad\quad+
    {
    \Bigg|\;
      \int_{A^{N-1}}
      p_{(2:N)}\left(dx_{(2:N)}\right)
      -
      q_{(2:N)}\left(dx_{(2:N)}\right)
    }
    \;\Bigg|.
    &&\quad\text{\textit{Applied \(p_n\left(A\right), q_n\left(A\right) \leq 1 \)}}
    \nonumber
\end{alignat}
\end{proofEnd}

\begin{theoremEnd}[all end]{lemma}\label{thm:second_moment_bound}
  Let \(\rvvg\) be a vector-valued, biased estimator of \(\vmu\), where the bias is denoted as \(\mathsf{Bias}\left[\rvvg\right] = \norm{ \mathbb{E}\rvvg - \vmu }_2\) and the mean-squared error is denoted as \(\mathsf{MSE}\left[\rvvg\right] = \mathbb{E}\norm{\rvvg - \vmu }_2^2\).
  Then, the second moment of \(\rvvg\) is bounded as
  \begin{enumerate}
  \item[\ding{182}]
  \(
    \mathbb{E}\norm{\rvvg}^2 
    \leq
    \mathbb{V}{\rvvg} + {\mathsf{Bias}\left[\rvvg\right]}^2 + 2\, \mathsf{Bias}\left[\rvvg\right] \norm{\vmu}_2 + \norm{\vmu}^2_2,
  \)
  \item[\ding{183}]
  \(
    \mathbb{E}\norm{\rvvg}^2 
    \leq
    \mathsf{MSE}\left[{\rvvg}\right] + 2\, \mathsf{Bias}\left[\rvvg\right] \norm{\vmu}_2 + \norm{\vmu}^2_2,
  \)
  \end{enumerate}
  where \(\mathbb{V}\rvvg = \mathbb{E}\norm{ \rvvg - \vmu }_2^2\) is the variance of the estimator.
\end{theoremEnd}
\begin{proofEnd}
  \ding{182} follows from the decomposition
  {%\small
  \begin{alignat}{2}
    \E{ \norm{\rvvg}^2_{2} }
    &\;=
    \mathbb{V}{ \rvvg }
    +
    \norm{ \mathbb{E}{ \rvvg } }_2^2
    \nonumber
    \\
    &\;=
    \mathbb{V}{ \rvvg }
    +
    \norm{
      \mathbb{E}{ \rvvg } - \vmu  + \vmu
    }_2^2
    \nonumber
    \\
    &\;=
    \mathbb{V}{ \rvvg }
    +
    \norm{
      \mathbb{E}{ \rvvg } - \vmu  
    }_2^2
    \nonumber
    +
    2 \, {\left(
      \mathbb{E}\rvvg - \vmu 
    \right)}^{\top}
    \vmu
    + 
    \norm{\vmu}^2
    &&\quad\text{\textit{Expanded quadratic}}
    \nonumber
    \\
    &\;\leq
    \mathbb{V}{ \rvvg }
    +
    \norm{
      \mathbb{E}{ \rvvg   } - \vmu 
    }_2^2
    +
    2 \,\norm{
      \mathbb{E}{ \rvvg  } - \vmu
    }_2
    \norm{\vmu}_2
    + 
    \norm{
      \vmu
    }_2^2
    \nonumber
    &&\quad\text{\textit{Cauchy-Shwarz inequality}}
    \\
    &\;=
    \mathbb{V}{ \rvvg  }
    +
    \mathsf{Bias}\left[\rvvg\right]^2
    +
    2 \,\mathsf{Bias}\left[\rvvg\right]
    \norm{\vmu}_2
    + 
    \norm{
      \vmu
    }_2^2.
    \nonumber
    &&\quad\text{\textit{Definition of bias}}
  \end{alignat}
  }
  Meanwhile, by the well-known bias-variance decomposition formula of the mean-squared error, \ding{183} directly follows from \ding{182}.
\end{proofEnd}