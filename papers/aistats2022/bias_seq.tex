
\begin{theoremEnd}{theorem}(Bias of seq.-IMH)\label{thm:bias_seq}
  Assuming \(w^* = \sup_{\vz} \nicefrac{p\left(\vz\mid\vx\right)}{q_{\vlambda_{t}}\left(\vz\right)} < \infty\) and the score function is bounded such that \(\left|\,s(\vz; \vlambda)\,\right| \leq \frac{L}{2}\), the bias of the sequential state estimator with an IMH kernel averaging \(N\) states at iteration \(t\) is bounded as
  {\small
  \[
  \mathrm{Bias}\left[ g_{\mathrm{seq.,\, t}} \right] \leq L \left( 1 - \frac{N}{2 \, w^*} \right) + \mathcal{O}\left({\left(\frac{1}{w^*}\right)}^2\right)
  \]
  }
\end{theoremEnd}
%
\begin{proofEnd}
  We employ a similar proof strategy with the works of~\citet[Theorem 4]{jiang_mcmc_2021}.

  Let us first denote the empirical distribution of the Markov-chain states at iteration \(t\) as
  \begin{align}
    \eta_{\mathrm{seq.},\, t}(\vz) = \frac{1}{N} \sum_{i=1}^N K^{i}(\vz_T, \vz),
  \end{align}
  where \(\vz_{T}\) is the last state of the Markov-chain at the previous SGD iteration.
  Consequently, the estimator can be described as
  \begin{align}
      g_{\mathrm{seq}, t}(\vlambda) = \int s\left(\vz; \vlambda\right) \, \eta_{\mathrm{seq.},\, t}(\vz) \, d\vz.
  \end{align}
  Now,
  \begin{align}
    \DTV{ \eta_{seq.,\, t}(\cdot) }{p\left(\cdot\mid\vx\right)}
    &= \DTV{\frac{1}{N} \sum_{i=1}^N K^{i}(\vz_T, \cdot)}{p\left(\cdot\mid\vx\right)} \\
    &\leq \frac{1}{N} \sum_{i=1}^N  \DTV{K^{i}(\vz_T, \cdot)}{p\left(\cdot\mid\vx\right)} &\text{ (Triangle inequality)}
  \end{align}
 For an IMH kernel with \(w^* < \infty\), the geometric ergodicity of the IMH kernel \citep[Theorem 2.1]{10.2307/2242610} gives the bound
 \begin{align}
   \DTV{K^t(\vz_{0}, \cdot)}{p(\cdot\mid\vx)} \leq {\left(1 - \frac{1}{w^*}\right)}^t.
 \end{align}
 For the SGD step \(t\), \(\vlambda_{t}\) is fixed, temporarily enabling ergodicity to hold.
 Therefore, 
  \begin{align}
    \DTV{ \eta_{\mathrm{seq.},\, t}(\cdot) }{p\left(\cdot\mid\vx\right)}
    &\leq \frac{1}{N} \sum_{i=1}^N {\left( 1 - \frac{1}{w^*} \right)}^i \\
    &=    \frac{1}{N} \sum_{i=1}^N C^i \\
    &=    \frac{1}{N} \left(\frac{ C \left(1 - C^{N}\right)}{1 - C}\right) \\
    &=    \frac{C}{N} \frac{ \left(1 - C^{N}\right) }{1 - C} \\
    &=  \frac{w^* - 1}{N} \left( 1 - {\left( 1 - \frac{1}{w^*} \right) }^N \right).
  \end{align}
  While this bound itself is not very intuitive, by performing a Laurent series expansiona at \(x = \infty\), we obtain a close approximation
  \begin{align}
    \frac{w^* - 1}{N} \left( 1 - {\left( 1 - \frac{1}{w^*} \right) }^N \right)
    \approx 1 - \frac{N+1}{2 w^*} + \mathcal{O}\left({\left(\frac{1}{w^*}\right)}^2\right)
  \end{align}

  Finally, by the definition of the total-variation distance, 
 \begin{align}
   \mathrm{bias}\left[ g_{\mathrm{seq., t}} \right]
   &\leq \sup_{h : \mathcal{Z} \rightarrow \left[ \text{-}\nicefrac{L}{2}, \nicefrac{L}{2} \right]} \left|\, \Esub{\eta_{\mathrm{seq.},\, t}(\cdot)}{h} - \Esub{p(\cdot\mid\vx)}{h} \,\right| \\
   &= L \, \DTV{ \eta_{\mathrm{seq.},\, t}(\cdot) }{p\left(\cdot\mid\vx\right)}  \\
   &\leq L \left( 1 - \frac{N}{2 \, w^*} \right) + \mathcal{O}\left({\left(\frac{1}{w^*}\right)}^2\right).
 \end{align}
\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
