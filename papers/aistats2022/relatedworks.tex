\vspace{-0.05in}
\section{Related Works}\label{section:related}
\vspace{-0.0in}
\paragraph{Inclusive KL minimization}
Our method directly builds on top of MSC~\citep{NEURIPS2020_b2070693}, which minimizes the inclusive KL divergence.
Concurrently,~\citet{pmlr-v124-ou20a} proposed JSA which they for training variational autoencoders with discrete latent variables.
Unlike MSC, JSA can specifically be applied to models with \textit{i.i.d.} data with minibatches.
Before the two, only a few have proposed methods that minimize the inclusive KL using SGD.
Notably,~\citet{DBLP:journals/corr/BornscheinB14} use SNIS for estimating the stochastic gradients, while~\citet{li_approximate_2017} use an MCMC kernel to refine samples from \(q_{\vlambda}(\vz)\) to better resemble samples from \(p(\vz\mid\vx)\).

\vspace{-0.05in}
\paragraph{MCMC for VI}
Not restricted to inclusive KL minimization, MCMC has been widely utilized in VI.
For example,~\citet{pmlr-v37-salimans15, pmlr-v97-ruiz19a} construct alternative divergence bounds from samples of an MCMC sampler.

\vspace{-0.05in}
\paragraph{Adaptive MCMC}
As pointed out by~\citet{pmlr-v124-ou20a}, using \(q_{\vlambda}\) within the MCMC kernel makes score climbing structurally equivalent to adaptive MCMC.
In particular,~\citet{10.1007/s11222-008-9110-y, garthwaite_adaptive_2016} discuss the use of stochastic approximation in adaptive MCMC.
Also,~\citet{andrieu_ergodicity_2006, keith_adaptive_2008, holden_adaptive_2009, giordani_adaptive_2010} specifically discuss adapting the propsosal of IMH kernels.
Most similar to score climbing VI is the work of~\citet{keith_adaptive_2008} where they propose to use cross-entropy minimization~\citep{barbakh_cross_2009}, which is mathematically identical to inclusive VI.

\vspace{-0.05in}
\paragraph{VI for MCMC}
More recently, several other methods that apply variational inference for adapting the MCMC kernel have been developed.
For adapting the proposals of an IMH sampler, \citet{habib2018auxiliary} minimize the exclusive KL divergence while~\cite{neklyudov_metropolishastings_2019} minimize the symmetric KL divergence.
And for HMC,~\citet{zhang_variational_2018, pmlr-v139-campbell21a} have proposed to use score matching, ELBO maximization, and kernelized Stein discrepancy minimization.

%% \vspace{-0.1in}
%% \paragraph{Ergodicity and Inclusive VI}
%% Meanwhile, in the context of MCMC,~\citet{10.2307/2242610} showed that it is necessary to ensure \(\sup_{\vz} w(\vz) = M < \infty\) (finite weight condition) for an IMH kernel to be geometrically ergodic.
%% While this might seem less relevant for inclusive VI, the bound
%% %
%% \vspace{-0.02in}
%% \begin{align}
%%   \DKL{p}{q_{\vlambda}} = \int p(\vz\mid\vx) \log w(\vz)\,d\vz \leq \int p(\vz\mid\vx) \log M \, d\vz = \log M.
%% \end{align}
%% \vspace{-0.02in}
%% %
%% suggests that it is in fact a sufficient condition for the KL divergence to be finite.
%% This condition can easily be violated as shown by \citet{10.1007/s11222-008-9110-y}.
%% To ensure this does not happen,~\citet{giordani_adaptive_2010, holden_adaptive_2009} use proposal distributions of the form of \(w\,q_0(\vz) + (1-w)\,q_{\vlambda}(\vz)\) for some \(0<w<1\) for their adaptive IMH sampler.
%% Here, \(q_0\) is supposed to be a heavy tailed distribution in the spirit of defensive mixtures~\citep{hesterberg_weighted_1995}.
%% %In the benchmark problems we considered, we observed that MSC converges without such precaution.
%% A research direction in the interest of both adaptive MCMC and inclusive VI would be to investigate whether such precaution is actually necessary for convergence.
%% If that is the case, it would be beneficial to consider variational families of heavy-tailed distributions as proposed by~\citet{NEURIPS2018_25db67c5} for exclusive VI.

%Therefore, for problems where MSC is not geometricaly ergodic, inclusive VI would also fail to converge.
%% On the other hand, for problems where MSC converges without problem, defensive mixtures shouldn't be necessary.
%% for problems where \(w(\vz)\) is not bounded, virtually all inclusive VI methods, including SNIS and RWS, will fail to work, as their weights will have very high variance~\citep{mcbook}.
%The boundedness of \(w(\vz)\) is more related to model specification and the selection of the variational family \(\mathcal{Q}\).

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
