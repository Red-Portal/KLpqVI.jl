
\section{Related Works}\label{section:related}
\paragraph{Inclusive KL minimization}
Our method directly builds on top of MSC~\citep{NEURIPS2020_b2070693}, which is a method for minizing the inclusive KL divergence.
Concurrently,~\citet{pmlr-v124-ou20a} has proposed JSA which is conceptually very similar to MSC.
Unlike~\citeauthor{NEURIPS2020_b2070693}, they apply JSA for training variational autoencoders with discrete latent variables.
Also, JSA can only be applied to models with \textit{i.i.d.}, which is more restrictive.
Other than MSC and JSA only a few have been proposed for general VI based on SGD.
Notably,~\citet{DBLP:journals/corr/BornscheinB14} use SNIS for estimating the stochastic gradients, while~\citet{li_approximate_2017} use an MCMC kernel to refine samples from \(q_{\vlambda}(\vz)\) to better resemble samples from \(p(\vz\mid\vx)\).

\paragraph{MCMC for VI}
Other than for minimizing the inclusive KL, MCMC has been widely utilized for VI.
For example,~\citet{pmlr-v97-ruiz19a, pmlr-v37-salimans15} construct alternative divergence bounds from samples from an MCMC sampler.
For the MCMC kernel, both caess used the costly HMC kernel.
It would be interesting to investigate whether these method would benefit from different types of MCMC kernels and estimation schemes as discussed in this work.

\paragraph{Adaptive MCMC}
As pointed out by~\citet{pmlr-v124-ou20a}, MSC is structurally equivalent to adaptive MCMC methods.
Strong resemblence can be found in methods using stochastic approximation for adapting the proposal distribution used inside the MCMC kernel.
In particular,~\citet{10.1007/s11222-008-9110-y, garthwaite_adaptive_2016} discuss the use of stochastic approximation in adaptive MCMC.
Among adaptive MCMC methods,~\citet{andrieu_ergodicity_2006, keith_adaptive_2008, holden_adaptive_2009, giordani_adaptive_2010} specifically discuss adapting the propsosal of IMH kernels.
In particular, \citet{keith_adaptive_2008} propose to use \textit{cross-entroy minimization}~\citep{barbakh_cross_2009}, which is mathematically identical to inclusive VI.
More recently, several other methods that apply variational inference for adapting the MCMC kernel have been developed.
For adapting the proposals of an IMH sampler, \citet{habib2018auxiliary} minimize the exclusive KL divergence while~\cite{neklyudov_metropolishastings_2019} minimize the symmetric KL divergence.
And for HMC,~\citet{zhang_variational_2018, pmlr-v139-campbell21a} have proposed to minimize various other divergence measures.

%% \vspace{-0.1in}
%% \paragraph{Ergodicity and Inclusive VI}
%% Meanwhile, in the context of MCMC,~\citet{10.2307/2242610} showed that it is necessary to ensure \(\sup_{\vz} w(\vz) = M < \infty\) (finite weight condition) for an IMH kernel to be geometrically ergodic.
%% While this might seem less relevant for inclusive VI, the bound
%% %
%% \vspace{-0.02in}
%% \begin{align}
%%   \DKL{p}{q_{\vlambda}} = \int p(\vz\mid\vx) \log w(\vz)\,d\vz \leq \int p(\vz\mid\vx) \log M \, d\vz = \log M.
%% \end{align}
%% \vspace{-0.02in}
%% %
%% suggests that it is in fact a sufficient condition for the KL divergence to be finite.
%% This condition can easily be violated as shown by \citet{10.1007/s11222-008-9110-y}.
%% To ensure this does not happen,~\citet{giordani_adaptive_2010, holden_adaptive_2009} use proposal distributions of the form of \(w\,q_0(\vz) + (1-w)\,q_{\vlambda}(\vz)\) for some \(0<w<1\) for their adaptive IMH sampler.
%% Here, \(q_0\) is supposed to be a heavy tailed distribution in the spirit of defensive mixtures~\citep{hesterberg_weighted_1995}.
%% %In the benchmark problems we considered, we observed that MSC converges without such precaution.
%% A research direction in the interest of both adaptive MCMC and inclusive VI would be to investigate whether such precaution is actually necessary for convergence.
%% If that is the case, it would be beneficial to consider variational families of heavy-tailed distributions as proposed by~\citet{NEURIPS2018_25db67c5} for exclusive VI.

%Therefore, for problems where MSC is not geometricaly ergodic, inclusive VI would also fail to converge.
%% On the other hand, for problems where MSC converges without problem, defensive mixtures shouldn't be necessary.
%% for problems where \(w(\vz)\) is not bounded, virtually all inclusive VI methods, including SNIS and RWS, will fail to work, as their weights will have very high variance~\citep{mcbook}.
%The boundedness of \(w(\vz)\) is more related to model specification and the selection of the variational family \(\mathcal{Q}\).

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
