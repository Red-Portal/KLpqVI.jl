
%\vspace{-0.05in}
\section{Markov-chain Monte Carlo Estimators for Score Climbing Variational Inference}
\vspace{-0.05in}
\subsection{Overview of Estimation Strategies}\label{section:jsa_msc}
%
\vspace{-0.05in}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures/diagram_1.png}
        \caption{Single State Estimator}\label{fig:single}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures/diagram_2.png}
        \caption{Sequential State Estimator}\label{fig:seq}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures/diagram_3.png}
        \caption{Parallel State Estimator (proposed)}\label{fig:par}
    \end{subfigure}
    \caption{Visualization of different ways of combining MCMC with stochastic approximation variational inference.
    The index \(t\) denotes the stochastic approximation iteration.
    The dark circles denote the MCMC samples used for estimating the score gradient at \(t=2\).
    }\label{fig:overview}
\end{figure*}

\paragraph{Overview}
Recently,~\citet{NEURIPS2020_b2070693} and~\citet{pmlr-v124-ou20a} proposed two similar but independent methods for performing inclusive variational inference.
Both methods estimate the score gradient by operating a Markov-chain in parallel with the VI optimization sequence.
Also, they both use MCMC kernels that can effectively used the variational approximation \(q_{\vlambda_t}(\vz)\).
Because of this, both methods are much more efficient compared to previous VI approaches~\citep{pmlr-v97-ruiz19a, pmlr-v70-hoffman17a} that used expensive MCMC kernels such as Hamiltonian Monte Carlo.

\vspace{-0.05in}
\paragraph{Single State Estimator}
In Markovian score climbing (MSC), \citeauthor{NEURIPS2020_b2070693} estimate the score gradient by performing an MCMC iteration and update the parameters such that
\vspace{-0.05in}
\begin{align*}
  &\vz_t \sim K_{\vlambda_{t-1}}\left(\vz_{t-1}, \cdot \right) \\
  &g_{\text{single-CIS}}(\vlambda) = s\,(\vz_t; \vlambda)
\end{align*}
where \(K_{\vlambda_{t-1}}\left(\vz_{t-1}, \cdot\right)\) is a MCMC kernel leaving \(p\,(\vz\mid\vx)\) invariant and \(g_{\text{single}}\left(\vlambda\right)\) denotes the score estimator.
For \(K_{\vlambda_{t-1}}\left(\vz_{t-1}, \cdot\right)\), they propose a new type of kernel inspired by particle MCMC~\citep{andrieu_particle_2010, andrieu_uniform_2018}, the conditional importance sampling (CIS) kernel.
Since the estimator uses \textit{a single state} created by the CIS kernel, we call it the single state estimator with the CIS kernel (single-CIS).
The CIS kernel internally uses \(N\) samples from the \(q_{\vlambda_{t-1}}(\vz)\), hence the dependence on \(\vlambda_{t-1}\).
When compared to MCMC kernels that only use a single sample from \(q_{\vlambda_{t-1}}(\vz)\), it is \(N\) times more expensive, but hopefully, statistically superior.

\vspace{-0.05in}
\paragraph{Sequential State Estimator}
On the other hand, at each SGD iteration \(t\),~\citep{pmlr-v124-ou20a} perform \(N\) sequential Markov-chain transitions and use the average of the intermediate states for estimation.
That is, for the index \(i \in \{1, \ldots, N\}\),
\vspace{-0.05in}
\begin{align*}
  &\vz_{T+i} \sim K_{\vlambda_{t-1}}^i\left(\vz_{T}, \cdot \right) \\
  &g_{\text{seq.-IMH}}(\vlambda) = \frac{1}{N} \sum_{i=1}^N s\,(\vz_{T+i}; \vlambda)
\end{align*}
where \(\vz_T\) is the last Markov-chain state of the previous SGD iteration.
\(K^{i}_{\vz_{t-1}}(\vz_{T}, \cdot)\) denotes the MCMC kernel sequentially applied \(i\) times.
For the MCMC kernel, they use the classic independent Metropolis-Hastings (IMH,~\citealt[Algorithm 25]{robert_monte_2004}~\citealt{hastings_monte_1970}) algorithm, which uses only a single sample from \(q_{\vlambda_{t-1}}(\vz)\) (notice the dependence on \(\vlambda_{t-1}\) just like the aforementioned CIS kernel).
Therefore, the cost of \(N\) state transitions with IMH is similar to the cost of a single transition with CIS.
Since the estimator uses sequential states, we call it the sequential state esimator with the IMH kernel (seq.-IMH)

%\subsection{The Parallel State Estimator}\label{section:overview}

\paragraph{Overview}
The single and sequential state estimators represent two different ways of using a fixed computational budget.
The former uses a single sample generated in an expensive way, while the latter uses multiple samples generated in a cheap way.
Illustrations of the two schemes are provided in~\cref{fig:single,fig:seq}.

\vspace{-0.05in}
\paragraph{Parallel State Estimator}
In this work, we add a new scheme into the mix: \textit{the parallel state estimator}.
Like the sequential state estimator, we use the cheaper IMH kernel, but instead of applying the MCMC kernel \(N\) times to a single chain, we apply the MCMC kernel a single time to \(N\) \textit{parallel Markov-chains}.
That is, for each Markov-chain \(i \in \{1, \ldots, N\}\),
%
\vspace{-0.05in}
\begin{align*}
  &\vz_{t}^{(i)} \sim K_{\vlambda_{t-1}}\big(\vz_{t-1}^{(i)}, \cdot \big) \\
  &g_{\text{par.-IMH}}(\vlambda) = \frac{1}{N} \sum_{i=1}^N s\,(\vz_{t}^{(i)}; \vlambda)
\end{align*}
%
where \(\vz_{t-1}^{(i)}\) is the state of the \(i\)th chain at the previous SGD step.
Computationally speaking, we are still applying \(K\big(\vz_{t-1}^{(i)}\big)\) \(N\) times in total, so the cost is similar to the sequential state estimator.
However, the Markov-chain are \(N\) times shorter, which, in a traditional MCMC view, might seem to result in worse statistical performance.
An illustration of the parallel state estimator is shown in~\cref{fig:par}
Detailed pseudocodes of the considered schemes are provided in the \textit{supplementary material}.

%
\input{table_cost}
%
\vspace{-0.05in}
\paragraph{Computational Cost}
The three scheme using the CIS kernel and the IMH kernel can have different computational cost depending on the parameter \(N\).
The computational costs of each scemes are organized in~\cref{table:cost}.
In the CIS kernel, \(N\) controls the number of internal proposals sampled from \(q_{\vlambda}(\vz)\).
In the sequential and parallel state estimators, the IMH kernel only uses a single sample from \(q_{\vlambda}(\vz)\), but applies the kernel \(N\) times.
When estimating the score, the single state estimator computes \(\nabla_{\vlambda} \log q_{\vlambda}(\vz)\) only once, while for the sequential and parallel state estimators compute it \(N\) times.
However,~\cite{NEURIPS2020_b2070693} also discuss a Rao-Blackwellized version of the CIS kernel, which also computes the gradient \(N\) times.

\subsection{Theoretical Analysis of Bias}\label{section:theory}

%\textcolor{red}{
\vspace{-0.05in}
\paragraph{Adaptive MCMC and Ergodicity}
For bounded functions, a bound on the bias of MCMC estimators can be easily derived from the convergence rates of MCMC kernels as shown by~\citet[Theorem 4]{jiang_mcmc_2021}.
In the context of MSC, the convergence rate of an MCMC kernel is a subtle subject since the kernel is now \(adaptive\) as it depends on \(\vlambda_t\), which is in turn dependent on all of the past MCMC samples.
This is clearly the type of problem adaptive MCMC algorithms have been concerned with~\citep{andrieu_ergodicity_2006}.
However, our setting crucially differs with adaptive MCMC in that our goal is not to obtain asymptotically unbiased samples.
Instead, we use the MCMC samples acquired during each SGD step, in which \(\vlambda_t\) is fixed.
That is, our MCMC kernel is instantaneously not adaptive, and we are thus we are free to use the ergodicity results of these kernels.
However, we note that, as far as Deoblin's condition holds such that \(w^* = \sup_{\vz, \vlambda} \nicefrac{p\left(\vz\mid\vx\right)}{q_{\vlambda}\left(\vz\right)}  < \infty\) and the SGD stepsize sequence satisfies the diminishing adaptation condition~\citep{10.2307/27595854}, the MCMC kernel will indeed result in asymptotically unbised samples.

\paragraph{Boundedness Assumption}
Since convergence rates are defined with the total-variation distance, our bias results assume that the score function is bounded.
That is, \(\norm{\nabla_{\vlambda} \log q_{\vlambda}(\vz)} < L\) for any \(\vlambda\).
This boundedness assumption is reasonable since theoretical guarentees of SGD often assume Lipschitz-continuity of the gradients, from which boundedness follows as a consequence.
%}

%For the seq.-IMH estimator, the bias is bounded geometrically with the number of states \(N\) and the number of SGD iteration \(t\).
%
\input{bias_seq}
%
\input{bias_par}
%
Finally, we analyze the bias of the single-CIS estimator.
Our proof is based on the fact that the CIS kernel is identical to the iterated sampling importance resampling (i-SIR) algorithm by~\citet{andrieu_uniform_2018}.
Especially, we utilize the the convergence rate of the i-SIR kernel.
In addition, we note that the CIS kernel can be reformulated as an accept-reject type kernel that uses Barker's acceptance function~\citep{barker_monte_1965}.
With this perspective, it is identical to the ensemble MCMC sampler independently proposed by~\citet{austad_parallel_2007, neal_mcmc_2011a}.
It can also be found in the review on multiple-try MCMC methods by~\citet[Table 12]{martino_review_2018a}.
%
\input{bias_cis}
%
\paragraph{Reducing Bias by Increasing \(N\)}
Our results suggest that, for the seq.-IMH estimator and single-CIS estimator, increasing \(N\) improves the bias decrease rate.
However, it is important to note that all bias bounds depend on \(w^*\).
By the following proposition, in the initial stages of VI where the KL divergence is large, \(w^*\) is bounded below exponentially.
%
\begin{proposition}
  \(w^* = \sup_{\vz} \nicefrac{p\left(\vz\mid\vx\right)}{q_{\vlambda}\left(\vz\right)} \) is bounded below expoentially by the KL divergence such that
  \(
  \exp\left(\DKL{p\left(\cdot\mid\vx\right)}{ q_{\vlambda}\left(\cdot\right) }\right) \leq w^*.
  \)
  \begin{proof}
    \(
    \DKL{p\left(\cdot\mid\vx\right)}{ q_{\vlambda}\left(\cdot\right) }
    = \int p\left(\vz\mid\vx\right) \log \frac{p\left(\vz\mid\vx\right)}{q_{\vlambda}\left(\vz\right)}\,d\vz
    \leq \int p\left(\vz\mid\vx\right) \log M \, d\vz = \log w^*
    \)
  \end{proof}
\end{proposition}
%
Thus, in the initial steps of VI, the bias decrease rate \(C\) will be close to 1, making the effect of \(N\) minimal.
On the other hand, in the later steps of VI, the KL divergence is small.
However, in this case \(t\) will be larege, therefore making the bias small regardless.

\subsection{Theoretical Analysis of Variance}
Geometric ergodicity of the CIS and IMH kernels guarentee that the bias will be small regardless of the kernel and parameter \(N\).
In contrast, variance often dominate the mean-square error of MCMC estimators.
Therefore, analyzing the variance will be more relevant in practice.

\input{var_seq}
\input{variance_approx}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
