
\prAtEndRestateii*
\label{proofsection:prAtEndii}\begin{proof}[Proof of \autoref{thm:prAtEndii}]\phantomsection\label{proof:prAtEndii}We employ a similar proof strategy with the works of~\citet [Theorem 4]{jiang_mcmc_2021}. \par Let us first denote the empirical distribution of the Markov-chain states at iteration \(t\) as \begin {align} \eta _{\mathrm {seq.},\, t}(\vz ) = \frac {1}{N} \sum _{i=1}^N K^{i}(\vz _T, \vz ), \end {align} where \(\vz _{T}\) is the last state of the Markov-chain at the previous SGD iteration. Consequently, the estimator can be described as \begin {align} g_{\mathrm {seq}, t}(\vlambda ) = \int s\left (\vz ; \vlambda \right ) \, \eta _{\mathrm {seq.},\, t}(\vz ) \, d\vz . \end {align} Now, \begin {align} \DTV { \eta _{seq.,\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} &= \DTV {\frac {1}{N} \sum _{i=1}^N K^{i}(\vz _T, \cdot )}{p\left (\cdot \mid \vx \right )} \\ &\leq \frac {1}{N} \sum _{i=1}^N \DTV {K^{i}(\vz _T, \cdot )}{p\left (\cdot \mid \vx \right )} &\text { (Triangle inequality)} \end {align} For an IMH kernel with \(w^* < \infty \), the geometric ergodicity of the IMH kernel \citep [Theorem 2.1]{10.2307/2242610} gives the bound \begin {align} \DTV {K^t(\vz _{0}, \cdot )}{p(\cdot \mid \vx )} \leq {\left (1 - \frac {1}{w^*}\right )}^t. \end {align} For the SGD step \(t\), \(\vlambda _{t}\) is fixed, temporarily enabling ergodicity to hold. Therefore, \begin {align} \DTV { \eta _{\mathrm {seq.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} &\leq \frac {1}{N} \sum _{i=1}^N {\left ( 1 - \frac {1}{w^*} \right )}^i \\ &= \frac {1}{N} \sum _{i=1}^N C^i \\ &= \frac {1}{N} \left (\frac { C \left (1 - C^{N}\right )}{1 - C}\right ) \\ &= \frac {C}{N} \frac { \left (1 - C^{N}\right ) }{1 - C} \\ &\leq \frac {1}{N} \frac { C }{1 - C} \\ &= \frac {1}{N} \frac { 1 - \nicefrac {1}{w^*} }{ \nicefrac {1}{w^*} } \\ &= \frac {1}{N} \left ( w^* - 1 \right ) \end {align} \par Finally, by the definition of the total-variation distance, \begin {align} \mathrm {bias}\left [ g_{\mathrm {seq., t}} \right ] &\leq \DTV {\eta _{seq.,\, t}(\cdot )}{p(\cdot \mid \vx )} \\ &\leq \sup _{h : \mathcal {Z} \rightarrow \left [ \text {-}\nicefrac {L}{2}, \nicefrac {L}{2} \right ]} \left |\, \Esub {\eta _{\mathrm {seq.},\, t}(\cdot )}{h} - \Esub {p(\cdot \mid \vx )}{h} \,\right | \\ &= L \, \DTV { \eta _{\mathrm {seq.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} \\ &\leq \frac {L}{N} \left ( w^* - 1 \right ). \end {align}\end{proof}
\prAtEndRestateiii*
\label{proofsection:prAtEndiii}\begin{proof}[Proof of \autoref{thm:prAtEndiii}]\phantomsection\label{proof:prAtEndiii}We denote the empirical distribution of the Markov-chain states at iteration \(t\) as \begin {align} \eta _{\mathrm {par.},\, t}(\vz ) = \frac {1}{N} \sum _{i=1}^N K\left (\vz _{t-1}^{(i)}, \vz \right ). \end {align} and consequently, \begin {align} g_{\mathrm {par.}, t}(\vlambda ) = \int s\left (\vz ; \vlambda \right ) \, \eta _{\mathrm {par.},\, t}(\vz ) \, d\vz . \end {align} Similarly with~\cref {thm:bias_seq}, \begin {align} \DTV { \eta _{\mathrm {par.},\, t}(\vz ) }{p\left (\cdot \mid \vx \right )} &= \DTV {\frac {1}{N} \sum _{i=1}^N K\left (\vz _{t-1}^{(i)}, \vz \right )}{p\left (\cdot \mid \vx \right )} \\ &\leq \frac {1}{N} \sum _{i=1}^N \DTV {K\left (\vz _{t-1}^{(i)}, \cdot \right )}{p\left (\cdot \mid \vx \right )} &\text {(Triangle inequality)} \\ &= \DTV {K(\vz _t, \cdot )}{p\left (\cdot \mid \vx \right )} &\text {(Uniform ergodicity)} \\ &\leq 1 - \frac {1}{w^*}. \end {align} And, finally the bias is given as \begin {align} \mathrm {bias}\left [ g_{\mathrm {par., t}} \right ] &\leq L \DTV {\eta _{\mathrm {par.},\, t}(\cdot )}{p(\cdot \mid \vx )} \\ &\leq L \sup _{h : \mathcal {Z} \rightarrow \left [ \text {-}\nicefrac {L}{2}, \nicefrac {L}{2} \right ]} \left |\, \Esub {\eta _{\mathrm {par.},\, t}(\cdot )}{h} - \Esub {p(\cdot \mid \vx )}{h} \,\right | \\ &= L\, \DTV { \eta _{\mathrm {par.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} \\ &\leq L \, \left (1 - \frac {1}{w^*}\right ) \end {align}\end{proof}
\prAtEndRestateiv*
\label{proofsection:prAtEndiv}\begin{proof}[Proof of \autoref{thm:prAtEndiv}]\phantomsection\label{proof:prAtEndiv}Let us first denote the empirical distribution of the Markov-chain states at iteration \(t\) as \begin {align} \eta _{\mathrm {cis.},\, t}(\vz ) = K\left (\vz _{t-1}, \vz \right ), \end {align} and consequently, \begin {align} g_{\mathrm {cis}, t}(\vlambda ) = \int s\left (\vz ; \vlambda \right ) \, \eta _{\mathrm {cis.},\, t}(\vz ) \, d\vz . \end {align} \par The CIS sampler is identical to the iterated sampling importance resampling (i-SIR) algorithm described by~\citet {andrieu_uniform_2018}. They showed that the i-SIR kernel achieves a geometric convergence rate such that \begin {align} \DTV {K^{t}\left (\vz _{t-1},\cdot \right )}{p\left (\cdot \mid \vx \right )} &\leq {\left (1 - \frac {N - 1}{2\,w^* + N - 2}\right )}^t. \end {align} From this, the bound can be shown as \begin {align} \mathrm {bias}\left [ g_{\mathrm {cis., t}} \right ] &\leq \DTV {\eta _{cis.,\, t}(\cdot )}{p(\cdot \mid \vx )} \\ &\leq \sup _{h : \mathcal {Z} \rightarrow \left [ \text {-}\nicefrac {L}{2}, \nicefrac {L}{2} \right ]} \left |\, \Esub {\eta _{\mathrm {cis.},\, t}(\cdot )}{h} - \Esub {p(\cdot \mid \vx )}{h} \,\right | \\ &= L \, \DTV { \eta _{\mathrm {cis.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} \\ &\leq L \, {\left (1 - \frac {N - 1}{2\,w^* + N - 2}\right )} \\ &\leq L \, {\left (1 - \frac {N}{w^*}\right )} &(\text {Monotonicity}) \end {align} given that \(N > 2\).\end{proof}
\prAtEndRestatev*
\label{proofsection:prAtEndv}\begin{proof}[Proof of \autoref{thm:prAtEndv}]\phantomsection\label{proof:prAtEndv}\begin {align} &\DKL {p\left (\cdot \mid \vx \right )}{ q_{\vlambda }\left (\cdot \right ) } \\ &= \int p\left (\vz \mid \vx \right ) \log \frac {p\left (\vz \mid \vx \right )}{q_{\vlambda }\left (\vz \right )}\,d\vz \\ &\leq \int p\left (\vz \mid \vx \right ) \log M \, d\vz \\ &= \log w^* \end {align}\end{proof}
\prAtEndRestatevi*
\label{proofsection:prAtEndvi}\begin{proof}[Proof of \autoref{thm:prAtEndvi}]\phantomsection\label{proof:prAtEndvi}By the law of total variance, \begin {align} \Vsub {q_{\vlambda }}{g_{\mathrm {single}}} &= \V { \Esub {q_{\vlambda }}{ g \mid \vz _{t-1} }} + \E {\Vsub {q_{\vlambda }}{ g \mid \vz _{t-1}}} \\ &= \V { \Esub {q_{\vlambda }}{ \E { g \mid \vz _{t-1}, \vz ^{(1:N)} } } }\nonumber \\ &\quad + \E { \Vsub {q_{\vlambda }}{ \E { g \mid \vz _{t-1}, \vz ^{(1:N)}} } + \Esub {q_{\vlambda }}{ \V { g \mid \vz _{t-1}, \vz ^{(1:N)}} } } \\ &= \V { \Esub {q_{\vlambda }}{ g_{\mathrm {IS}} \mid \vz _{t-1} } } + \E { \Vsub {q_{\vlambda }}{ g_{\mathrm {IS}} \mid \vz _{t-1} } }\nonumber \\ &\quad + \E { \Esub {q_{\vlambda }}{ \V { g \mid \vz _{t-1}, \vz ^{(1:N)}} } } \\ &= \V { \Esub {q_{\vlambda }}{ g_{\mathrm {IS}} \mid \vz _{t-1} } } + \E { \Vsub {q_{\vlambda }}{ g_{\mathrm {IS}} \mid \vz _{t-1} } }\nonumber \\ &\quad + \E { \Esub {q_{\vlambda }}{ \E { g^2_{\mathrm {IS}} \mid \vz _{t-1}, \vz ^{(1:N)} } } + {\left (\, \E { g_{\mathrm {IS}} \mid \vz _{t-1}, \vz ^{(1:N)} } \,\right )}^2 } \end {align} where \(g_{\mathrm {IS}}\) denotes the importance sampling estimator given \(\vz ^{(1:N)}\) and \(\vz ^{(0)} = \vz _{t-1}\) defined as \(g_{\mathrm {IS}} = \sum _{i=0}^N \frac {w\left (\vz ^{(i)}\right )}{\sum _{i=0}^N w\left (\vz ^{(i)}\right )} s\left (\vz ^{(i)}\right ) \). \par \par If we use Rao-Blackwellization, the last variance term vanishes, and we are left with \begin {align} \Vsub {q_{\vlambda }}{g_{\mathrm {single}}} = \V { \Esub {q_{\vlambda }}{ g_{\mathrm {IS}} \mid \vz _{t-1} } } + \E { \Vsub {q_{\vlambda }}{ g_{\mathrm {IS}} \mid \vz _{t-1} } }. \end {align} \par \par Therefore, in expectation, \par Now, \begin {align} &\Esub {q_{\vlambda }}{ g_{\mathrm {IS}} \mid \vz _{t-1}} \\ &= \Esub {q_{\vlambda }}{ \frac {\sum _{i=0}^N w\left (\vz ^{(i)}\right ) s\left (\vz ^{(i)}\right )}{\sum _{i=0}^N w\left (\vz ^{(i)}\right )} \,\middle \vert \, \vz _{t-1} } \\ &= \Esub {q_{\vlambda }}{ \frac {\sum _{i=1}^N w\left (\vz ^{(i)}\right ) s\left (\vz ^{(i)}\right )}{\sum _{i=1}^N w\left (\vz ^{(i)}\right ) + w\left (\vz _{t-1}\right )} + \frac {w\left (\vz _{t-1}\right )}{\sum _{i=1}^N w\left (\vz ^{(i)}\right ) + w\left (\vz _{t-1}\right )} s\left (\vz _{t-1}\right ) \,\middle \vert \, \vz _{t-1}} \\ &= \Esub {q_{\vlambda }}{ \frac { \sum _{i=1}^N w\left (\vz ^{(i)}\right ) s\left (\vz ^{(i)}\right )}{\sum _{i=1}^N w\left (\vz ^{(i)}\right ) + w\left (\vz _{t-1}\right )} \,\middle \vert \, \vz _{t-1}} \\ &\qquad \qquad + \Esub {q_{\vlambda }}{ \frac {w\left (\vz _{t-1}\right )}{\sum _{i=1}^N w\left (\vz ^{(i)}\right ) + w\left (\vz _{t-1}\right )} \,\middle \vert \, \vz _{t-1}} s\left (\vz _{t-1}\right ) \\ &= \Esub {q_{\vlambda }}{ \frac {\sum _{i=1}^N w\left (\vz ^{(i)}\right ) s\left (\vz ^{(i)}\right )}{\sum _{i=1}^N w\left (\vz ^{(i)}\right ) + w\left (\vz _{t-1}\right )} \,\middle \vert \, \vz _{t-1}} + r\left (\vz _{t-1}\right ) s\left (\vz _{t-1}\right ) \\ &= \Esub {q_{\vlambda }}{ \frac {\sum _{i=1}^N w\left (\vz ^{(i)}\right )}{\sum _{i=1}^N w\left (\vz ^{(i)}\right ) + w\left (\vz _{t-1}\right )} \frac {\sum _{i=1}^N w\left (\vz ^{(i)}\right ) s\left (\vz ^{(i)}\right )}{\sum _{i=1}^N w\left (\vz ^{(i)}\right )} \,\middle \vert \, \vz _{t-1}} + r\left (\vz _{t-1}\right ) s\left (\vz _{t-1}\right ) \end {align} \par Write \begin {align} \mathbb {V}_{q_\lambda }[f|\vz _{t-1}] = \mathbb {V}_{q_\lambda }\left [\frac {\sum _{i=1}^N w(\vz ^{(i)})}{\sum _{i=0}^N w(\vz ^{(i)})} \frac {\sum _{i=1}^N w(\vz ^{(i)})f(\vz ^{(i)})}{\sum _{i=1}^N w(\vz ^{(i)})} + \left .\frac {w(\vz _{t-1})}{\sum _{i=0}^N w(\vz ^{(i)})} f(\vz _{t-1}) \right \vert \vz _{t-1} \right ]. \end {align} Note that if $a>0$, then we can approximate the function $\sum _{i=1}^N x_i/(a+\sum _{i=1}^N x_i)$ using the first-order Taylor series expansion about $(Z,\dots ,Z)$ by $$ \frac {\sum _{i=1}^N x_i}{a+\sum _{i=1}^N x_i} \approx \frac {NZ}{a+NZ}+\sum _{i=1}^N \frac {a}{(a+NZ)^2} (x_i -Z). $$ Hence, given $\vz _{t-1}$, we approximate $\sum _{i=1}^N w(\vz ^{(i)})/\sum _{i=0}^N w(\vz ^{(i)})$ by \begin {align} \frac {\sum _{i=1}^N w(\vz ^{(i)})}{\sum _{i=0}^N w(\vz ^{(i)})} &\approx \frac {NZ}{w(\vz _{t-1})+NZ} + \sum _{i=1}^N \frac {w(\vz _{t-1})}{(w(\vz _{t-1})+ NZ)^2} (w(\vz ^{(i)})-Z) \\ & = \frac {N^2 Z^2 + w(\vz _{t-1}) \sum _{i=1}^N w(\vz ^{(i)})}{(w(\vz _{t-1})+NZ)^2} \end {align} so that \begin {align} \mathbb {V}_{q_\lambda }[f|\vz _{t-1}] \approx \mathbb {V}_{q_\lambda } \left [\frac {N^2Z^2}{(w(\vz _{t-1})+NZ)^2} f_{IS} + \frac {w(\vz _{t-1})}{(w(\vz _{t-1})+NZ)^2}\sum _{i=1}^N w(\vz ^{(i)})f(\vz ^{(i)})\right . \\ \left . + \left . \frac {w(\vz _{t-1})}{\sum _{i=0}^N w(\vz ^{(i)})} f(\vz _{t-1}) \right \vert \vz _{t-1} \right ]. \end {align} Observe that $\sum _{i=1}^N w(\vz ^{(i)}) f(\vz ^{(i)}) = O(N)$ since $\{\vz ^{(1)}, \dots , \vz ^{(N)}\}$ are independent and identically distributed and that $w(\vz _{t-1})f(\vz _{t-1})/\sum _{i=0}^N w(\vz ^{(i)})=o(N)$. Combining these, we obtain \begin {align} \Vsub {q_{\vlambda }}{ f \mid \vz _{t-1}, \vz ^{(1:N)} } \approx \frac {N^4 Z^4}{(w(\vz _{t-1}) + NZ)^4} \Vsub {q_{\vlambda }}{ f_{\text {IS}} \,\middle \vert \, \vz _{t-1} }, \end {align} as was to be shown.\end{proof}
