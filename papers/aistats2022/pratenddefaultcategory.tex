
\prAtEndRestateii*
\label{proofsection:prAtEndii}\begin{proof}[Proof of \autoref{thm:prAtEndii}]\phantomsection\label{proof:prAtEndii}We employ a similar proof strategy with the works of~\citet [Theorem 4]{jiang_mcmc_2021}. \par Let us first denote the empirical distribution of the Markov-chain states at iteration \(t\) as \begin {align} \eta _{\mathrm {seq.},\, t}(\vz ) = \frac {1}{N} \sum _{i=1}^N K^{i}(\vz _T, \vz ), \end {align} where \(\vz _{T}\) is the last state of the Markov-chain at the previous SGD iteration. Consequently, the estimator can be described as \begin {align} g_{\mathrm {seq}, t}(\vlambda ) = \int s\left (\vz ; \vlambda \right ) \, \eta _{\mathrm {seq.},\, t}(\vz ) \, d\vz . \end {align} Now, \begin {align} \DTV { \eta _{seq.,\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} &= \DTV {\frac {1}{N} \sum _{i=1}^N K^{i}(\vz _T, \cdot )}{p\left (\cdot \mid \vx \right )} \\ &\leq \frac {1}{N} \sum _{i=1}^N \DTV {K^{i}(\vz _T, \cdot )}{p\left (\cdot \mid \vx \right )} &\text { (Triangle inequality)} \end {align} For an IMH kernel with \(w^* < \infty \), the geometric ergodicity of the IMH kernel \citep [Theorem 2.1]{10.2307/2242610} gives the bound \begin {align} \DTV {K^t(\vz _{0}, \cdot )}{p(\cdot \mid \vx )} \leq {\left (1 - \frac {1}{w^*}\right )}^t. \end {align} For the SGD step \(t\), \(\vlambda _{t}\) is fixed, temporarily enabling ergodicity to hold. Therefore, \begin {align} \DTV { \eta _{\mathrm {seq.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} &\leq \frac {1}{N} \sum _{i=1}^N {\left ( 1 - \frac {1}{w^*} \right )}^i \\ &= \frac {1}{N} \sum _{i=1}^N C^i \\ &= \frac {1}{N} \left (\frac { C \left (1 - C^{N}\right )}{1 - C}\right ) \\ &= \frac {C}{N} \frac { \left (1 - C^{N}\right ) }{1 - C} \\ &= \frac {w^* - 1}{N} \left ( 1 - {\left ( 1 - \frac {1}{w^*} \right ) }^N \right ). \end {align} While this bound itself is not very intuitive, by performing a Laurent series expansiona at \(x = \infty \), we obtain a close approximation \begin {align} \frac {w^* - 1}{N} \left ( 1 - {\left ( 1 - \frac {1}{w^*} \right ) }^N \right ) \approx 1 - \frac {N+1}{2 w^*} + \mathcal {O}\left ({\left (\frac {1}{w^*}\right )}^2\right ) \end {align} \par Finally, by the definition of the total-variation distance, \begin {align} \mathrm {bias}\left [ g_{\mathrm {seq., t}} \right ] &\leq \sup _{h : \mathcal {Z} \rightarrow \left [ \text {-}\nicefrac {L}{2}, \nicefrac {L}{2} \right ]} \left |\, \Esub {\eta _{\mathrm {seq.},\, t}(\cdot )}{h} - \Esub {p(\cdot \mid \vx )}{h} \,\right | \\ &= L \, \DTV { \eta _{\mathrm {seq.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} \\ &\leq L \left ( 1 - \frac {N}{2 \, w^*} \right ) + \mathcal {O}\left ({\left (\frac {1}{w^*}\right )}^2\right ). \end {align}\end{proof}
\prAtEndRestateiii*
\label{proofsection:prAtEndiii}\begin{proof}[Proof of \autoref{thm:prAtEndiii}]\phantomsection\label{proof:prAtEndiii}We denote the empirical distribution of the Markov-chain states at iteration \(t\) as \begin {align} \eta _{\mathrm {par.},\, t}(\vz ) = \frac {1}{N} \sum _{i=1}^N K\left (\vz _{t-1}^{(i)}, \vz \right ). \end {align} and consequently, \begin {align} g_{\mathrm {par.}, t}(\vlambda ) = \int s\left (\vz ; \vlambda \right ) \, \eta _{\mathrm {par.},\, t}(\vz ) \, d\vz . \end {align} Similarly with~\cref {thm:bias_seq}, \begin {align} \DTV { \eta _{\mathrm {par.},\, t}(\vz ) }{p\left (\cdot \mid \vx \right )} &= \DTV {\frac {1}{N} \sum _{i=1}^N K\left (\vz _{t-1}^{(i)}, \vz \right )}{p\left (\cdot \mid \vx \right )} \\ &\leq \frac {1}{N} \sum _{i=1}^N \DTV {K\left (\vz _{t-1}^{(i)}, \cdot \right )}{p\left (\cdot \mid \vx \right )} &\text {(Triangle inequality)} \\ &= \DTV {K(\vz _t, \cdot )}{p\left (\cdot \mid \vx \right )} &\text {(Uniform ergodicity)} \\ &\leq 1 - \frac {1}{w^*}. \end {align} And, finally the bias is given as \begin {align} \mathrm {Bias}\left [ g_{\mathrm {par., t}} \right ] &\leq \sup _{h : \mathcal {Z} \rightarrow \left [ \text {-}\nicefrac {L}{2}, \nicefrac {L}{2} \right ]} \left |\, \Esub {\eta _{\mathrm {par.},\, t}(\cdot )}{h} - \Esub {p(\cdot \mid \vx )}{h} \,\right | \\ &= L\, \DTV { \eta _{\mathrm {par.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} \\ &\leq L \, \left (1 - \frac {1}{w^*}\right ) \end {align}\end{proof}
\prAtEndRestateiv*
\label{proofsection:prAtEndiv}\begin{proof}[Proof of \autoref{thm:prAtEndiv}]\phantomsection\label{proof:prAtEndiv}Let us first denote the empirical distribution of the Markov-chain states at iteration \(t\) as \begin {align} \eta _{\mathrm {cis.},\, t}(\vz ) = K\left (\vz _{t-1}, \vz \right ), \end {align} and consequently, \begin {align} g_{\mathrm {cis}, t}(\vlambda ) = \int s\left (\vz ; \vlambda \right ) \, \eta _{\mathrm {cis.},\, t}(\vz ) \, d\vz . \end {align} \par The CIS sampler is identical to the iterated sampling importance resampling (i-SIR) algorithm described by~\citet {andrieu_uniform_2018}. They showed that the i-SIR kernel achieves a geometric convergence rate such that \begin {align} \DTV {K^{t}\left (\vz _{t-1},\cdot \right )}{p\left (\cdot \mid \vx \right )} &\leq {\left (1 - \frac {N - 1}{2\,w^* + N - 2}\right )}^t. \end {align} From this, the bound can be shown as \begin {align} \mathrm {bias}\left [ g_{\mathrm {cis., t}} \right ] &\leq \sup _{h : \mathcal {Z} \rightarrow \left [ \text {-}\nicefrac {L}{2}, \nicefrac {L}{2} \right ]} \left |\, \Esub {\eta _{\mathrm {cis.},\, t}(\cdot )}{h} - \Esub {p(\cdot \mid \vx )}{h} \,\right | \\ &= L \, \DTV { \eta _{\mathrm {cis.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} \\ &\leq L \, {\left (1 - \frac {N - 1}{2\,w^* + N - 2}\right )} \end {align} given that \(N > 2\).\end{proof}
\prAtEndRestatev*
\label{proofsection:prAtEndv}\begin{proof}[Proof of \autoref{thm:prAtEndv}]\phantomsection\label{proof:prAtEndv}\begin {align} &\DKL {p\left (\cdot \mid \vx \right )}{ q_{\vlambda }\left (\cdot \right ) } \\ &= \int p\left (\vz \mid \vx \right ) \log \frac {p\left (\vz \mid \vx \right )}{q_{\vlambda }\left (\vz \right )}\,d\vz \\ &\leq \int p\left (\vz \mid \vx \right ) \log w^* \, d\vz \\ &= \log w^* \end {align}\end{proof}
