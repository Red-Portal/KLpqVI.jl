
\section{Introduction}
Given some observed data \(\vx\), Bayesian inference aims to analyze \(p\,(\vz\mid\vx)\) given the unnormalized joint density \(p\,(\vz,\,\vx)\) where the relationship is given by Bayes' rule such that \(p\,(\vz\mid\vx) = \frac{p\,(\vz,\,\vx)}{Z} \propto p\,(\vz,\,\vx)\).
Instead of working directly with the target distribution \(p\,(\vz\mid\vx)\), variational inference (VI,~\citealt{jordan_introduction_1999, blei_variational_2017, zhang_advances_2019}) searches for a variational approximation \(q_{\lambda}(\vz) \in \mathcal{Q}\) similar to \(p\,(\vz\mid\vx)\) according to a discrepancy measure \(D\,(p, q)\).

Naturally, choosing a good discrepancy measure, or objective function, is a critical part of the problem.
This fact had lead to a quest for good divergence measures~\citep{NIPS2016_7750ca35, NIPS2017_35464c84, NEURIPS2018_1cd138d0, pmlr-v97-ruiz19a}.
So far, the exclusive KL divergence \(\DKL{q_{\lambda}}{p}\) (or reverse KL divergence) has been used ``exclusively'' among various discrepancy measures.
This is partly because the exclusive KL is defined as an average over \(q_{\lambda} \in \mathcal{Q}\), which can be estimated efficiently.
By contrast, the inclusive KL is defined as
%
\vspace{-0.02in}
\begin{align}
  %% \DKL{p}{q_{\lambda}} = \int p\,(\vz\mid\vx) \log\big(\, p\,(\vz\mid\vx)/q_{\lambda}(\vz) \,\big)\,d\vz
  %% = \Esub{p(\vz\mid\vx)}{\log\big(\, p\,(\vz\mid\vx)/q_{\lambda}(\vz) \,\big) } \label{eq:klpq}
  \DKL{p}{q_{\lambda}} = \int p\,(\vz\mid\vx) \log \frac{p\,(\vz\mid\vx)\,}{\,q_{\lambda}(\vz)} \,d\vz
  = \Esub{p(\vz\mid\vx)}{\log \frac{p\,(\vz\mid\vx)\,}{\,q_{\lambda}(\vz)} } \label{eq:klpq}
\end{align}
\vspace{-0.02in}
%
where the average is taken over \(p\,(\vz\mid\vx)\). 
Interestingly, this is a chicken-and-egg problem, and minimizing~\eqref{eq:klpq} has drawn the attention of researchers because it can overcome some known limitations of the exclusive KL~\citep{minka2005divergence, mackay_local_2001}.

For performing inclusive VI,~\citet{NEURIPS2020_b2070693, pmlr-v124-ou20a} recently proposed \textit{Markovian score climbing} (MSC), which is a blend of Markov-chain Monte Carlo (MCMC) and variational inference.
In MSC, stochastic gradients of the inclusive KL are obtained by operating a Markov-chain in parallel with the VI optimizer.
In this paper, we find an interesting property of MSC when it is combined with specific types of MCMC kernels.
Specifically, we show that \textit{independent Metropolis-Hastings} (IMH,~\citealt{robert_monte_2004}) type kernels can automatically trade off bias and variance when used for MSC.
This family of kernels includes the \textit{condition importance sampling} (CIS,~\citealt{NEURIPS2020_b2070693}) kernel, which was originally proposed for MSC.
Surprisingly, this automatic tradeoff property is unique to IMH type kernels and does not occur in MCMC kernels with state-dependent proposals such as Hamiltonian Monte Carlo (HMC,~\citealt{duane_hybrid_1987, neal_mcmc_2011, betancourt_conceptual_2017}).

Following our analysis of the CIS kernel, we also show that its performance can degrade with the number of proposals (which is equivalent to the \textit{per-transition computational budget}) used in each Markov-chain transition.
As a simple solution to this, we propose to use parallel IMH (MSC-PIMH) chains, which reduces variance with the same amount of computation.
We evaluate the performance of MSC with PIMH against other inclusive VI~\citep{DBLP:journals/corr/BornscheinB14, NEURIPS2020_b2070693} and exclusive VI~\citep{pmlr-v33-ranganath14, JMLR:v18:16-107} methods.
%Finally, some interesting connections with adaptive MCMC methods~\citep{10.1007/s11222-008-9110-y} are discussed.

\vspace{-0.1in}
\paragraph{Contribution Summary}
\begin{enumerate*}[label=\textbf{(\roman*)}]
\item We show that IMH type kernels (which include the CIS kernel originally used in MSC; \textbf{\cref{section:cis_imh}}) automatically perform bias-variance tradeoff (\textbf{\cref{section:bias_variance}}).
\item We show that increasing the computation budget of the CIS kernel may \textit{increase} its variance.
  To overcome this limication, we propose to use parallel IMH (PIMH) (\textbf{\cref{section:cis_bias}}).
\item We evaluate the performance of MSC with PIMH against other inclusive and exclusive VI methods (\textbf{\cref{section:eval}}).
%\item We discuss connections with adaptive IMH methods (\textbf{\cref{section:related}}).
\end{enumerate*}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
