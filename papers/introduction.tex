
\section{Introduction}
%Searching for good discrepancy measures for variational inference is 
Variational inference (VI,~\citealt{jordan_introduction_1999, blei_variational_2017}) is a method of converting Bayesian inference into an optimization problem.
Instead of working directly with our target distribution \(p\,(\vz\mid\vx)\), we find a \textit{variational approximation} \(q_{\lambda}(\vz) \in \mathcal{Q}\) closely approximating \(p\,(\vz\mid\vx)\) according to a discrepancy measure \(D(p, q)\).
Naturally, choosing a good discrepancy measure, or obejctive function, is a critical part of the problem.
This fact had lead to the quest for good divergence measures~\citep{NIPS2016_7750ca35, NIPS2017_35464c84, pmlr-v97-ruiz19a}.
However, until now, the exclusive KL divergence \(\DKL{q_{\lambda}}{p}\) (or reverse KL divergence) has seen ``exclusive'' use among various discrepancy measures.
This is partly due to the fact that the exclusive KL is defined as an average over \(q_{\lambda} \in \mathcal{Q}\) which can often be estimated efficiently.
In contrast, the inclusive KL is defined as
%
\begin{align}
  \DKL{p}{q_{\lambda}} = \int p\,(\vz\mid\vx) \log\big(\, p\,(\vz\mid\vx)/q_{\lambda}(\vz) \,\big)\,d\vz
  = \Esub{p(\vz\mid\vx)}{\log\big(\, p\,(\vz\mid\vx)/q_{\lambda}(\vz) \,\big) } \label{eq:klpq}
\end{align}
%
where the average is taken cover \(p\), which is the integral we wish to solve in the first place.
Despite this chicken-and-egg problem, minimizing the inclusive KL has alluded researchers as it does not have some limitations of the exclusive KL divergence~\citep{minka2005divergence, mackay_local_2001}.

For performing inclusive VI,~\citet{NEURIPS2020_b2070693} recently proposed \textit{Markovian score climbing} (MSC), which is a blend of Markov-chain Monte Carlo (MCMC) and variational inference.
In MSC, stochastic gradients of the inclusive KL are obtained by operating a Markov-chain in parallel with the VI optimizer.

 An interesting property of MSC emerges when combined

In this paper, we show that, when combined with an MCMC kernel that uses the varitional approximation \(q_{\vlambda}\) for generating the proposals, MSC shows very interesting properties.
Specifically, \textit{independent Metropolis-Hastings} (IMH,~\citealt{robert_monte_2004}) type kernels which include the \textit{condition importance sampling} (CIS,~\citealt{NEURIPS2020_b2070693}) kernel originally used in MSC, are able to automatically trade-off bias and variance.
Surprisingly, this property is unique to IMH type kernels, and other types of state-denpendent MCMC kernels such as Hamiltonian Monte Carlo (HMC,~\citealt{duane_hybrid_1987, neal_mcmc_2011, betancourt_conceptual_2017}) do not enjoy similar properties.

Our analysis also reveals that, in certain conditions, the performance of CIS kernel worsens with the number of proposals (or computational budget) used in each Markov-chain transition.
As a simple solution to this problem, we propose to use parallel IMH (PIMH) chains, which enjoys reduced variance with exactly the same amount of computation.
We evaluate the performance of MSC with PIMH against other inclusive~\cite{DBLP:journals/corr/BornscheinB14, NEURIPS2020_b2070693} and exlusive~\cite{pmlr-v33-ranganath14, JMLR:v18:16-107} VI methods.
Lastly, some interesting connections with adaptive MCMC methods~\citep{10.1007/s11222-008-9110-y} are discussed.

\paragraph{Contribution Summary}
\begin{enumerate*}[label=\textbf{(\roman*)}]
\item We show that IMH type kernels (which include the CIS kernel originally used in MSC; \textbf{Section~\ref{section:cis_imh}}) automatically performs bias-variance tradeoff (\textbf{Section~\ref{section:bias_variance}}).
\item We show that increasing the computation budget of the CIS kernel can \textit{increase} its variance, and propose to use parallel IMH (PIMH) chains as an alternative (\textbf{Section~\ref{section:cis_bias}}).
\item We evaluate the performance of MSC with PIMH against other inclusive and exclusive VI methods (\textbf{Section~\ref{section:eval}}).
\end{enumerate*}

%Conveniently, parallel IMH (PIMH) enables the use of convergence metrics such as the the average rejection rate~\citep{neklyudov_metropolishastings_2019} or the \(\widehat{R}\) metric popularly used in MCMC~\cite{gelman_inference_2011, vehtari_ranknormalization_2020}.
%We discuss various connections with adaptive MH and show that the samples sued 

%% The key contributions of this paper are as follows:
%% \begin{itemize}
%%   \item We show that Independent Metropolis-Hastings kernels automatically trade-off bias and variance when used in Markovian score climibing.
%%   \item We propose parallel Independent Metropolis-Hastings for reducing the variance of the condition importance sampling kernel originally used by~\citet{NEURIPS2020_b2070693}.
%%   \item We show parallel Independent Metropolis-Hastings has not only technical but also practical benefits such as convergence diagnostics.
%% \end{itemize}


%%% Local Variables:
%%% TeX-master: "master"
%%% End:
