
\vspace{-0.15in}
\section{Discussions}\label{section:discussion}
\vspace{-0.1in}
This paper presented the parallel state estimator for inclusive KL divergence minimization.
Compared to previously proposed estimation schemes, our estimator enjoys substantially low variance.
We also showed that the parallel state estimator does not generally result in higher bias.
We demonstrated empirical evidence of our analysis on Bayesian inference problems.

In our results, minimizing the inclusive KL divergence showed to be competitive against exclusive KL divergence minimization.
This is against the conclusions of~\citet{dhaka_challenges_2021} that the inclusive KL does not work in high-dimensional problems (\citeauthor{dhaka_challenges_2021} consider few hundreds of dimensions).
Theoretically, dimensionality becomes a challenge when the posterior has complex nonlinear correlations.
A practical question would be how correlated our posteriors are in practice.
Also, many of the benefits of alternative divergences are shadowed by the challenges in our inference algorithms.
Therefore, our results motivate the development of better inference algorithms for alternative divergence measures, including the inclusive KL. 

%An interesting direction for future research would be to invstigate whether such result 
%This is against the conclusions of~\citet{dhaka_challenges_2021} that exclusive VI should outperform inclusive VI in high-dimensions. 

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
