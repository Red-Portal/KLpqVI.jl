
\section{Introduction}
Given an observed data \(\vx\) and a latent variable \(\vz\), Bayesian inference aims to analyze \(p\,(\vz\,|\,\vx)\) given an unnormalized joint density \(p\,(\vz,\,\vx)\) where the relationship is given by Bayes' rule such that \(p\,(\vz\,|\,\vx) = {p\,(\vz,\,\vx)}/{p\,(\vx)} \propto p\,(\vz,\,\vx)\).
Instead of working directly with the target distribution \(p\,(\vz\,|\,\vx)\), variational inference (VI,~\citealt{jordan_introduction_1999, blei_variational_2017, zhang_advances_2019}) searches for a variational approximation \(q_{\lambda}(\vz)\) that is similar to \(p\,(\vz\,|\,\vx)\) according to a discrepancy measure \(D\,(p,\, q_{\vlambda})\).

Naturally, choosing a good discrepancy measure, or objective function, is critical to the problem.
This fact had lead to a quest for suitable divergence measures~\citep{pmlr-v37-salimans15, NIPS2016_7750ca35, NIPS2017_35464c84, NEURIPS2018_1cd138d0, pmlr-v97-ruiz19a}.
So far, the exclusive KL divergence \(\DKL{q_{\lambda}}{p}\) (or reverse KL divergence) has been used ``exclusively'' among various discrepancy measures.
This is partly because the exclusive KL is defined as an average over \(q_{\lambda}(\vz)\), which can be estimated efficiently.
By contrast, the inclusive KL is defined as
%
%\vspace{-0.02in}
\begin{align}
  %% \DKL{p}{q_{\lambda}} = \int p\,(\vz\mid\vx) \log\big(\, p\,(\vz\mid\vx)/q_{\lambda}(\vz) \,\big)\,d\vz
  %% = \Esub{p(\vz\mid\vx)}{\log\big(\, p\,(\vz\mid\vx)/q_{\lambda}(\vz) \,\big) } \label{eq:klpq}
  \DKL{p}{q_{\lambda}}
  &= \int p\,(\vz\mid\vx) \log \frac{p\,(\vz\mid\vx)\,}{\,q_{\lambda}(\vz)} \,d\vz \\
  &= \Esub{p(\vz\mid\vx)}{\log \frac{p\,(\vz\mid\vx)\,}{\,q_{\lambda}(\vz)} } \label{eq:klpq}
\end{align}
%\vspace{-0.02in}
%
\noindent where the average is taken over \(p\,(\vz\,|\,\vx)\). 
Interestingly, this is a chicken-and-egg problem as our goal is to obtain \(p\,(\vz\,|\,\vx)\) in the first place.
Despite this challenge, minimizing~\eqref{eq:klpq} has drawn the attention of researchers because it is believed to result in favorable statistical properties~\citep{minka2005divergence, mackay_local_2001}.

For minimizing the inclusive KL divergence,~\citet{NEURIPS2020_b2070693} and~\citet{pmlr-v124-ou20a} have recently proposed  methods that perform stochastic gradient descent (SGD,~\citealt{robbins_stochastic_1951}) with the score function estimated using Markov-chain Monte Carlo (MCMC).
These MCMC score climbing schemes operate a Markov-chain in conjunction with the VI optimizer.
In addition, within the MCMC kernel, they both use Metropolis-Hastings proposals generated from the variational approximation \(\vz^* \sim q_{\vlambda}\left(\cdot\right)\).
The MCMC kernel itself benefits from VI, enjoying better proposals over time without the need for computationally expensive proposals as in Hamiltonian Monte Carlo~\citep{duane_hybrid_1987, neal_mcmc_2011, betancourt_conceptual_2017}.
Also, in terms of computational cost, score climbing is efficient compared to other divergences since we do not need to differentiate through the likelihood.

While the methods by~\citet{NEURIPS2020_b2070693} and~\citet{pmlr-v124-ou20a} are conceptually similar, they both use their MCMC kernels in different ways.
At each SGD iteration, for estimating the score function, \citeauthor{NEURIPS2020_b2070693} use a single sample generated from a relatively expansive MCMC kernel, while~\citeauthor{pmlr-v124-ou20a} average multiple samples generated from a cheaper MCMC kernel.
We call the former scheme the \textit{single state estimator} and the later the \textit{sequential state estimator}.
Given the two options, it is natural to ask, ``which is better? An estimator with multiple cheap samples? or one with a single expensive sample?''.

In this paper, we propose a third novel scheme, \textit{the parallel state estimator}.
The parallel state estimator operates \(N\) parallel Markov-chains parallel, where only a single state transition is performed on each chain.
The variance of this estimator linearly decreases with the computational budget \(N\), unlike the single and sequential state estimators.
While the substantial variance reduction comes at the cost of slightly higher bias, we observe that this is not a significant downside in practice.

We compare the bias and variance of the three different schemes and conduct experiments on general Bayesian inference problems.
Our results show that, given a similar computational budget \(N\), the parallel state estimator results in the best performance.
Also, score climbing VI with the parallel state estimator is competitive against evidence lower-bound  (ELBO) maximization.
Interestingly, this observation is against the conclusions of~\cite{dhaka_challenges_2021}
We further discuss this discrepancy in the Discussions~\cref{section:discussion}.


%% generated by the conditional importance sampling (CIS,~\citealt{NEURIPS2020_b2070693, andrieu_particle_2010, andrieu_uniform_2018}) kernel, which uses \(N\) samples from \(q_{\vlambda}(\cdot)\) per transition.
%% \citeauthor{pmlr-v124-ou20a}, on the other hand, use multiple sequentially generated states from independent Metropolis-Hastings (IMH,~\citealt[Algorithm 25]{hastings_monte_1970}), which only uses 1 sample from \(q_{\vlambda}(\cdot)\).
%% Therfore, \citeauthor{pmlr-v124-ou20a} use multiple cheaply generated samples, while \citeauthor{NEURIPS2020_b2070693} use a single expensively generated samples.

%Finally, some interesting connections with adaptive MCMC methods~\citep{10.1007/s11222-008-9110-y} are discussed.

\paragraph{Contribution Summary}
\begin{itemize}[noitemsep]
\item[\ding{228}] We propose the parallel state estimator for estimating the score function using MCMC (\textbf{\cref{section:overview}}).
\item[\ding{228}] We discuss the bias and variance of the MCMC score estimation schemes (\textbf{\cref{section:theory}}).
\item[\ding{228}] We experimentally compare the VI performance of the considered MCMC estimation schemes on general Bayesian inference benchmarks (\textbf{\cref{section:eval}}).
\item[\ding{228}] We also show that inclusive KL minimization with score climbing is competitive against exclusive KL divergence minimization (\textbf{\cref{section:eval}}).
\end{itemize}
%\item We discuss connections with adaptive IMH methods (\textbf{\cref{section:related}}).
%\end{enumerate*}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
