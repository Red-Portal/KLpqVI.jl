
\section{Background}
\vspace{-0.05in}
\subsection{Inclusive Variational Inference Until Now}\label{section:ivi_previous}
\vspace{-0.05in}
\paragraph{Variational Inference}
The goal of VI is to find the optimal variational parameter \(\vlambda\) identifying \(q_{\vlambda} \in \mathcal{Q}\) that minimizes some discrepancy measure \(D\left(\pi, q_{\vlambda}\right)\).
A typical way to perform VI is to use stochastic gradient descent (SGD,~\citealt{robbins_stochastic_1951}), provided that unbiased gradient estimates of the optimization target \(\vg(\vlambda)\) are available.
In this case, SGD is performed by repeating the update
\vspace{-0.05in}
\begin{align}
  \vlambda_{t} = \vlambda_{t-1} + \gamma_{t} \, \vg\left(\vlambda_{t-1}\right)
\end{align}
where \(\gamma_1, \ldots, \gamma_T\) is a step-size schedule following the conditions of~\citet{robbins_stochastic_1951, bottou_online_1999}.
In the case of inclusive KL divergence minimization, \(\vg\) estimates
%
\vspace{-0.05in}
{
\begin{align}
  \nabla_{\vlambda} \DKL{\pi}{q_{\lambda}}
  = -\Esub{\pi}{ \vs\,(\vlambda; \vz) } 
  \approx \vg\left(\vlambda\right)
\end{align}
}%
%\vspace{-0.05in}
%
where \(s\,(\vlambda;\vz) = \nabla_{\vlambda} \log q_{\vlambda}(\vz)\) is known as the \textit{score function}.
%% Evidently, estimating \(\nabla_{\vlambda} \DKL{\pi}{q_{\lambda}}\) requires integrating the score function over \(\pi\), which is prohibitive.
%Different inclusive variational inference methods form a different estimator \(\vg\).

\vspace{-0.05in}
\paragraph{Importance Sampling}
When it is easy to sample from the variational approximation \(q_{\lambda}(\vz)\), one can use importance sampling (IS, \citealt{robert_monte_2004}) for estimating the score following
\vspace{-0.05in}
\begin{align}
  \Esub{\pi}{ \vs\,(\vlambda;\vz) } 
  &\propto Z\,\Esub{\pi}{ \vs\,(\vz; \vlambda) } \label{eq:is_est} \\
  %&= \Esub{q_{\vlambda}}{ w\,(\vz) \, \vs\,(\vz; \vlambda) } \\
  &\approx \frac{1}{N} \sum^{N}_{i=1} w\,(\vz^{(i)}) \, \vs\,(\vlambda; \vz^{(i)}) \\
  &\triangleq \vg_{IS}(\vlambda)
\end{align}
%\vspace{-0.05in}
where \(w\,(\vz) = p\,(\vz,\vx) / q_{\vlambda}(\vz)\) is known as the \textit{importance weight}, \(Z\) is the marginal \(p\left(\vx\right) = \int p\left(\vx, \vz\right) \, d\vz\), and \(\vz^{(1)},\, \ldots,\, \vz^{(N)}\) are \(N\) independent samples from \(q_{\vlambda}(\vz)\).
This scheme is equivalent to adaptive IS methods~\citep{bugallo_adaptive_2017} since the IS proposal \(q_{\vlambda}(\vz)\) is iteratively optimized based on the current samples.
Although IS is unbiased, it is numerically unstable because of \(Z\) in \cref{eq:is_est}.
A more stable alternative is to use the \textit{normalized weight} \(\widetilde{w}^{(i)} = \nicefrac{w\,(\vz^{(i)})}{\sum_{i=1}^N w\,(\vz^{(i)}) }\), which results in the self-normalized IS (SNIS) approximation.
Unfortunately, SNIS still fails to converge even on moderate dimensional objectives and unlike IS, it is no longer unbiased~\citep{robert_monte_2004}.

%
  %% \begin{minipage}[l]{0.45\linewidth}
  %%   \small
  %%   \centering
  %%   \begin{algorithm2e}[H]
  %%     \DontPrintSemicolon
  %%     \SetAlgoLined
  %%     \KwIn{MCMC kernel \(K(\vz,\cdot)\),
  %%       initial sample \(\vz_0\),
  %%       initial parameter \(\vlambda_0\),
  %%       number of iterations \(T\),
  %%       stepsize schedule \(\gamma_t\)}
  %%     \For{\textcolor{black}{\(t = 1, 2, \ldots, T\)}}{
  %%       \textit{Sample} \hspace{0.035in} \( \vz_{t} \sim K(\vz_{t-1}, \cdot) \)\;
  %%       \textit{Estimate} \( s(\vz; \vlambda) = \nabla_{\vlambda} \log q_{\vlambda}(\vz_t) \)\;
  %%       \textit{Update} \hspace{0.03in} \( \vlambda_{t} = \vlambda_{t-1} + \gamma_t\, s\,(\vz_t;\vlambda_{t-1}) \)\;
  %%     }
  %%     \caption{Markovian Score Climbing}\label{alg:msc}
  %%   \end{algorithm2e}
  %%   \vspace{-0.1in}
  %% \end{minipage}
  %% \qquad
  %% \begin{minipage}[r]{0.5\linewidth}
  %%   \vspace{-0.1in}
  %%   \begin{figure}[H]
  %%     \centering
  %%     \input{figures/trace.tex}
  %%     \caption{KL divergence and trace of \(\vz_t\) of MSC with a CIS kernel.
  %%       \(\vz_t\) barely moves until \(t=250\) around which \(\DKL{p}{q_{\vlambda}}\) starts to converge.}\label{fig:motivating}
  %%   \end{figure}
  %%   \vspace{-0.1in}
  %% \end{minipage}
%
%%% Local Variables:
%%% TeX-master: "master"
%%% End:
