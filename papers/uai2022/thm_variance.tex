
\begin{theoremEnd}{theorem}\label{thm:var}
  Assuming that the Markov-chains have achieved stationarity and \(N \geq 2\), the variance of the single state estimator (\(\vg_{\text{single}}\)), sequential state estimator with \(N\) states of the IMH kernel (\(\vg_{\text{seq.-IMH}}\)), and parallel state estimator (\(\vg_{\text{par.}}\)) with \(N\) chains are given as 
  \vspace{-0.08in}
  {\small
  \begin{align*}
    \V{\vg_{\text{single}}}   = \sigma^2,\;\;
    \V{\vg_{\text{seq.-IMH}}}  = \frac{\sigma^2}{N} + C_{\text{gap}},\;\;
    \V{\vg_{\text{par.}}}    = \frac{\sigma^2}{N}
  \end{align*}
  }%
  where \(\sigma^2 = \Vsub{\pi}{\vs\left(\vlambda; \vz\right)} \) and \(C_{\text{gap}} \geq 0\).
  The variance gap \(C_{\text{gap}}\) is bounded below as
  \vspace{-0.05in}
  {\small
  \begin{align*}
    C_{\text{gap}}
    \geq
    \frac{2}{N} \, \Big[\,
    &2\,\sigma^2
    +
    \Esub{q}{\Delta^2} \, \big\{\,
    \exp\,(\,
    \DKL{q}{\pi}
    \,)\,
    -
    3
    \,\big\}
    \\
    &\; +
    \text{Cov}_{q}\left(\Delta^2\left(\vz\right),\, 1/ w\left(\vz\right) \right)
    \,\Big]
    +
    \mathcal{O}\left(N^{-2}\right)
  \end{align*}
  }%
  where \(q\) is the proposal distribution, \(w\left(\vz\right)=\pi\left(\vz\right)/q\left(\vz\right)\), \(C\) is a positive constant, \(\Delta\left(\vz\right) = \vs\left(\vlambda;\vz\right) - \Esub{\pi}{\vs\left(\vlambda;\vz\right)}\),
  and, as \(N \rightarrow \infty\), the bound converges to
  \vspace{-0.05in}
  {\small
  \begin{align*}
    C_{\text{gap}}
    \geq
    \frac{2}{N} \, \Big[\,
    \sigma^2 \, \big\{\,
    \exp\,(\,
    \DKL{\pi}{q}
    \,)\,
    -
    1
    \,\big\}
     +
    \text{Cov}_{\pi}\left(\Delta^2\left(\vz\right),\, w\left(\vz\right) \right)
    \,\Big].
  \end{align*}
  }
\end{theoremEnd}
%
\begin{proofEnd}

  \textit{\textbf{Variance of the Single State Estimator}}\quad
  For the single state estimator,  
  \begin{alignat}{2}
    \V{\vg_{\text{single}}\left(\vlambda\right)} 
    &= \E{ \Vsub{K_{\vlambda_{t-1}}\left(\vz_{t-1}, \vz\right)}{ \vs\left(\vz; \vlambda\right) \mid \vz_{t-1} }}  \nonumber \\
    &\quad + \V{ \Esub{K_{\vlambda_{t-1}}\left(\vz_{t-1}, \vz\right)}{ \vs\left(\vz; \vlambda\right) \mid \vz_{t-1} } },
    &&\qquad\text{(Total Variance)}
    \\
    \intertext{and assuming stationarity such that \(\vz_{t-1} \sim \pi\left(\vz\right)\),}
    &=
    \Esub{\pi}{ \Vsub{K_{\vlambda_{t-1}}\left(\vz_{t-1}, \vz\right)}{ \vs\left(\vz; \vlambda\right) \mid \vz_{t-1} }} \nonumber \\
    &\quad + \Vsub{\pi}{ \Esub{K_{\vlambda_{t-1}}\left(\vz_{t-1}, \vz\right)}{ \vs\left(\vz; \vlambda\right) \mid \vz_{t-1} } }
    &&\qquad\text{(Total Variance)}
    \\
    &= \Vsub{\pi}{\vs\left(\vz; \vlambda\right)}  \\
    &= \sigma^2.\label{eq:var_single}
  \end{alignat}

  \textit{\textbf{Variance of the Parallel State Estimator}}\quad
  Meanwhile, the parallel state estimator is an average of idenpendent and identically distributed (\textit{i.i.d.}) since the parallel chains are independent.
  Therefore, from \cref{eq:var_single}, its variance is 
  \begin{align}
    \V{\vg_{\text{par.}}} = \frac{\sigma^2}{N}.
  \end{align}

  \textit{\textbf{Variance of the Sequential State Estimator}}\quad
  Now, for the single state estimator, we first derive an MCMC kernel independent expression.
  First, remember that the estimator is defined as
  \begin{align}
    \vg_{\text{seq.}}\left(\vlambda\right) = \frac{1}{N} \sum_{i=1}^N \vs\left(\vlambda; \vz_{T+i}\right),
  \end{align}
where \( \vz_{T+i} \sim K_{\vlambda_{t-1}}^i\left( \vz_{T}, \cdot \right) \) and \(\vz_T\) is the last Markov-chain state at the previous SGD iteration \(t-1\).
Then, the variance is given as
\begin{alignat}{2}
  \V{\vg_{\text{seq.}}}
  &= \V{ \Esub{K\left(\vz_{T}, \vz\right)}{ \frac{1}{N} \sum_{i=1}^N \vs\left(\vlambda; \vz_{T+i}\right)  \,\Bigg\vert\, \vz_T } }
  + \E{ \Vsub{K\left(\vz_{T}, \vz\right)}{ \frac{1}{N} \sum_{i=1}^N \vs\left(\vlambda; \vz_{T+i}\right) \,\Bigg\vert\,  \vz_T } }
  &&\quad\text{(Total Variance)}
  \nonumber
  \\
  &= \frac{1}{N^2} \sum_{i=1}^N \Vsub{K\left(\vz_{T}, \vz\right)}{ \E{  \vs\left(\vlambda; \vz_{T+i}\right) \mid \vz_T } }
  \nonumber
  \\
  &\qquad + \E{ \frac{1}{N^2}  \sum_{i=1}^N  \Vsub{K\left(\vz_{T}, \vz\right)}{ \vs\left(\vlambda; \vz_{T+i}\right)  \mid  \vz_T } 
+ \frac{2}{N^2}  \sum_{i < j}^N \Cov{ s\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
  }
  \nonumber
  \\
  &= \frac{1}{N^2} \sum_{i=1}^N \Vsub{K\left(\vz_{T}, \vz\right)}{ \E{  \vs\left(\vlambda; \vz_{T+i}\right) \mid \vz_T } }
  \nonumber
  \\
  &\qquad+ \frac{1}{N^2} \sum_{i=1}^N   \Esub{K\left(\vz_{T}, \vz\right)}{ \V{ \vs\left(\vlambda; \vz_{T+i}\right)  \mid  \vz_T } }
  \nonumber
  \\
   &\qquad+ \frac{2}{N^2}  \sum_{i < j} \E{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
  },
  \nonumber
\intertext{\text{where, by assuming stationarity such that \(\vz_{T} \sim \pi\left(\vz\right)\),}}
  &= \frac{1}{N^2} \sum_{i=1}^N \Vsub{\pi}{ \E{  \vs\left(\vlambda; \vz_{T+i}\right) \mid \vz_T } } 
\qquad+ \frac{1}{N^2} \sum_{i=1}^N   \Esub{\pi}{ \V{ \vs\left(\vlambda; \vz_{T+i}\right)  \mid  \vz_T } }
\nonumber
\\
 &\qquad+ \frac{2}{N^2}  \sum_{i < j} \Esub{\pi}{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
}
\nonumber
\\
  &= \frac{1}{N^2} \sum_{i=1}^N \Vsub{\pi}{ \vs\left(\vlambda; \vz\right) } 
 + \frac{2}{N^2}  \sum_{i < j} \Esub{\pi}{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
 }
\nonumber
 &&\quad\text{(Total Variance)}
 \\
  &= \frac{1}{N} \Vsub{\pi}{ \vs\left(\vlambda; \vz\right) } 
 + \frac{2}{N^2}  \sum_{i < j} \Esub{\pi}{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
 }
 \nonumber
 \\
  &= \frac{\sigma^2}{N}
 + \frac{2}{N^2}  \sum_{i < j}^N \Esub{\pi}{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
 }
 \nonumber
 \\
  &= \frac{\sigma^2}{N}
 + \frac{2}{N} \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right) \gamma_k \label{eq:seq_cov}
\end{alignat}
where \(\gamma_k\) is the \(k\)-lag autocovariance.

 Specifically for the IMH kernel,~\citet{tan_monte_2006} has analyzed \(\gamma_k\) based on the results of~\citet{Smith96exacttransition}.
We extend his analysis to obtain our desired conclusion.
Let us denote \(\gamma_k\) as the covariance between a state \(\vz\) and its \(k\)th lagged counterpart \(\vz_k\), and \(\Delta \left(\vz\right) = \vs\left(\vz\right) - \Esub{\pi}{\vs}\).
Then,
\begin{align*}
  \gamma_k
  &= \int \Delta\left(\vz\right) \left(
  \int K^k\left(\vz, \vz_k \right) \, \Delta\left(\vz_k\right) \, d\vz_k
  \right) \, \pi\left(\vz\right) \, d\vz
  \\
  &=
  \int \Delta\left(\vz\right)
  \left(
  \int \Delta\left(\vz_k\right) \,
  T_k\left(w\left(\vz\right) \vee w\left(\vz_k\right) \right) \, \pi\left(\vz_k\right) \, d\vz_k
  + \Delta\left(\vz\right) \, \lambda^k\left(w\left(\vz\right)\right)
  \right) \, \pi\left(\vz\right) \, d\vz
  \\
  &=
  \int \int
  \Delta\left(\vz\right) \,
  \Delta\left(\vz_k\right) \,
  T_k\left(w\left(\vz\right) \vee w\left(\vz_k\right) \right) \, \pi\left(\vz_k\right) \, \pi\left(\vz\right) \,  d\vz_k \, d\vz
  +
  \int
  \Delta^2\left(\vz\right) \, \lambda^k\left(w\left(\vz\right)\right) \, \pi\left(\vz\right) \, d\vz.
\end{align*}

For the first term,~\citet[Theorem 3]{tan_monte_2006} have shown that
\begin{align*}
  \int \int
  \Delta\left(\vz\right) \,
  \Delta\left(\vz_k\right) \,
  T_k\left(w\left(\vz\right) \vee w\left(\vz_k\right) \right) \, \pi\left(\vz_k\right) \, \pi\left(\vz\right)  \, d\vz_k  \, d\vz
  \geq
  0.
\end{align*}
And for the second term,
\begin{align}
  \frac{2}{N} \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right) \gamma_k
  &\geq
  \frac{2}{N} \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right)
  \int
  \Delta^2\left(\vz\right) \, \lambda^k\left(w\left(\vz\right)\right) \, \pi\left(\vz\right) \, d\vz
  \nonumber
  \\
  &=
  \frac{2}{N}
  \int
  \Delta^2\left(\vz\right) \,
   \sum_{k=1}^{N-1} 
  \left(1 - \frac{k}{N}\right) \, \lambda^k\left(w\left(\vz\right)\right)
  \, \pi\left(\vz\right) \, d\vz \label{eq:autocov_bound}  \\
  &\geq 0,
  \nonumber
\end{align}
which proves \(\V{\vg_{\text{seq.-IMH}}} - \V{\vg_{\text{par.}}} = C_{\text{gap}} \geq 0\).

Analyzing the variance gap \(C_{\text{gap}}\) is a little more involved.
Usually, traditional MCMC analysis invokes Ces\`aro's summability theorem for the sum in~\cref{eq:autocov_bound} by assuming \(N \rightarrow \infty\)~\citep{kung_discussions_1994}.
In our case, we avoid this path in order to generalize our result to the small \(N\) regime.
For clarity, we denote \(r = \lambda\left(w\left(\vz\right)\right)\).
Then,
\begin{align}
 \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right) \, r^k
 &=
 \sum_{k=1}^{N-1} \, r^k
 -
 \frac{1}{N}\,\sum_{k=1}^{N-1} \,k\, r^k
  \nonumber
 \\
 &=
 \frac{r \, \left( 1 - r^{N-1} \right) }{1 - r} 
 -
 \frac{r}{N}
 \,
 \frac{1 - N\,r^{N-1} + \left(N-1\right)\,r^{N} }{{\left(1 - r\right)}^2} 
  \nonumber
 \\
 &=
 \frac{
   r\,\left(r^N - N\,r + N - 1\right)
 }{
   N\,{\left(1-r\right)}^2
 }
  \nonumber
 \\
 &=
 \frac{
   r\,\left( N \left(1-r\right) - \left(1-r^N\right) \right)
 }{
   N\,{\left(1-r\right)}^2
 }
 \nonumber
 \\
 &=
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{\left(1-r^N\right)}{{\left(1-r\right)}^2},\label{eq:r_exact}
\end{align}
which is monotonically increasing with respect to \(r\).

There are several ways to analyze the behavior of this function.
For example, the upper bound 
\begin{align*}
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{\left(1-r^N\right)}{{\left(1-r\right)}^2}
 &\leq
 \frac{r}{1-r}
\end{align*}
becomes a good approximation for large \(N\).
For small \(N\), it is still accurate for small values of \(r\).
In the limit \(N \rightarrow \infty\), the upper bound becomes exact where we retrieve the result of~\citet[Theorem 3]{tan_monte_2006}.

\textit{\textbf{Lower Bound of \(C_{\text{gap}}\)}}\quad
On the other hand, it is also possible to derive a lower bound using the formula of geometric sums as
\begin{align*}
  -\frac{\left(1-r^N\right)}{{\left(1-r\right)}}
  &=
  -\frac{\left(1-r\right)\,\left(r^{N-1} + r^{N-2} + \ldots + r + 1\right)}{{\left(1-r\right)}}
  \\
  &=
  -\left(r^{N-1} + r^{N-2} + \ldots + r^{N-2} + r + 1\right)
  \\
  &\geq
  -\left(r^2 + r^2 + \ldots + r^2 +  r + 1\right)
  \\
  &=
  -\left((N-2)\,r^2 + r + 1\right)
\end{align*}
where we have used the fact that \(0 \leq r \leq 1\) and \(N \geq 2\).
By applying this to \cref{eq:r_exact},
\begin{align}
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{\left(1-r^N\right)}{{\left(1-r\right)}^2}
 &\geq 
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{(N-2)\,r^2 + r + 1}{\left(1-r\right)}
 \nonumber
 \\
 &=
 \left(1 - \frac{1}{N}\right) \frac{r}{1-r}
 -
 \frac{1}{N} \frac{r^2}{1-r}
 -
 \frac{N-2}{N} \frac{r^3}{1-r}.\label{eq:r_lowerbound}
\end{align}

\begin{figure}[H]
  \centering
  \subfloat[\(N=16\)]{
    \begin{tikzpicture}
      \begin{semilogyaxis}[
          tuftelike,
          axis x line shift=10pt,
          axis y line shift=10pt,
          xlabel = {\(r\)},
          xmin = 0,
          xmax = 1.0,
          ymin = 0.01,
          ymax = 100,
          width  = 6.0cm,
          height = 6.0cm,
          legend style = { 
            draw           = none,
            at={(0.02,0.98)},
            anchor=north west,
            legend cell align={left},
          }
        ]
        \addplot[
          domain = 0.0001:0.999,
          thick,
          samples=256
        ] {
          x/(1.0-x) - x/16.0*(1.0-x^(16))/(1-x)^(2)
        };
        \addlegendentry{Exact}

        \addplot[
          red,
          domain = 0.0001:0.9999,
          thick,
          samples=128
        ] {
          (1 - 1/16)*(x/(1-x))
          -
          (1/16)*(x^2/(1-x))
          -
          (14/16)*(x^3/(1-x))
        };
        \addlegendentry{Lower bound}
        \addplot[
          blue,
          domain = 0.0001:0.9999,
          thick,
          samples=128
        ] {
          x/(1.0-x)
        };
        \addlegendentry{Upper bound}
      \end{semilogyaxis}
    \end{tikzpicture}
  }
  \subfloat[\(N=64\)]{
    \begin{tikzpicture}
      \begin{semilogyaxis}[
          tuftelike,
          axis x line shift=10pt,
          axis y line shift=10pt,
          xlabel = {\(r\)},
          xmin = 0,
          xmax = 1.0,
          ymin = 0.01,
          ymax = 100,
          width  = 6.0cm,
          height = 6.0cm,
          legend style = { 
            draw           = none,
            at={(0.02,0.98)},
            anchor=north west,
            legend cell align={left},
          }
        ]
        \addplot[
          domain = 0.0001:0.999,
          thick,
          samples=256
        ] {
          x/(1.0-x) - x/64.0*(1.0-x^(64))/(1-x)^(2)
        };
        \addlegendentry{Exact}

        \addplot[
          red,
          domain = 0.0001:0.9999,
          thick,
          samples=128
        ] {
          (1 - 1/64)*(x/(1-x))
          -
          (1/64)*(x^2/(1-x))
          -
          (62/64)*(x^3/(1-x))
        };
        \addlegendentry{Lower bound} %\cref{eq:r_lowerbound}}
        \addplot[
          blue,
          domain = 0.0001:0.9999,
          thick,
          samples=128
        ] {
          x/(1.0-x)
        };
        \addlegendentry{Upper bound}
      \end{semilogyaxis}
    \end{tikzpicture}
  }
  \caption{Comparison of \cref{eq:r_exact} against its lower and upper bounds.}\label{fig:r_comp}.
\end{figure}

The quality of the lower and upper bounds are visualized in \cref{fig:r_comp}.
We can see that the lower bound is quite optimistic in general, but is more accurate for the small \(N\) regime.

Now, let us develop our lower bound.
Recall that \(r = \lambda\left(w\left(\vz\right)\right)\).
\citet[Proof of Theorem 3]{tan_monte_2006} have shown that \(1 - \frac{1}{u} \leq \lambda\left(u\right) \leq 1 - \frac{1}{w^*}\).
Since the lower bound in \cref{eq:r_lowerbound} is monotonically increasing, applying the bound \(r \geq 1 - \frac{1}{w\left(\vz\right)}\) results in
\begin{align}
  &\left(1 - \frac{1}{N}\right) \frac{r}{1-r}
  -
  \frac{1}{N} \frac{r^2}{1-r}
  -
  \left(1 - \frac{2}{N}\right) \frac{r^3}{1-r}
  \nonumber \\
  &\geq
  \left(1 - \frac{1}{N}\right) \, w\left(\vz\right) \, \left(1 - \frac{1}{w\left(\vz\right)}\right)
  -
  \frac{1}{N} \, w\left(\vz\right) \, {\left(1 - \frac{1}{w\left(\vz\right)}\right)}^2
  -
  \left(1 - \frac{2}{N}\right) \, w\left(\vz\right) \, {\left(1 - \frac{1}{w\left(\vz\right)}\right)}^3.\label{eq:w_init}
\end{align}

For clarity, we temporarily set \(\alpha = 1 - \frac{1}{N}\), \(\beta = \frac{1}{N}\), \(\gamma = 1 - \frac{2}{N}\).
Then, \cref{eq:w_init} becomes
\begin{align*}
  &\alpha \, w\left(\vz\right) \, \left(1 - \frac{1}{w\left(\vz\right)}\right)
  -
  \beta \, w\left(\vz\right) \, {\left(1 - \frac{1}{w\left(\vz\right)}\right)}^2
  -
  \gamma \, w\left(\vz\right) \, {\left(1 - \frac{1}{w\left(\vz\right)}\right)}^3 \nonumber
  \\
  &=
  \alpha \, \left(w\left(\vz\right) - 1\right)
  -
  \beta \, {\left(w\left(\vz\right) - 2 + \frac{1}{w\left(\vz\right)} \right)}
  -
  \gamma \, {\left(w\left(\vz\right) - 3 + \frac{3}{w\left(\vz\right)} - \frac{1}{w^2\left(\vz\right)}  \right)}
  \\
  &=
  \left(\alpha - \beta - \gamma\right) \, w\left(\vz\right)
  +
  \left(-\alpha + 2\,\beta + 3\,\gamma\right)
  +
  \left(-\beta - 3 \gamma\right) \frac{1}{w\left(\vz\right)}
  +
  \gamma \frac{1}{w^2\left(\vz\right)}
  \\
  &=
  \left(2 - \frac{3}{N}\right)
  +
  \left(-3 + \frac{5}{N}\right) \frac{1}{w\left(\vz\right)}
  +
  \left(1 - \frac{2}{N}\right)  \frac{1}{w^2\left(\vz\right)}.
\end{align*}

Applying this to~\cref{eq:autocov_bound},
\begin{align*}
  &\frac{2}{N}
  \int
  \Delta^2\left(\vz\right) \,
   \sum_{k=1}^{N-1} 
  \left(1 - \frac{k}{N}\right) \, \lambda^k\left(w\left(\vz\right)\right)
  \, \pi\left(d\vz\right)
  \\
  &\geq
  \frac{2}{N}
  \int
  \Delta^2\left(\vz\right) \,
  \left\{
  \left(2 - \frac{3}{N}\right)
  +
  \left(-3 + \frac{5}{N}\right) \frac{1}{w\left(\vz\right)}
  +
  \left(1 - \frac{2}{N}\right)  \frac{1}{w^2\left(\vz\right)}
  \right\}
  \, \pi\left(d\vz\right)
  \\
  &=
  \frac{2}{N}\,
  \left(2 - \frac{3}{N}\right) \, \sigma^2
  +
  \frac{2}{N}\,
  \left(-3 + \frac{5}{N}\right) 
  \int
  \Delta^2\left(\vz\right) \,
  \frac{1}{w\left(\vz\right)}
  \, \pi\left(d\vz\right)
  +
  \frac{2}{N}\,
  \left(1 - \frac{2}{N}\right)
  \int
  \Delta^2\left(\vz\right) \,
  \frac{1}{w^2\left(\vz\right)}
  \, \pi\left(d\vz\right)
  \\
  &=
  \frac{2}{N}\,
  \left(2 - \frac{3}{N}\right) \, \sigma^2
  +
  \frac{2}{N}\,
  \left(-3 + \frac{5}{N}\right) 
  \int
  \Delta^2\left(\vz\right) \,
  \frac{q\left(\vz\right)}{\pi\left(\vz\right)}
  \, \pi\left(d\vz\right)
  +
  \frac{2}{N}\,
  \left(1 - \frac{2}{N}\right)
  \int
  \Delta^2\left(\vz\right) \,
  \frac{q^2\left(\vz\right)}{\pi^2\left(\vz\right)}
  \, \pi\left(d\vz\right)
  \\
  &=
  \frac{2}{N}\,
  \left(2 - \frac{3}{N}\right) \, \sigma^2
  +
  \frac{2}{N}\,
  \left(-3 + \frac{5}{N}\right) 
  \int
  \Delta^2\left(\vz\right) \,
  q\left(\vz\right)\,d\vz
  +
  \frac{2}{N}\,
  \left(1 - \frac{2}{N}\right)
  \int
  \Delta^2\left(\vz\right) \,
  \frac{q^2\left(\vz\right)}{\pi\left(\vz\right)}\,
  d\vz.
\end{align*}

For the last term,
\begin{align*}
  \int
  \Delta^2\left(\vz\right) \,
  \frac{q^2\left(\vz\right)}{\pi\left(\vz\right)}\,
  d\vz
  &=
  \Esub{q}{
    \Delta^2\left(\vz\right) 
    \, \frac{q\left(\vz\right)}{\pi\left(\vz\right)}
  }
  \\
  &=
  \Esub{q}{
    \Delta^2\left(\vz\right) 
  }
  \,
  \Esub{q}{
    \frac{q\left(\vz\right)}{\pi\left(\vz\right)}
  }
  +
  \text{Cov}_{q}\left(\Delta^2\left(\vz\right),\, w^{-1}\left(\vz\right)\right)
  \\
  &\geq
  \Esub{q}{
    \Delta^2\left(\vz\right) 
  }
  \,
  \exp\left(
    \DKL{q}{\pi}
  \right)
  +
  \text{Cov}_{q}\left(\Delta^2\left(\vz\right),\, w^{-1}\left(\vz\right)\right).
\end{align*}

After some algebra, we obtain our result
\begin{align*}
  &\frac{2}{N} \,
  \left(2 - \frac{3}{N}\right)
  \, \sigma^2
  +
  \frac{2}{N} \,
  \left(-3 + \frac{5}{N}\right) 
  \int
  \Delta^2\left(\vz\right) \,
  q\left(\vz\right)\,d\vz
  +
  \frac{2}{N} \,
  \left(1 - \frac{2}{N}\right)
  \int
  \Delta^2\left(\vz\right) \,
  \frac{q^2\left(\vz\right)}{\pi\left(\vz\right)}\,
  d\vz
  \\
  &\geq
    \frac{2}{N} \,
    \left(2 - \frac{3}{N}\right)
  \, \sigma^2
  +
  \frac{2}{N} \,
  \left(-3 + \frac{5}{N}\right) 
  \Esub{q}{
    \Delta^2 
  }\nonumber
  \\
  &\quad+
  \frac{2}{N} \,
  \left(1 - \frac{2}{N}\right)
  \left\{\,
  \Esub{q}{
    \Delta^2
  }
  \,
  \exp\left(
    \DKL{q}{\pi}
  \right)
  +
  \text{Cov}_{q}\left(\Delta^2\left(\vz\right),\, w^{-1}\left(\vz\right)\right)
  \,\right\}
  \\
  &\approx
  \frac{4}{N} \, \sigma^2
  -
  \frac{6}{N}
  \Esub{q}{
    \Delta^2 
  }
  +
  \frac{2}{N} \,
  \left\{\,
  \Esub{q}{
    \Delta^2
  }
  \,
  \exp\left(
    \DKL{q}{\pi}
  \right)
  +
  \text{Cov}_{q}\left(\Delta^2\left(\vz\right),\, w^{-1}\left(\vz\right)\right)
  \,\right\}
  +
  \mathcal{O}\left(\frac{1}{N^2}\right)
  \\
  &=
  \frac{4}{N} \, \sigma^2
  +
  \frac{2}{N} \,
  \Esub{q}{
    \Delta^2 
  }
  \,
  \left(\,
  \exp\left(
    \DKL{q}{\pi}
  \right)
  -
  3
  \,\right)
  +
  \frac{2}{N} \,
  \text{Cov}_{q}\left(\Delta^2\left(\vz\right),\, w^{-1}\left(\vz\right)\right)
  +
  \mathcal{O}\left(\frac{1}{N^2}\right).
\end{align*}

\textit{\textbf{Asymptotic Approximation of \(C_{\text{gap}}\)}}\quad
Before concluding, we discuss the case where \(N\) is large such that
\begin{align*}
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{\left(1-r^N\right)}{{\left(1-r\right)}^2}
 \xrightarrow{N \rightarrow \infty}
 \frac{r}{1-r}.
\end{align*}
Using the lower bound \(r \geq 1 - \frac{1}{w\left(\vz\right)}\),
\begin{align*}
  \frac{r}{1-r} \geq w\left(\vz\right) - 1.
\end{align*}
Therefore,
\begin{align*}
  \int
  \Delta^2\left(\vz\right) \,
  \sum_{k=1}^{N-1} 
  \left(1 - \frac{k}{N}\right) \, \lambda^k\left(w\left(\vz\right)\right)
  \, \pi\left(d\vz\right)
  &\rightarrow
  \int
  \Delta^2\left(\vz\right) \,
  \left(
  w\left(\vz\right)
  -
  1
  \right)
  \, \pi\left(d\vz\right)
  \\
  &=
  \int
  \Delta^2\left(\vz\right) \,
  w\left(\vz\right)
  \, \pi\left(d\vz\right)
  - \sigma^2
  \\
  &=
  \int
  \Delta^2\left(\vz\right) \,
  \frac{\pi\left(\vz\right)}{q\left(\vz\right)}
  \, \pi\left(d\vz\right)
  - \sigma^2
  \\
  &=
  \Esub{\pi}{\Delta^2\left(\vz\right)} \, \Esub{\pi}{\frac{\pi\left(\vz\right)}{q\left(\vz\right)}}
  +
  \text{Cov}_{\pi}\left(\Delta^2\left(\vz\right), w\left(\vz\right) \right)
  - \sigma^2
  \\
  &=
  \sigma^2 \, \left(\int \pi\left(\vz\right)\,\frac{\pi\left(\vz\right)}{q\left(\vz\right)} d\vz  - 1\right)
  +
  \text{Cov}_{\pi}\left(\Delta^2\left(\vz\right), w\left(\vz\right) \right)
  \\
  &\geq
  \sigma^2 \, \left( \exp\left(\DKL{\pi}{q}\right) - 1\right)
  +
  \text{Cov}_{\pi}\left(\Delta^2\left(\vz\right), w\left(\vz\right) \right).
\end{align*}
\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
