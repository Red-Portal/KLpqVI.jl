
\prAtEndRestateii*
\prAtEndRestateiii*
\label{proofsection:prAtEndiii}\begin{proof}[Proof of \autoref{thm:prAtEndiii}]\phantomsection\label{proof:prAtEndiii}\par \textit {\textbf {Variance of the Single State Estimator}}\quad For the single state estimator, \begin {align} \V {\vg _{\text {single}}\left (\vlambda \right )} &= \E { \Vsub {K_{\vlambda _{t-1}}\left (\vz _{t-1}, \vz \right )}{ \vs \left (\vz ; \vlambda \right ) \mid \vz _{t-1} }} \nonumber \\ &\quad + \V { \Esub {K_{\vlambda _{t-1}}\left (\vz _{t-1}, \vz \right )}{ \vs \left (\vz ; \vlambda \right ) \mid \vz _{t-1} } }, \\ \intertext {and assuming stationarity such that \(\vz _{t-1} \sim \pi \left (\vz \right )\),} &= \Esub {\pi }{ \Vsub {K_{\vlambda _{t-1}}\left (\vz _{t-1}, \vz \right )}{ \vs \left (\vz ; \vlambda \right ) \mid \vz _{t-1} }} \nonumber \\ &\quad + \Vsub {\pi }{ \Esub {K_{\vlambda _{t-1}}\left (\vz _{t-1}, \vz \right )}{ \vs \left (\vz ; \vlambda \right ) \mid \vz _{t-1} } } \\ &= \Vsub {\pi }{\vs \left (\vz ; \vlambda \right )} \\ &= \sigma ^2.\label {eq:var_single} \end {align} \par \textit {\textbf {Variance of the Parallel State Estimator}}\quad Meanwhile, the parallel state estimator is an average of idenpendent and identically distributed (\textit {i.i.d.}) since the parallel chains are independent. Therefore, from \cref {eq:var_single}, its variance is \begin {align} \V {\vg _{\text {par.}}} = \frac {\sigma ^2}{N}. \end {align} \par \textit {\textbf {Variance of the Sequential State Estimator}}\quad Now, for the single state estimator, we first derive an MCMC kernel independent expression. First, remember that the estimator is defined as \begin {align} \vg _{\text {seq.}}\left (\vlambda \right ) = \frac {1}{N} \sum _{i=1}^N \vs \left (\vlambda ; \vz _{T+i}\right ), \end {align} where \( \vz _{T+i} \sim K_{\vlambda _{t-1}}^i\left ( \vz _{T}, \cdot \right ) \) and \(\vz _T\) is the last Markov-chain state at the previous SGD iteration \(t-1\). Then, the variance is given as \begin {align} \V {\vg _{\text {seq.}}} &= \V { \Esub {K\left (\vz _{T}, \vz \right )}{ \frac {1}{N} \sum _{i=1}^N \vs \left (\vlambda ; \vz _{T+i}\right ) \,\Bigg \vert \, \vz _T } } + \E { \Vsub {K\left (\vz _{T}, \vz \right )}{ \frac {1}{N} \sum _{i=1}^N \vs \left (\vlambda ; \vz _{T+i}\right ) \,\Bigg \vert \, \vz _T } } &\text {(Total Variance)} \\ &= \frac {1}{N^2} \sum _{i=1}^N \Vsub {K\left (\vz _{T}, \vz \right )}{ \E { \vs \left (\vlambda ; \vz _{T+i}\right ) \mid \vz _T } } \nonumber \\ &\qquad + \E { \frac {1}{N^2} \sum _{i=1}^N \Vsub {K\left (\vz _{T}, \vz \right )}{ \vs \left (\vlambda ; \vz _{T+i}\right ) \mid \vz _T } + \frac {2}{N^2} \sum _{i < j}^N \Cov { s\left (\vlambda ; \vz _{T+i}\right ), \vs \left (\vlambda ; \vz _{T+j}\right ) \mid \vz _T } } \\ &= \frac {1}{N^2} \sum _{i=1}^N \Vsub {K\left (\vz _{T}, \vz \right )}{ \E { \vs \left (\vlambda ; \vz _{T+i}\right ) \mid \vz _T } } \nonumber \\ &\qquad + \frac {1}{N^2} \sum _{i=1}^N \Esub {K\left (\vz _{T}, \vz \right )}{ \V { \vs \left (\vlambda ; \vz _{T+i}\right ) \mid \vz _T } } \nonumber \\ &\qquad + \frac {2}{N^2} \sum _{i < j} \E { \Cov { \vs \left (\vlambda ; \vz _{T+i}\right ), \vs \left (\vlambda ; \vz _{T+j}\right ) \mid \vz _T } }, \intertext {\text {where, by assuming stationarity such that \(\vz _{T} \sim \pi \left (\vz \right )\),}} &= \frac {1}{N^2} \sum _{i=1}^N \Vsub {\pi }{ \E { \vs \left (\vlambda ; \vz _{T+i}\right ) \mid \vz _T } } \qquad + \frac {1}{N^2} \sum _{i=1}^N \Esub {\pi }{ \V { \vs \left (\vlambda ; \vz _{T+i}\right ) \mid \vz _T } } \nonumber \\ &\qquad + \frac {2}{N^2} \sum _{i < j} \Esub {\pi }{ \Cov { \vs \left (\vlambda ; \vz _{T+i}\right ), \vs \left (\vlambda ; \vz _{T+j}\right ) \mid \vz _T } } \\ &= \frac {1}{N^2} \sum _{i=1}^N \Vsub {\pi }{ \vs \left (\vlambda ; \vz \right ) } + \frac {2}{N^2} \sum _{i < j} \Esub {\pi }{ \Cov { \vs \left (\vlambda ; \vz _{T+i}\right ), \vs \left (\vlambda ; \vz _{T+j}\right ) \mid \vz _T } }&\text {(Total Variance)} \\ &= \frac {1}{N} \Vsub {\pi }{ \vs \left (\vlambda ; \vz \right ) } + \frac {2}{N^2} \sum _{i < j} \Esub {\pi }{ \Cov { \vs \left (\vlambda ; \vz _{T+i}\right ), \vs \left (\vlambda ; \vz _{T+j}\right ) \mid \vz _T } } \\ &= \frac {\sigma ^2}{N} + \frac {2}{N^2} \sum _{i < j}^N \Esub {\pi }{ \Cov { \vs \left (\vlambda ; \vz _{T+i}\right ), \vs \left (\vlambda ; \vz _{T+j}\right ) \mid \vz _T } } \\ &= \frac {\sigma ^2}{N} + \frac {2}{N} \sum _{k=1}^{N-1} \left (1 - \frac {k}{N}\right ) \gamma _k \label {eq:seq_cov} \end {align} where \(\gamma _k\) is the \(k\)-lag autocovariance. \par Specifically for the IMH kernel,~\citet {tan_monte_2006} has analyzed \(\gamma _k\) based on the results of~\citet {Smith96exacttransition}. We extend his analysis to obtain our desired conclusion. Let us denote \(\gamma _k\) as the covariance between a state \(\vz \) and its \(k\)th lagged counterpart \(\vz _k\), and \(\Delta \left (\vz \right ) = \vs \left (\vz \right ) - \Esub {\pi }{\vs }\). Then, \begin {align} \gamma _k &= \int \Delta \left (\vz \right ) \left ( \int K^k\left (\vz , \vz _k \right ) \, \Delta \left (\vz _k\right ) \, d\vz _k \right ) \pi \left (d\vz \right ) \\ &= \int \Delta \left (\vz \right ) \left ( \int \Delta \left (\vz _k\right ) \, T_k\left (w\left (\vz \right ) \vee w\left (\vz _k\right ) \right ) \, \pi \left (d\vz _k\right ) + \Delta \left (\vz \right ) \, \lambda ^k\left (w\left (\vz \right )\right ) \right ) \pi \left (d\vz \right ) \\ &= \int \int \Delta \left (\vz \right ) \, \Delta \left (\vz _k\right ) \, T_k\left (w\left (\vz \right ) \vee w\left (\vz _k\right ) \right ) \, \pi \left (d\vz _k\right ) \, \pi \left (d\vz \right ) + \int \Delta ^2\left (\vz \right ) \, \lambda ^k\left (w\left (\vz \right )\right ) \, \pi \left (d\vz \right ). \end {align} \par For the first term,~\citet [Theorem 3]{tan_monte_2006} have shown that \begin {align} \int \int \Delta \left (\vz \right ) \, \Delta \left (\vz _k\right ) \, T_k\left (w\left (\vz \right ) \vee w\left (\vz _k\right ) \right ) \, \pi \left (d\vz _k\right ) \, \pi \left (d\vz \right ) \geq 0. \end {align} And for the second term, \begin {align} \frac {2}{N} \sum _{k=1}^{N-1} \left (1 - \frac {k}{N}\right ) \gamma _k &\geq \frac {2}{N} \sum _{k=1}^{N-1} \left (1 - \frac {k}{N}\right ) \int \Delta ^2\left (\vz \right ) \, \lambda ^k\left (w\left (\vz \right )\right ) \, \pi \left (d\vz \right ) \\ &= \frac {2}{N} \int \Delta ^2\left (\vz \right ) \, \sum _{k=1}^{N-1} \left (1 - \frac {k}{N}\right ) \, \lambda ^k\left (w\left (\vz \right )\right ) \, \pi \left (d\vz \right )\label {eq:autocov_bound} \\ &\geq 0, \end {align} which proves \(\V {\vg _{\text {seq.-IMH}}} - \V {\vg _{\text {par.}}} = C_{\text {gap}} \geq 0\). \par Analyzing the variance gap \(C_{\text {gap}}\) is a little more involved. Usually, traditional MCMC analysis invokes Ces\`aro's summability theorem for the sum in~\cref {eq:autocov_bound} by assuming \(N \rightarrow \infty \)~\citep {kung_discussions_1994}. In our case, we avoid this path in order to generalize our result to the small \(N\) regime. For clarity, we denote \(r = \lambda \left (w\left (\vz \right )\right )\). Then, \begin {align} \sum _{k=1}^{N-1} \left (1 - \frac {k}{N}\right ) \, r^k &= \sum _{k=1}^{N-1} \, r^k - \frac {1}{N}\,\sum _{k=1}^{N-1} \,k\, r^k \\ &= \frac {r \, \left ( 1 - r^{N-1} \right ) }{1 - r} - \frac {r}{N} \, \frac {1 - N\,r^{N-1} + \left (N-1\right )\,r^{N} }{{\left (1 - r\right )}^2} \\ &= \frac { r\,\left (r^N - N\,r + N - 1\right ) }{ N\,{\left (1-r\right )}^2 } \\ &= \frac { r\,\left ( N \left (1-r\right ) - \left (1-r^N\right ) \right ) }{ N\,{\left (1-r\right )}^2 } \\ &= \frac {r}{1-r} - \frac {r}{N}\,\frac {\left (1-r^N\right )}{{\left (1-r\right )}^2},\label {eq:r_exact} \end {align} which is monotonically increasing with respect to \(r\). \par There are several ways to analyze the behavior of this function. For example, the upper bound \begin {align} \frac {r}{1-r} - \frac {r}{N}\,\frac {\left (1-r^N\right )}{{\left (1-r\right )}^2} &\leq \frac {r}{1-r}\label {eq:r_approx} \end {align} becomes a good approximation for large \(N\). For small \(N\), it is still accurate for small values of \(r\). In the limit \(N \rightarrow \infty \), the upper bound becomes exact where we retrieve the result of~\citet [Theorem 3]{tan_monte_2006}. \par \textit {\textbf {Lower Bound of \(C_{\text {gap}}\)}}\quad On the other hand, it is also possible to derive a lower bound using the formula of geometric sums as \begin {align} -\frac {\left (1-r^N\right )}{{\left (1-r\right )}} &= -\frac {\left (1-r\right )\,\left (r^{N-1} + r^{N-2} + \ldots + r + 1\right )}{{\left (1-r\right )}} \\ &= -\left (r^{N-1} + r^{N-2} + \ldots + r^{N-2} + r + 1\right ) \\ &\geq -\left (r^2 + r^2 + \ldots + r^2 + r + 1\right ) \\ &= -\left ((N-2)\,r^2 + r + 1\right ) \end {align} where we have used the fact that \(0 \leq r \leq 1\) and \(N \geq 2\). By applying this to \cref {eq:r_exact}, \begin {align} \frac {r}{1-r} - \frac {r}{N}\,\frac {\left (1-r^N\right )}{{\left (1-r\right )}^2} &\geq \frac {r}{1-r} - \frac {r}{N}\,\frac {(N-2)\,r^2 + r + 1}{\left (1-r\right )} \\ &= \left (1 - \frac {1}{N}\right ) \frac {r}{1-r} - \frac {1}{N} \frac {r^2}{1-r} - \frac {N-2}{N} \frac {r^3}{1-r}.\label {eq:r_lowerbound} \end {align} \par \begin {figure}[H] \centering \subfloat [\(N=16\)]{ \begin {tikzpicture} \begin {semilogyaxis}[ tuftelike, axis x line shift=10pt, axis y line shift=10pt, xlabel = {\(r\)}, xmin = 0, xmax = 1.0, ymin = 0.01, ymax = 100, width = 6.0cm, height = 6.0cm, legend style = { draw = none, at={(0.02,0.98)}, anchor=north west, legend cell align={left}, } ] \addplot [ domain = 0.0001:0.999, thick, samples=256 ] { x/(1.0-x) - x/16.0*(1.0-x^(16))/(1-x)^(2) }; \addlegendentry {Exact} \par \addplot [ red, domain = 0.0001:0.9999, thick, samples=128 ] { (1 - 1/16)*(x/(1-x)) - (1/16)*(x^2/(1-x)) - (14/16)*(x^3/(1-x)) }; \addlegendentry {Lower bound} \addplot [ blue, domain = 0.0001:0.9999, thick, samples=128 ] { x/(1.0-x) }; \addlegendentry {Upper bound} \end {semilogyaxis} \end {tikzpicture} } \subfloat [\(N=64\)]{ \begin {tikzpicture} \begin {semilogyaxis}[ tuftelike, axis x line shift=10pt, axis y line shift=10pt, xlabel = {\(r\)}, xmin = 0, xmax = 1.0, ymin = 0.01, ymax = 100, width = 6.0cm, height = 6.0cm, legend style = { draw = none, at={(0.02,0.98)}, anchor=north west, legend cell align={left}, } ] \addplot [ domain = 0.0001:0.999, thick, samples=256 ] { x/(1.0-x) - x/64.0*(1.0-x^(64))/(1-x)^(2) }; \addlegendentry {Exact} \par \addplot [ red, domain = 0.0001:0.9999, thick, samples=128 ] { (1 - 1/64)*(x/(1-x)) - (1/64)*(x^2/(1-x)) - (62/64)*(x^3/(1-x)) }; \addlegendentry {Lower bound} \addplot [ blue, domain = 0.0001:0.9999, thick, samples=128 ] { x/(1.0-x) }; \addlegendentry {Upper bound} \end {semilogyaxis} \end {tikzpicture} } \caption {Comparison of \cref {eq:r_exact} against its lower and upper bounds.}\label {fig:r_comp}. \end {figure} \par The quality of the lower and upper bounds are visualized in \cref {fig:r_comp}. We can see that the lower bound is quite optimistic in general, but is more accurate for the small \(N\) regime. \par Now, let us develop our lower bound. Recall that \(r = \lambda \left (w\left (\vz \right )\right )\). \citet [Proof of Theorem 3]{tan_monte_2006} have shown that \(1 - \frac {1}{u} \leq \lambda \left (u\right ) \leq 1 - \frac {1}{w^*}\). Since the lower bound in \cref {eq:r_lowerbound} is monotonically increasing, applying the bound \(r \geq 1 - \frac {1}{w\left (\vz \right )}\) results in \begin {align} &\left (1 - \frac {1}{N}\right ) \frac {r}{1-r} - \frac {1}{N} \frac {r^2}{1-r} - \left (1 - \frac {2}{N}\right ) \frac {r^3}{1-r} \nonumber \\ &\geq \left (1 - \frac {1}{N}\right ) \, w\left (\vz \right ) \, \left (1 - \frac {1}{w\left (\vz \right )}\right ) - \frac {1}{N} \, w\left (\vz \right ) \, {\left (1 - \frac {1}{w\left (\vz \right )}\right )}^2 - \left (1 - \frac {2}{N}\right ) \, w\left (\vz \right ) \, {\left (1 - \frac {1}{w\left (\vz \right )}\right )}^3.\label {eq:w_init} \end {align} \par For clarity, we temporarily set \(\alpha = 1 - \frac {1}{N}\), \(\beta = \frac {1}{N}\), \(\gamma = 1 - \frac {2}{N}\). Then, \cref {eq:w_init} becomes \begin {align} &\alpha \, w\left (\vz \right ) \, \left (1 - \frac {1}{w\left (\vz \right )}\right ) - \beta \, w\left (\vz \right ) \, {\left (1 - \frac {1}{w\left (\vz \right )}\right )}^2 - \gamma \, w\left (\vz \right ) \, {\left (1 - \frac {1}{w\left (\vz \right )}\right )}^3 \nonumber \\ &= \alpha \, \left (w\left (\vz \right ) - 1\right ) - \beta \, {\left (w\left (\vz \right ) - 2 + \frac {1}{w\left (\vz \right )} \right )} - \gamma \, {\left (w\left (\vz \right ) - 3 + \frac {3}{w\left (\vz \right )} - \frac {1}{w^2\left (\vz \right )} \right )} \\ &= \left (\alpha - \beta - \gamma \right ) \, w\left (\vz \right ) + \left (-\alpha + 2\,\beta + 3\,\gamma \right ) + \left (-\beta - 3 \gamma \right ) \frac {1}{w\left (\vz \right )} + \gamma \frac {1}{w^2\left (\vz \right )} \\ &= \left (2 - \frac {3}{N}\right ) + \left (-3 + \frac {5}{N}\right ) \frac {1}{w\left (\vz \right )} + \left (1 - \frac {2}{N}\right ) \frac {1}{w^2\left (\vz \right )}. \end {align} \par Applying this to~\cref {eq:autocov_bound}, \begin {align} &\frac {2}{N} \int \Delta ^2\left (\vz \right ) \, \sum _{k=1}^{N-1} \left (1 - \frac {k}{N}\right ) \, \lambda ^k\left (w\left (\vz \right )\right ) \, \pi \left (d\vz \right ) \\ &\geq \frac {2}{N} \int \Delta ^2\left (\vz \right ) \, \left \{ \left (2 - \frac {3}{N}\right ) + \left (-3 + \frac {5}{N}\right ) \frac {1}{w\left (\vz \right )} + \left (1 - \frac {2}{N}\right ) \frac {1}{w^2\left (\vz \right )} \right \} \, \pi \left (d\vz \right ) \\ &= \frac {2}{N}\, \left (2 - \frac {3}{N}\right ) \, \sigma ^2 + \frac {2}{N}\, \left (-3 + \frac {5}{N}\right ) \int \Delta ^2\left (\vz \right ) \, \frac {1}{w\left (\vz \right )} \, \pi \left (d\vz \right ) + \frac {2}{N}\, \left (1 - \frac {2}{N}\right ) \int \Delta ^2\left (\vz \right ) \, \frac {1}{w^2\left (\vz \right )} \, \pi \left (d\vz \right ) \\ &= \frac {2}{N}\, \left (2 - \frac {3}{N}\right ) \, \sigma ^2 + \frac {2}{N}\, \left (-3 + \frac {5}{N}\right ) \int \Delta ^2\left (\vz \right ) \, q\left (\vz \right )\,d\vz + \frac {2}{N}\, \left (1 - \frac {2}{N}\right ) \int \Delta ^2\left (\vz \right ) \, \frac {q^2\left (\vz \right )}{\pi \left (\vz \right )}\, d\vz . \end {align} \par For the last term, \begin {align} \int \Delta ^2\left (\vz \right ) \, \frac {q^2\left (\vz \right )}{\pi \left (\vz \right )}\, d\vz &= \Esub {q}{ \Delta ^2\left (\vz \right ) \, \frac {q\left (\vz \right )}{\pi \left (\vz \right )} } \\ &= \Esub {q}{ \Delta ^2\left (\vz \right ) } \, \Esub {q}{ \frac {q\left (\vz \right )}{\pi \left (\vz \right )} } + \text {Cov}_{q}\left (\Delta ^2\left (\vz \right ),\, w^{-1}\left (\vz \right )\right ) \\ &\geq \Esub {q}{ \Delta ^2\left (\vz \right ) } \, \exp \left ( \DKL {q}{\pi } \right ) + \text {Cov}_{q}\left (\Delta ^2\left (\vz \right ),\, w^{-1}\left (\vz \right )\right ). \end {align} \par After some algebra, we obtain our result \begin {align} &\frac {2}{N} \, \left (2 - \frac {3}{N}\right ) \, \sigma ^2 + \frac {2}{N} \, \left (-3 + \frac {5}{N}\right ) \int \Delta ^2\left (\vz \right ) \, q\left (\vz \right )\,d\vz + \frac {2}{N} \, \left (1 - \frac {2}{N}\right ) \int \Delta ^2\left (\vz \right ) \, \frac {q^2\left (\vz \right )}{\pi \left (\vz \right )}\, d\vz \\ &\geq \frac {2}{N} \, \left (2 - \frac {3}{N}\right ) \, \sigma ^2 + \frac {2}{N} \, \left (-3 + \frac {5}{N}\right ) \Esub {q}{ \Delta ^2 }\nonumber \\ &\quad + \frac {2}{N} \, \left (1 - \frac {2}{N}\right ) \left \{\, \Esub {q}{ \Delta ^2 } \, \exp \left ( \DKL {q}{\pi } \right ) + \text {Cov}_{q}\left (\Delta ^2\left (\vz \right ),\, w^{-1}\left (\vz \right )\right ) \,\right \} \\ &\approx \frac {4}{N} \, \sigma ^2 - \frac {6}{N} \Esub {q}{ \Delta ^2 } + \frac {2}{N} \, \left \{\, \Esub {q}{ \Delta ^2 } \, \exp \left ( \DKL {q}{\pi } \right ) + \text {Cov}_{q}\left (\Delta ^2\left (\vz \right ),\, w^{-1}\left (\vz \right )\right ) \,\right \} + \mathcal {O}\left (\frac {1}{N^2}\right ) \\ &= \frac {4}{N} \, \sigma ^2 + \frac {2}{N} \, \Esub {q}{ \Delta ^2 } \, \left (\, \exp \left ( \DKL {q}{\pi } \right ) - 3 \,\right ) + \frac {2}{N} \, \text {Cov}_{q}\left (\Delta ^2\left (\vz \right ),\, w^{-1}\left (\vz \right )\right ) + \mathcal {O}\left (\frac {1}{N^2}\right ). \end {align} \par \textit {\textbf {Asymptotic Approximation of \(C_{\text {gap}}\)}}\quad Before concluding, we discuss the case where \(N\) is large such that \begin {align} \frac {r}{1-r} - \frac {r}{N}\,\frac {\left (1-r^N\right )}{{\left (1-r\right )}^2} \xrightarrow {N \rightarrow \infty } \frac {r}{1-r}. \end {align} Using the lower bound \(r \geq 1 - \frac {1}{w\left (\vz \right )}\), \begin {align} \frac {r}{1-r} \geq w\left (\vz \right ) - 1. \end {align} Therefore, \begin {align} \int \Delta ^2\left (\vz \right ) \, \sum _{k=1}^{N-1} \left (1 - \frac {k}{N}\right ) \, \lambda ^k\left (w\left (\vz \right )\right ) \, \pi \left (d\vz \right ) &\rightarrow \int \Delta ^2\left (\vz \right ) \, \left ( w\left (\vz \right ) - 1 \right ) \, \pi \left (d\vz \right ) \\ &= \int \Delta ^2\left (\vz \right ) \, w\left (\vz \right ) \, \pi \left (d\vz \right ) - \sigma ^2 \\ &= \int \Delta ^2\left (\vz \right ) \, \frac {\pi \left (\vz \right )}{q\left (\vz \right )} \, \pi \left (d\vz \right ) - \sigma ^2 \\ &= \Esub {\pi }{\Delta ^2\left (\vz \right )} \, \Esub {\pi }{\frac {\pi \left (\vz \right )}{q\left (\vz \right )}} + \text {Cov}_{\pi }\left (\Delta ^2\left (\vz \right ), w\left (\vz \right ) \right ) - \sigma ^2 \\ &= \sigma ^2 \, \left (\int \pi \left (\vz \right )\,\frac {\pi \left (\vz \right )}{q\left (\vz \right )} d\vz - 1\right ) + \text {Cov}_{\pi }\left (\Delta ^2\left (\vz \right ), w\left (\vz \right ) \right ) \\ &\geq \sigma ^2 \, \left ( \exp \left (\DKL {\pi }{q}\right ) - 1\right ) + \text {Cov}_{\pi }\left (\Delta ^2\left (\vz \right ), w\left (\vz \right ) \right ). \end {align}\end{proof}
\prAtEndRestateiv*
\label{proofsection:prAtEndiv}\begin{proof}[Proof of \autoref{thm:prAtEndiv}]\phantomsection\label{proof:prAtEndiv}Since the kernel \(K\) is a mixture of the accept and reject cases, \begin {align} \frac {K^t\left (\vz , \vz ^*\right )}{\pi \left (\vz ^*\right )} &\leq \max \Bigg \{ \underbrace { \frac {T_t\left (w\left (\vz \right ) \vee w\left (\vz ^*\right )\right )}{\pi \left (\vz ^*\right )} }_{\text {accept}}, \underbrace { \frac {\lambda ^t\left (w\left (\vz \right )\right )}{\pi \left (\vz \right )} }_{\text {accept}} \Bigg \} &\text {(\cref {thm:imh_exact_kernel})}.\label {eq:prop_assumption_ratio} \end {align} For the accept case, \begin {align} \frac {T_t\left (w\left (\vz \right ) \vee w\left (\vz ^*\right )\right )}{\pi \left (\vz ^*\right )} &\leq \frac {t}{w\left (\vz \right ) \vee w\left (\vz ^*\right )} \, {\left (1 - \frac {1}{w^*}\right )}^{t-1} &\text {(\cref {thm:tn_bound})} \\ &\leq \frac {t}{\epsilon } \, {\left (1 - \frac {1}{w^*}\right )}^{t-1} . \end {align} and for the reject case, \begin {align} \frac {\lambda ^t\left (w\left (\vz \right )\right )}{\pi \left (\vz \right )} \leq \frac {\lambda ^t\left (w\left (\vz \right )\right )}{\pi \left (\vz \right )} \leq \frac {1}{\pi \left (\vz \right )} {\left (1 - \frac {1}{w^*}\right )}^{t} \leq \frac {1}{\epsilon } {\left (1 - \frac {1}{w^*}\right )}^{t}. \end {align} Therefore,~\cref {eq:prop_assumption_ratio} becomes \begin {align} \frac {K^t\left (\vz , \vz ^*\right )}{\pi \left (\vz ^*\right )} \leq \max \left \{\; \frac {t}{\epsilon } \, {\left (1 - \frac {1}{w^*}\right )}^{t-1}, \frac {1}{\epsilon } \, {\left (1-\frac {1}{w^*}\right )}^t \;\right \} &\leq \frac {t}{\epsilon } \, {\left (1 - \frac {1}{w^*}\right )}^{t-1}, \end {align} where the last inequality follows from \(1 - \frac {1}{w^*} \leq 1\)\end{proof}
\prAtEndRestatev*
\label{proofsection:prAtEndv}\begin{proof}[Proof of \autoref{thm:prAtEndv}]\phantomsection\label{proof:prAtEndv}The bias of a the parallel state estimator kernel is bounded as {\small \begin {align} &\mathrm {Bias}\left [ \vg _{\text {par.-IMH}} \right ] \\ &= \norm { \int \eta \left (\vz ^{(i)}\right ) \, \int \frac {1}{N}\,\sum ^{N}_{i=1} K\left (\vz ^{(i)}, \vz ^*\right )\,\vs \left (\vlambda ;\vz ^*\right ) \, d\vz ^* \, d\vz _{(1:N)} - \int \pi \left (\vz ^*\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* } \\ &= \norm { \frac {1}{N} \, \int \eta \left (\vz ^{(i)}\right ) \, \int \sum ^{N}_{i=1} \big (\, K\,(\vz ^{(i)}, \vz ^*\,) - \pi \,(\vz ^*\,) \,\big ) \, \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz _{(1:N)} } \\ &= \norm { \frac {1}{N} \sum ^{N}_{i=1} \, \int \eta \left (\vz ^{(i)}\right ) \, \int \big (\, K\,(\vz ^{(i)}, \vz ^*\,) - \pi \,(\vz ^*\,) \,\big ) \, \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz _{(1:N)} } \\ &\leq \frac {1}{N} \sum ^{N}_{i=1} \, \norm { \int \eta \left (\vz ^{(i)}\right ) \, \int \big (\, K\,(\vz ^{(i)}, \vz ^*\,) - \pi \,(\vz ^*\,) \,\big ) \, \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz _{(1:N)} }&\text {(Triangle Inequality)} \\ &= \norm { \int \eta \left (\vz \right ) \, \int \big (\, K\,(\vz , \vz ^*\,) - \pi \,(\vz ^*\,) \,\big ) \, \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz }&\text {(Independence)} \\ &= \norm { \int \eta \left (\vz \right ) \, \int \pi \,(\vz ^*\,) \, \big (\, T_1\left (w\left (\vz \right ) \vee w\left (\vz ^*\right )\right ) - 1 \,\big ) \, \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz + \int \eta \left (\vz \right ) \lambda \left (w\left (\vz \right )\right ) \, \vs \left (\vlambda ;\vz \right ) }&\text {(\cref {thm:imh_exact_kernel})} \\ &\leq \norm { \int \eta \left (\vz \right ) \int \pi \left (\vz ^*\right )\, \big (\, T_1\,(w\,(\vz \,) \vee w\,(\vz ^*\,)) - 1) \,\big )\, \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz } + \norm { \int \eta \left (\vz \right ) \lambda \left (w\left (\vz \right )\right ) \, \vs \left (\vlambda ;\vz \right ) d\vz }&\text {(Triangle Inequality)} \\ &= \underbrace { \norm { \int \eta \left (\vz \right ) \int \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz \right ) \vee w\left (\vz ^*\right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* d\vz } }_{\text {T1}} + \underbrace { \norm { \int \eta \left (\vz \right ) \, \lambda \left (w\left (\vz \right )\right ) \, \vs \left (\vlambda ;\vz \right ) d\vz }}_{\text {T2}}.\label {eq:imh_bias_main} \end {align} } \par For T1, we denote the set of the accepted proposals \(A\) as \(A\left (w\right ) = \{\,\vz ^* \mid w\left (\vz ^*\right ) > w\,\}\), and the set of the rejected proposals \(R\) as \(R\left (w\right ) = A^c\left (w\right )\). Then, \begin {align} &\norm { \int \eta \left (\vz \right ) \left ( \int \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz \right ) \vee w\left (\vz ^*\right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \right ) d\vz } \\ &= \norm { \int \eta \left (\vz \right ) \left \{ \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz ^*\right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* + \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz \right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \right \} d\vz } \\ &\leq \underbrace { \norm { \int \eta \left (\vz \right ) \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz ^*\right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* d\vz } }_{\text {T3}} + \underbrace { \norm { \int \eta \left (\vz \right ) \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz \right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* d\vz } }_{\text {T4}} \end {align} For T3, \begin {align} \norm { \int \eta \left (\vz \right ) \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz ^*\right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* d\vz } &= \norm { \int \eta \left (\vz \right )\,d\vz \; \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz ^*\right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* } \\ &= \norm { \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz ^*\right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* } \\ &\leq \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \abs { \frac {1}{w\left (\vz ^*\right )} - 1 } \, \norm { \vs \left (\vlambda ;\vz ^*\right ) } \, d\vz ^* \\ &\leq \int \pi \left (\vz ^*\right )\, \abs { \frac {1}{w\left (\vz ^*\right )} - 1 } \, \norm { \vs \left (\vlambda ;\vz ^*\right ) } \, d\vz ^* \\ &\leq \int \abs { \pi \left (\vz ^*\right ) - q\left (\vz ^*\right ) } \, \norm { \vs \left (\vlambda ;\vz ^*\right ) } \, d\vz ^* \\ &\leq L \, \int \abs { \pi \left (\vz ^*\right ) - q\left (\vz ^*\right ) } \, d\vz ^* \\ &\leq 2 \, L \, \DTV {\pi }{q}.\label {eq:imh_bias_left} \end {align} \par Now, for T4, \begin {align} \norm { \int \eta \left (\vz \right ) \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \left (\frac {1}{w\left (\vz \right )} - 1\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* d\vz } &= \norm { \int \eta \left (\vz \right ) \, \left (\frac {1}{w\left (\vz \right )} - 1\right ) \, d\vz \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \vs \left (\vz ^*\right ) d\vz ^* } \\ &= \underbrace { \norm { \int \eta \left (\vz \right ) \, \left (\frac {1}{w\left (\vz \right )} - 1\right ) d\vz } }_{\text {T5}} \, \underbrace { \norm { \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \vs \left (\vz ^*\right ) d\vz ^*, } }_{\text {T6}}, \end {align} while T5 is bounded as \begin {align} \norm { \int \eta \left (\vz \right ) \, \left (\frac {1}{w\left (\vz \right )} - 1\right ) \, d\vz } &= \int \eta \left (\vz \right ) \, \abs {\frac {q\left (\vz \right )}{\pi \left (\vz \right )} - 1} \, d\vz \\ &= \int \frac {\eta \left (\vz \right )}{\pi \left (\vz \right )} \, \abs {\left (q\left (\vz \right ) - \pi \left (\vz \right )\right )} \, d\vz \\ &\leq M \int \, \abs {q\left (\vz \right ) - \pi \left (\vz \right )} \, d\vz \\ &= 2 \, M \, \DTV {\pi }{q} \end {align} and T6 is bounded as \begin {align} \norm { \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* } &\leq \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right )\, \norm {\vs \left (\vlambda ;\vz ^*\right )} d\vz ^* \\ &\leq \int \pi \left (\vz ^*\right )\, \norm {\vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \\ &\leq L \, \int \pi \left (\vz ^*\right )\, \, d\vz ^* \\ &\leq L. \end {align} \par \par Finally, for T2, \begin {align} \norm { \int \eta \left (\vz \right ) \, \lambda \left (w\left (\vz \right )\right ) \, \vs \left (\vlambda ;\vz \right ) d\vz } &\leq \int \eta \left (\vz \right ) \, \lambda \left (w\left (\vz \right )\right ) \, \norm { \vs \left (\vlambda ;\vz \right ) }\, d\vz \\ &\leq \int \eta \left (\vz \right ) \, \left (1 - \frac {1}{w^*}\right ) \, \norm { \vs \left (\vlambda ;\vz \right ) }\, d\vz \\ &= \left (1 - \frac {1}{w^*}\right ) \int \eta \left (\vz \right ) \, \norm { \vs \left (\vlambda ;\vz \right ) }\, d\vz \\ &\leq L \, \left (1 - \frac {1}{w^*}\right ) \, \int \eta \left (\vz \right ) \, d\vz \\ &= L \, \left (1 - \frac {1}{w^*}\right ) \end {align} \par By combining T1, T2, T3, and T4, we conclude \begin {align} \mathrm {Bias}\left [ \vg _{\text {par.-IMH}} \right ] &\leq 2 \, L \, \DTV {\pi }{q} + 2 \, L \, M \, \DTV {\pi }{q} + L \, \left (1 - \frac {1}{w^*}\right ) \\ &= 2 \, L \, \left ( 1 + M \right ) \, \DTV {\pi }{q} + L \, \left (1 - \frac {1}{w^*}\right ) \\ &\leq 2 \, L \, \left ( 1 + M \right ) \, \sqrt {\DKL {\pi }{q}} + L \, \left (1 - \frac {1}{w^*}\right ). &\text {(Pinsker's Inequality)} \end {align}\end{proof}
\prAtEndRestatevi*
\label{proofsection:prAtEndvi}\begin{proof}[Proof of \autoref{thm:prAtEndvi}]\phantomsection\label{proof:prAtEndvi}{\small \begin {align} &\abs { \text {Bias}\left [\vg _{\text {seq.-IMH}}\right ] - \text {Bias}\left [\vg _{\text {par.-IMH}}\right ] } \\ &\leq \norm { \int \eta \left (\vz \right ) \int \frac {1}{N} \sum _{n=1}^{N} K^n\left (\vz , \vz ^*\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz - \int \eta \left (\vz \right ) \int \frac {1}{N} \sum _{n=1}^{N} K\left (\vz ^{(n)}, \vz ^*\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz _{(1:N)} }&\text {\footnotesize (Triangle Inequality)} \\ &= \norm { \int \eta \left (\vz \right ) \int \frac {1}{N} \sum _{n=1}^{N} K^n\left (\vz , \vz ^*\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz - \int \eta \left (\vz \right ) \int \frac {1}{N} \sum _{n=1}^{N} K\left (\vz , \vz ^*\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz }&\text {\footnotesize (Independence)} \\ &= \norm { \int \eta \left (\vz \right ) \int \frac {1}{N} \sum _{n=1}^{N} \left (K^n\left (\vz , \vz ^*\right ) - K\left (\vz , \vz ^*\right ) \right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz } \\ &\leq \frac {1}{N} \sum _{n=1}^{N} \norm { \int \int \eta \left (\vz \right ) \left (K^n\left (\vz , \vz ^*\right ) - K\left (\vz , \vz ^*\right ) \right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz }&\text {\footnotesize (Triangle Inequality)} \\ &= \frac {1}{N} \sum _{n=1}^{N} \Big \lVert \; \int \eta \left (\vz \right ) \int \pi \left (\vz ^*\right ) \, \left ( T_n\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) - T_1\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) \right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz \nonumber \\ &\quad \qquad \qquad + \int \eta \left (\vz \right ) \left (\lambda ^{n}\left ( w\left (\vz \right ) \right ) - \lambda \left ( w\left (\vz \right ) \right )\right ) \vs \left (\vlambda ;\vz \right ) \, d\vz \;\Big \rVert &\text {\footnotesize (\cref {thm:imh_exact_kernel})} \\ &\leq \frac {1}{N} \sum _{n=1}^{N} \norm { \int \eta \left (\vz \right ) \int \pi \left (\vz ^*\right ) \, \left ( T_n\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) - T_1\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right )\right ) \vs \left (\vlambda ;\vz ^*\right ) d\vz ^* \, d\vz } \nonumber \\ &\qquad + \frac {1}{N} \sum _{n=1}^{N} \norm { \int \eta \left (\vz \right ) \left (\lambda ^{n}\left ( w\left (\vz \right ) \right ) - \lambda \left ( w\left (\vz \right ) \right )\right ) \vs \left (\vlambda ;\vz \right ) \, d\vz } &\text {\footnotesize (Triangle Inequality)} \\ &\leq \frac {1}{N} \sum _{n=1}^{N} \underbrace { \int \eta \left (\vz \right ) \int \pi \left (\vz ^*\right ) \, \abs { T_n\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) - T_1\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) } \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \, d\vz }_{T1} \nonumber \\ &\qquad + \frac {1}{N} \sum _{n=1}^{N} \underbrace { \int \eta \left (\vz \right ) \abs { \lambda ^{n}\left ( w\left (\vz \right ) \right ) - \lambda \left ( w\left (\vz \right ) \right )} \; \norm {\vs \left (\vlambda ;\vz \right )} \, d\vz }_{T2}\label {eq:bias_diff_bound_master} \end {align} } \par Now, denoting \(w^{\prime } = w\left (\vz \right ) \vee w\left (\vz ^*\right )\), \begin {align} \abs { T_n\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) - T_1\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) } &= \abs { \int _{w^{\prime }}^{\infty } \frac {1}{v^2} \, n \, \lambda ^{n-1}\left (w\left (\vz \right )\right ) \, dv - \int _{w^{\prime }}^{\infty } \frac {1}{v^2} dv } \\ &= \abs { \int _{w^{\prime }}^{\infty } \frac {1}{v^2} \left ( n \, \lambda ^{n-1}\left (w\left (\vz \right )\right ) - 1 \right ) \, dv } \\ &\leq \int _{w^{\prime }}^{\infty } \frac {1}{v^2} \, \abs { n \, \lambda ^{n-1}\left (w\left (\vz \right )\right ) - 1 } \, dv.\label {eq:diff_T} \end {align} \par Since \[ 0 \leq \lambda \left (w\left (\vz \right )\right ) \leq 1 - \frac {1}{w^*} \leq 1, \] if follows that \[ -1 \leq n \, \lambda ^{n-1}\left (w\left (\vz \right )\right ) - 1 \leq n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, \] and thus \[ \abs { n \, \lambda ^{n-1}\left (w\left (\vz \right )\right ) - 1 } \leq \max \left \{\; n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1\;\right \}. \] \par Then, we can bound~\cref {eq:diff_T} as \begin {align} \int _{w^{\prime }}^{\infty } \frac {1}{v^2} \, \abs { n \, \lambda ^{n-1}\left (w\left (\vz \right )\right ) - 1 } \, dv &\leq \int _{w^{\prime }}^{\infty } \frac {1}{v^2} \, \max \left \{\, n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1 \,\right \}\, dv \\ &= \max \left \{\, n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1 \,\right \}\,\int _{w^{\prime }}^{\infty } \frac {1}{v^2} \\ &= \max \left \{\, n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1 \,\right \} \, \frac {1}{w^{\prime }} \\ &= \max \left \{\, n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1 \,\right \} \frac {1}{w\left (\vz \right ) \vee w\left (\vz ^*\right )}. \end {align} \par Now, back to~\cref {eq:bias_diff_bound_master}, T1 can be bounded as \begin {align} &\int \eta \left (\vz \right ) \int \pi \left (\vz ^*\right ) \, \abs { T_n\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) - T_1\left (w\left (\vz \right ) \vee w\left (\vz ^*\right ) \right ) } \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \, d\vz \\ &\leq \int \eta \left (\vz \right ) \int \pi \left (\vz ^*\right ) \, \max \left \{\, n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1 \,\right \}\,\frac {1}{w\left (\vz \right ) \vee w\left (\vz ^*\right )}\, \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \, d\vz \\ &= \max \left \{\, n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1 \,\right \} \nonumber \\ &\qquad \times \int \eta \left (\vz \right ) \, \left \{\; \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right ) \, \frac {1}{w\left (\vz ^*\right )} \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* + \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right ) \, \frac {1}{w\left (\vz \right )} \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \;\right \} \, d\vz \\ &= \max \left \{\, n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1 \,\right \} \nonumber \\ &\qquad \times \left \{\; \underbrace { \int \eta \left (\vz \right ) \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right ) \, \frac {1}{w\left (\vz ^*\right )} \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \, d\vz }_{T3} + \underbrace { \int \eta \left (\vz \right ) \, \frac {1}{w\left (\vz \right )} \, d\vz \, \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right ) \, \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \, d\vz }_{T4} \;\right \}. \end {align} \par T3 reduces to \begin {align} \int \eta \left (\vz \right ) \int _{A\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right ) \, \frac {1}{w\left (\vz ^*\right )} \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \, d\vz &\leq \int \eta \left (\vz \right ) \int \pi \left (\vz ^*\right ) \, \frac {1}{w\left (\vz ^*\right )} \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \, d\vz \\ &= \int \eta \left (\vz \right ) \, d\vz \, \int \pi \left (\vz ^*\right ) \, \frac {1}{w\left (\vz ^*\right )} \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \\ &= \int \pi \left (\vz ^*\right ) \, \frac {1}{w\left (\vz ^*\right )} \; \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \\ &= \int q\left (\vz ^*\right ) \, \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \\ &= \Esub {q}{\norm { \vs \left (\vlambda ;\vz ^*\right )}}, \end {align} which is a positive constant. \par On the other hand, T4 becomes \begin {align} \int \eta \left (\vz \right ) \, \frac {1}{w\left (\vz \right )} \, d\vz \, \int _{R\left (w\left (\vz \right )\right )} \pi \left (\vz ^*\right ) \, \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* &\leq \int \eta \left (\vz \right ) \, \frac {q\left (\vz \right )}{\pi \left (\vz \right )} \, d\vz \, \int \pi \left (\vz ^*\right ) \, \norm { \vs \left (\vlambda ;\vz ^*\right )} \, d\vz ^* \\ &= \Esub {\pi }{\norm { \vs \left (\vlambda ;\vz ^*\right )}} \, \int \eta \left (\vz \right ) \, \frac {q\left (\vz \right )}{\pi \left (\vz \right )} \, d\vz \, \\ &= \Esub {\pi }{\norm { \vs \left (\vlambda ;\vz ^*\right )}} \, \int q\left (\vz \right ) \, \frac {\eta \left (\vz \right )}{\pi \left (\vz \right )} \, d\vz \, \\ &\leq M \, \Esub {\pi }{\norm { \vs \left (\vlambda ;\vz ^*\right )}} \int q\left (\vz \right ) \, d\vz \, \\ &= M \, \Esub {\pi }{\norm { \vs \left (\vlambda ;\vz ^*\right )}}. \end {align} \par Meanwhile, for T2, \begin {align} \frac {1}{N} \sum _{n=1}^{N} \int \eta \left (\vz \right ) \abs { \lambda ^{n}\left ( w\left (\vz \right ) \right ) - \lambda \left ( w\left (\vz \right ) \right )} \; \norm {\vs \left (\vlambda ;\vz \right )} \, d\vz &= \frac {1}{N} \sum _{n=1}^{N} \int \eta \left (\vz \right ) \lambda \left ( w\left (\vz \right ) \right ) \, \abs {\lambda ^{n-1}\left ( w\left (\vz \right ) \right ) - 1} \; \norm {\vs \left (\vlambda ;\vz \right )} \, d\vz \\ &\leq \frac {1}{N} \sum _{n=1}^{N} \int \eta \left (\vz \right ) \lambda \left ( w\left (\vz \right ) \right ) \, \norm {\vs \left (\vlambda ;\vz \right )} \, d\vz \\ &= \int \eta \left (\vz \right ) \lambda \left ( w\left (\vz \right ) \right ) \, \norm {\vs \left (\vlambda ;\vz \right )} \, d\vz \\ &\leq \left ( 1 - \frac {1}{w^*} \right ) \, \int \eta \left (\vz \right ) \, \norm {\vs \left (\vlambda ;\vz \right )} \, d\vz \\ &\leq \left ( 1 - \frac {1}{w^*} \right ) \, \Esub {\eta }{\norm {\vs \left (\vlambda ;\vz \right )}}. \end {align} Combining T1, T2, T3, and T4, we conclude by \begin {align} &\abs { \text {Bias}\left [\vg _{\text {seq.-IMH}}\right ] - \text {Bias}\left [\vg _{\text {par.-IMH}}\right ] } \\ &\leq \max \left \{\, n \, {\left (1 - \frac {1}{w^*}\right )}^{n-1} - 1, 1 \,\right \} \, \left (\Esub {q}{\norm {\vs \left (\vlambda ;\vz \right )}} + M \, \Esub {\pi }{\norm { \vs \left (\vlambda ;\vz ^*\right )}}\right ) + \left ( 1 - \frac {1}{w^*} \right ) \, \Esub {\eta }{\norm {\vs \left (\vlambda ;\vz \right )}}. \end {align}\end{proof}
\prAtEndRestatevii*
\label{proofsection:prAtEndvii}\begin{proof}[Proof of \autoref{thm:prAtEndvii}]\phantomsection\label{proof:prAtEndvii}\begin {align*} \DKL {\pi }{ q_{\vlambda } } = \int \pi \left (\vz \right ) \log \frac {\pi \left (\vz \right )}{q_{\vlambda }\left (\vz \right )}\,d\vz \leq \int \pi \left (\vz \right ) \log w^* \, d\vz = \log w^* \end {align*}\end{proof}
