
\begin{theoremEnd}{theorem}
Assuming stationarity, the variance of the \(N\)-state sequential state estimator with an IMH kernel is larger than the variance of the parallel state estimator.
\end{theoremEnd}
%
\begin{proofEnd}
First, remember that the estimator is defined as
  \begin{align*}
    \vg_{\text{seq.}}\left(\vlambda\right) = \frac{1}{N} \sum_{i=1}^N \vs\left(\vlambda; \vz_{T+i}\right),
  \end{align*}
where \( \vz_{T+i} \sim K_{\vlambda_{t-1}}^i\left( \vz_{T}, \cdot \right) \) and \(\vz_T\) is the last Markov-chain state at the previous SGD iteration \(t-1\).
Then, the variance is given as
\begin{align}
  \V{g_{\text{seq.}}}
  &= \V{ \Esub{K\left(\vz_{T}, \vz\right)}{ \frac{1}{N} \sum_{i=1}^N \vs\left(\vlambda; \vz_{T+i}\right)  \,\Bigg\vert\, \vz_T } }
  + \E{ \Vsub{K\left(\vz_{T}, \vz\right)}{ \frac{1}{N} \sum_{i=1}^N \vs\left(\vlambda; \vz_{T+i}\right) \,\Bigg\vert\,  \vz_T } } &\text{(Total Variance)}  \nonumber\\
  &= \frac{1}{N^2} \sum_{i=1}^N \Vsub{K\left(\vz_{T}, \vz\right)}{ \E{  \vs\left(\vlambda; \vz_{T+i}\right) \mid \vz_T } }  \nonumber\\
  &\qquad + \E{ \frac{1}{N^2}  \sum_{i=1}^N  \Vsub{K\left(\vz_{T}, \vz\right)}{ \vs\left(\vlambda; \vz_{T+i}\right)  \mid  \vz_T } 
+ \frac{2}{N^2}  \sum_{i < j}^N \Cov{ s\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
  }  \nonumber\\
  &= \frac{1}{N^2} \sum_{i=1}^N \Vsub{K\left(\vz_{T}, \vz\right)}{ \E{  \vs\left(\vlambda; \vz_{T+i}\right) \mid \vz_T } }  \nonumber\\
   &\qquad+ \frac{1}{N^2} \sum_{i=1}^N   \Esub{K\left(\vz_{T}, \vz\right)}{ \V{ \vs\left(\vlambda; \vz_{T+i}\right)  \mid  \vz_T } } \nonumber\\
   &\qquad+ \frac{2}{N^2}  \sum_{i < j}^N \E{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
  },\nonumber
\intertext{\text{where by assuming stationarity such that \(\vz_{T} \sim \pi\left(\vz\right)\),}}
  &= \frac{1}{N^2} \sum_{i=1}^N \Vsub{\pi}{ \E{  \vs\left(\vlambda; \vz_{T+i}\right) \mid \vz_T } } 
   \qquad+ \frac{1}{N^2} \sum_{i=1}^N   \Esub{\pi}{ \V{ \vs\left(\vlambda; \vz_{T+i}\right)  \mid  \vz_T } } \nonumber\\
 &\qquad+ \frac{2}{N^2}  \sum_{i < j}^N \Esub{\pi}{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
  } \nonumber\\
  &= \frac{1}{N^2} \sum_{i=1}^N \Vsub{\pi}{ \vs\left(\vlambda; \vz\right) } 
 + \frac{2}{N^2}  \sum_{i < j}^N \Esub{\pi}{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
  }&\text{(Total Variance)} \nonumber\\
  &= \frac{1}{N} \Vsub{\pi}{ \vs\left(\vlambda; \vz\right) } 
 + \frac{2}{N^2}  \sum_{i < j}^N \Esub{\pi}{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
  } \nonumber\\
  &= \frac{\sigma^2}{N}
 + \frac{2}{N^2}  \sum_{i < j}^N \Esub{\pi}{ \Cov{ \vs\left(\vlambda; \vz_{T+i}\right), \vs\left(\vlambda; \vz_{T+j}\right) \mid  \vz_T }
  } \nonumber\\
  &= \frac{\sigma^2}{N}
 + \frac{2}{N} \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right) \gamma_k \label{eq:seq_cov}
\end{align}
where \(\gamma_k\) is the \(k\)-lag autocovariance.
Normally, theoretical analysis of MCMC tends to invoke Ces\`aro's summability theorem by assuming \(N \rightarrow \infty\).
In our case, avoid this path to generlize to the small \(N\) regime.

For the IMH kernel,~\citet{tan_monte_2006} has analyzed \(\gamma_k\) based on the results of~\citet{Smith96exacttransition}.
We extend his analysis to obtain our desired conclusion.
Let us denote the \(\gamma_k\) as the covariance between \(\vz\) and \(\vz_k\), and \(\Delta \left(\vz\right) = \vs\left(\vz\right) - \Esub{\pi}{\vs}\).
Then,
\begin{align}
  \gamma_k
  &= \int \Delta\left(\vz\right) \left(
  \int K^k\left(\vz, \vz_k \right) \, \Delta\left(\vz_k\right) \, d\vz_k
  \right) \pi\left(d\vz\right) 
  \\
  &=
  \int \Delta\left(\vz\right)
  \left(
  \int \Delta\left(\vz_k\right) \,
  T_k\left(w\left(\vz\right) \vee w\left(\vz_k\right) \right) \, \pi\left(d\vz_k\right)
  + \Delta\left(\vz\right) \, \lambda^k\left(w\left(\vz\right)\right)
  \right) \pi\left(d\vz\right)
  \\
  &=
  \int \int
  \Delta\left(\vz\right) \,
  \Delta\left(\vz_k\right) \,
  T_k\left(w\left(\vz\right) \vee w\left(\vz_k\right) \right) \, \pi\left(d\vz_k\right) \, \pi\left(d\vz\right)
  +
  \int
  \Delta^2\left(\vz\right) \, \lambda^k\left(w\left(\vz\right)\right) \, \pi\left(d\vz\right).
\end{align}

For the first term,~\citet[Theorem 3]{tan_monte_2006} have shown that
\begin{align}
  \int \int
  \Delta\left(\vz\right) \,
  \Delta\left(\vz_k\right) \,
  T_k\left(w\left(\vz\right) \vee w\left(\vz_k\right) \right) \, \pi\left(d\vz_k\right) \, \pi\left(d\vz\right)
  \geq
  0.
\end{align}
Also, they show that \(\lambda\left(v\right) \geq 1 - \frac{1}{v}\).
From this, we show that
\begin{align}
  \frac{2}{N} \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right) \gamma_k
  &\geq
  \frac{2}{N} \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right)
  \int
  \Delta^2\left(\vz\right) \, \lambda^k\left(w\left(\vz\right)\right) \, \pi\left(d\vz\right)
  \\
  &=
  \frac{2}{N}
  \int
  \Delta^2\left(\vz\right) \,
   \sum_{k=1}^{N-1} 
  \left(1 - \frac{k}{N}\right) \, \lambda^k\left(w\left(\vz\right)\right)
  \, \pi\left(d\vz\right)\label{eq:autocov_bound}
\end{align}

Now, by denoting \(r = \lambda^k\left(w\left(\vz\right)\right)\),
\begin{align}
 \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right) \, r^k
 &=
 \sum_{k=1}^{N-1} \, r^k
 -
 \frac{1}{N}\,\sum_{k=1}^{N-1} \,k\, r^k
 \\
 &=
 \frac{r \, \left( 1 - r^{N-1} \right) }{1 - r} 
 -
 \frac{r}{N}
 \,
 \frac{1 - N\,r^{N-1} + \left(N-1\right)\,r^{N} }{{\left(1 - r\right)}^2} 
 \\
 &=
 \frac{
   r\,\left(r^N - N\,r + N - 1\right)
 }{
   N\,{\left(1-r\right)}^2
 }
 \\
 &=
 \frac{
   r\,\left( N \left(1-r\right) - \left(1-r^N\right) \right)
 }{
   N\,{\left(1-r\right)}^2
 }
 \\
 &=
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{\left(1-r^N\right)}{{\left(1-r\right)}^2},
\end{align}
which is monotonically increasing.

By the formula of geometric sums,
\begin{align}
  -\frac{\left(1-r^N\right)}{{\left(1-r\right)}}
  &=
  -\frac{\left(1-r\right)\,\left(r^{N-1} + r^{N-2} + \ldots + r + 1\right)}{{\left(1-r\right)}}
  \\
  &=
  -\left(r^{N-1} + r^{N-2} + \ldots + r + 1\right)
  \\
  &\geq
  -\left(r + r + \ldots + r + 1\right)
  \\
  &=
  -\left((N-1)\,r + 1\right)
\end{align}
where we have used the fact that \(0 \leq r \leq 1\).
Now, 
\begin{align}
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{\left(1-r^N\right)}{{\left(1-r\right)}^2}
 &\geq 
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{(N-1)\,r + 1}{\left(1-r\right)}
 \\
 &=
 \frac{r}{1-r} -
 \frac{r}{N}\,\frac{N\,r + 1 - r}{\left(1-r\right)}
 \\
 &=
 \frac{r}{1-r}
 -
 \frac{r^2}{1-r}
 -
 \frac{r}{N}
 \\
 &=
 r\,\left(1 - \frac{1}{N}\right).
\end{align}

Recall that \(r = \lambda\left(w\left(\vz\right)\right)\) and \(\max\left(1 - \frac{1}{w\left(\vz\right)}, 0\right) \leq \lambda\left(w\left(\vz\right)\right) \leq 1 - \frac{1}{w^*}\).
Therefore,
\begin{align}
 \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right) \, r^k
 &\geq 
 \frac{\lambda\left(w\left(\vz\right)\right)}{1-\lambda\left(w\left(\vz\right)\right)} -
 \frac{\lambda\left(w\left(\vz\right)\right)}{N}\,\frac{\left(1-\lambda^N\left(w\left(\vz\right)\right)\right)}{{\left(1-\lambda\left(w\left(\vz\right)\right)\right)}^2} \label{eq:tight_cov_bound}
 \\
 &=
 \lambda\left(w\left(\vz\right)\right)\,\left(1 - \frac{1}{N}\right)
 \\
 &\geq
 \max\left(1 - \frac{1}{w\left(\vz\right)}, 0\right)\,\left(1 - \frac{1}{N}\right).
\end{align}
This bound is exact when \(N=2\), and is tight when \(N\) and \(\lambda\left(\vz\right)\) are small.
When the rejection rate \(\lambda\left(\vz\right)\) is large, the actual performance will be significantly worse than predicted by the bound.

Now, applying this to~\cref{eq:autocov_bound},
\begin{align}
\frac{2}{N}
  \int
  \Delta^2\left(\vz\right) \,
   \sum_{k=1}^{N-1} 
  \left(1 - \frac{k}{N}\right) \, \lambda^k\left(w\left(\vz\right)\right)
  \, \pi\left(d\vz\right)
  &\geq
  \int
  \Delta^2\left(\vz\right)
  \,
  \max\left(1 - \frac{1}{w\left(\vz\right)}, 0\right)\,\left(1 - \frac{1}{N}\right)
  \,
  \pi\left(d\vz\right)
  \\
  &=
  \int
  \Delta^2\left(\vz\right)
  \,
  \max\left(1 - \frac{q\left(\vz\right)}{\pi\left(\vz\right)}, 0\right)\,\left(1 - \frac{1}{N}\right)
  \,
  \pi\left(d\vz\right)
  \\
  &=
  \left(1 - \frac{1}{N}\right)
  \int
  \Delta^2\left(\vz\right)
  \,
  \max\left(\pi\left(\vz\right) - q\left(\vz\right), 0\right)\,
  d\vz
  \\
  &=
  \abs{
    \left(1 - \frac{1}{N}\right)
    \int
    \Delta^2\left(\vz\right)
    \,
    \max\left(\pi\left(\vz\right) - q\left(\vz\right), 0\right)\,
    d\vz
  }
\end{align}

%%   \\
%%   &=
%%   \Esub{\pi}{\Delta^2\left(\vz\right)}
%%   \,
%%   \Esub{\pi}{w\left(\vz\right)}
%%   +
%%   \Cov{\Delta^2\left(\vz\right), w\left(\vz\right)}
%%   \\
%%   &=
%%   \sigma^2 \, \DChi{\pi}{q} + \sigma^2
%%   +
%%   \Cov{\Delta^2\left(\vz\right), w\left(\vz\right)}
%%   \\
%%   &\geq
%%   \sigma^2 \, \DChi{\pi}{q} + \sigma^2
%%   -
%%   \sqrt{
%%     \Vsub{\pi}{\Delta^2\left(\vz\right)}
%%     \Vsub{\pi}{w\left(\vz\right)}
%%   }
%%   &\text{(Cauchy-Schwarz)}
%%   \\
%%   &=
%%   \sigma^2 \, \DChi{\pi}{q} + \sigma^2
%%   -
%%   \sqrt{
%%     \Vsub{\pi}{\Delta^2\left(\vz\right)}
%%     \left(\DChi{\pi}{q} - 1\right)
%%   }
%% \end{align}
%% Furthermore, since the \(\chi^2\) divergence bounds the KL divergence as \(\frac{1}{\log e}\DKL{\pi}{q} \leq \DChi{\pi}{q}\),
%% \begin{align}
%%   &\sigma^2 \, \DChi{\pi}{q} + \sigma^2
%%   -
%%   \sqrt{
%%     \Vsub{\pi}{\Delta^2\left(\vz\right)}
%%   }
%%   \sqrt{
%%     \left(\DChi{\pi}{q} - 1\right)
%%   }
%%   \\
%%   &=
%%   \sigma^2 \, \DChi{\pi}{q} + \sigma^2
%%   -
%%   \frac{1}{\log e}
%%   \sqrt{
%%     M_4 - \sigma^4
%%   }
%%   \sqrt{
%%     \DKL{\pi}{q} - \log e
%%   }
%%   \\
%%   &=
%%   \sigma^2 \, \frac{1}{\log e} \, \DKL{\pi}{q} + \sigma^2
%%   -
%%   \frac{1}{\log e}
%%   \,
%%   \sqrt{
%%     M_4 - \sigma^4
%%   }
%%   \sqrt{
%%     \DKL{\pi}{q} - \log e
%%   }
%%   \\
%%   &\geq
%%   \sigma^2 \, \frac{1}{\log e} \, \DKL{\pi}{q} + \sigma^2
%%   -
%%   \frac{1}{\log e}
%%   \,
%%   \sqrt{
%%     M_4 - \sigma^4
%%   }
%%   \sqrt{
%%     \DKL{\pi}{q}
%%   }
%% \end{align}
%% where \(M_4\) is the fourth central moment.

%% Combining the results, we obtain the lower bound on the variance
%% \begin{align}
%%   \V{g_{\text{seq.}}}
%%   &\geq
%%   \frac{\sigma^2}{N}
%%   + \frac{2}{N} \sum_{k=1}^{N-1} \left(1 - \frac{k}{N}\right) \gamma_k
%%   \\
%%   &\geq
%%   \frac{2}{N}
%%   \int
%%   \Delta^2\left(\vz\right) \,
%%   w\left(\vz\right) \,
%%   \pi\left(d\vz\right)
%%   -
%%   \frac{\sigma^2}{N}
%%   \\
%%   &\geq
%%   \frac{\sigma^2}{N}
%%   \left(
%%   1
%%   +
%%   \frac{2}{\log e}
%%   \,
%%   \DKL{\pi}{q}
%%   -
%%   \,
%%   \sqrt{
%%     M_4 - \sigma^4
%%   }
%%   \,
%%   \sqrt{
%%     \DKL{\pi}{q}
%%   }
%%   \right)
%% \end{align}

The bound in \cref{eq:tight_cov_bound} converges to \(\frac{\lambda\left(w\left(\vz\right)\right)}{1-\lambda\left(w\left(\vz\right)\right)}\) as \(N \rightarrow \infty\) given that \(\lambda\left(w\left(\vz\right)\right) < 1\).
This shows the relationship with the results of \citet[Theorem 3]{tan_monte_2006}.


\end{proofEnd}

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
