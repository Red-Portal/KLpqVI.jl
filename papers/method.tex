
\section{Inclusive Variational Inference with Independent Metropolis-Hastings}

\subsection{Markov-Chain Monte Carlo in Markovian Score Climbing}\label{section:msc_mcmc}
\paragraph{MCMC uses marginal estimates.}
To explain the mysterious phenomenon shown in~\cref{section:msc}, we first discuss a subtle difference between the usual MCMC setting and the MSC setting.
In MCMC, we are interested in the estimate provided by the average of \textit{all} the states.
Thus, the marginal estimate of \(f\) is given as
\begin{align}
  \frac{1}{T} \sum^{T}_{t=1} f(\vz_t) = \E{f(\vz_t)} = \E{\Esub{K(\vz_{t-1},\,\vz)}{ f(\vz) \mid \vz_{t-1}}}
\end{align}
where \(\Esub{K(\vz_{t-1},\,\vz)}{ f(\vz) \mid \vz_{t-1}}\) is the \textit{conditional estimate} of \(K\) given the previous state \(\vz_{t-1}\).

\paragraph{MSC uses conditional estimates.}
On the other hand, in MSC, the stochastic gradient is estimated using a \textit{single} state of the Markov-chain \(\vz_t \sim K(\vz_{t-1}, \cdot)\).
This means that we are estimating the gradients using the conditional estimate such as
\begin{align}
  \nabla_{\lambda} \DKL{q_{\lambda}}{p} \approx \Esub{K(\vz_{t-1},\vz_t)}{s\,(\vz_t;\, \vlambda) \mid \vz_{t-1}}.
\end{align}
This seemingly subtle difference of using either the marginal or conditional estimate reveals many interesting facts about MSC.
\begin{enumerate*}[label=(\roman*)]
\item The usual central limit theorem (CLT) guarantees of MCMC do not apply.
\item Also, ergodicity guarantees work differently since, at each iteration, \(q_{\vlambda}\) is updated.
\end{enumerate*}
The most important difference, however, is the way we should analyze the variance of the estimates.

%% \begin{wrapfigure}{r}{0.35\textwidth}
%%   \vspace{-0.2in}
%%   \begin{algorithm2e}[H]
%%     \DontPrintSemicolon
%%     \SetAlgoLined
%%     \KwIn{Proposal \(q_{\vlambda}\),
%%     Previous state \(\vz_{t-1}\)}
%%     \(\vz^* \sim q_{\vlambda}(\vz)\)\\
%%     \eIf{\(\vz^*\) is accepted}{
%%       \textit{(Accept)} \(\vz_t = \vz^*\)
%%     }
%%     {
%%       \textit{(Reject)} \(\vz_t = \vz_{t-1}\)
%%     }
%%     \caption{Generic IMH Kernel}\label{alg:imh}
%%   \end{algorithm2e}
%% \end{wrapfigure}
%
\subsection{Conditional Importance Sampling as Independent Metropolis-Hastings}\label{section:cis_imh}
Independent Metropolis-Hastings (IMH) kernels are kernels where proposals are generated independently of the previous state \(\vz_{t-1}\).
We first show that the CIS kernel proposed in~\citep{NEURIPS2020_b2070693} is a type of IMH kernel that uses Barker's accept-reject mechanism~\citep{barker_monte_1965}.
This IMH view will reveal several properties of the CIS kernel and IMH kernels in general.

\begin{figure}
  \small
  \begin{algorithm2e}[H]
    \DontPrintSemicolon
    \SetAlgoLined
    \KwIn{previous sample \(\vz_{t-1}\),
      previous parameter \(\vlambda_{t-1}\),
      number of proposals \(N\)
    }
    \(\vz^{(0)} = \vz_{t-1}\) \;
    \textit{Propose} \(\;\vz^{(i)} \sim q_{\vlambda_{t-1}}(\vz)\) for \(i = 1, 2,\ldots, N\) \;
    \textit{Weight} \(\;w(\vz^{(i)}) = p(\vz^{(i)},\vx) \,/\, q_{\vlambda_{t-1}}(\vz^{(i)}) \) for \(i = 0, 1,\ldots, N\)\;
    \textit{Normalize} \(\;\widetilde{w}^{(i)} = \nicefrac{w(\vz^{(i)})}{ \sum^{N}_{i=0} w(\vz^{(i)}) }\) for \(i = 0, 1,\ldots, N\)\;
    \(\vz_{t} \sim \mathrm{Multinomial}(\widetilde{w}^{(0)}, \widetilde{w}^{(1)}, \ldots, \widetilde{w}^{(N)}) \)\;
    \caption{Conditional Importance Sampling Kernel}\label{alg:cis}
  \end{algorithm2e}
  \vspace{-0.15in}
\end{figure}
%
\vspace{-0.1in}
\paragraph{Conditional Importance Sampling}
A pseudocode of the CIS kernel is shown in~\cref{alg:cis}.
The original algorithmic description of CIS is to
\begin{enumerate*}[label=(\roman*)]
  \item sample \(N\) samples from \(q_{\vlambda}(\vz)\),
  \item compute the importance weight including the previous Markov-chain state \(\vz_{t-1}\),
  \item and resample \(\vz_{t-1}\) from the multinomial distribution of the \(N+1\) proposals.
\end{enumerate*}
%
While particle MCMC originally inspired the CIS kernel~\citep{andrieu_particle_2010}, it is possible to find connections in multiple-try MCMC methods~\citep{martino_review_2018}.
In particular, the CIS kernel is identical to the previously proposed \textit{ensemble MCMC sampler}~\citep{neal_mcmc_2011a, austad_parallel_2007} with independent proposals~\citep[Table 12]{martino_review_2018}.

\vspace{-0.1in}
\paragraph{Conditional Importance Sampling as Metropolis Hastings}
We now show that the CIS kernel is an accept-reject type kernel with Barker's acceptance ratio.
First, by defining \(\vz_t = \vz_{t-1}\) as ``reject'' and \(\vz_t \neq \vz_{t-1}\) as ``accept'', it is easy to notice that the CIS kernel can be understood as an accept-reject type kernel.
By denoting the \(N\) parallel proposals as an \textit{ensemble state} \(\vz^{(1:N)} = (\vz^{(1)}, \ldots, \vz^{(N)})\)~\citep{neal_mcmc_2011a}, the CIS kernel conditional estimate can be written as
\begin{align}
  &\Esub{K(\vz_{t-1}, \cdot)}{f(\vz)}  = \nonumber \\  
   &\int \alpha\,(\vz_{t-1}, \vz^{(1:N)}) \,
    \frac{
      \sum^{N}_{i=1} w\,(\vz^{(i)})\,f(\vz^{(i)})
    }
    {
      \sum^{N}_{i=1} w\,(\vz^{(i)})
    } \,
    q_{\vlambda}(\vz^{(1:N)})\,d\vz^{(1:N)}
    + r\,(\vz_{t-1}) \, f(\vz_{t-1})\label{eq:cis_kernel}
\end{align}
where \(q_{\vlambda}(\vz^{(1:N)}) = \prod^N_{i=1} q_{\vlambda}(\vz^{(i)}) \),
\(
  \alpha(\vz_{t-1}, \vz^{(1:N)})
  = \nicefrac{\sum^{N}_{i=1} w\,(\vz^{(i)})}{\sum^{N}_{i=0} w\,(\vz^{(i)})}
\)
is the probability of accepting the ensemble state \(\vz^{(1:N)}\), and
{\small
  \begin{align}
    r\,(\vz_{t-1}) &= \int r\,(\vz_{t-1} \mid \vz^{(1:N)})\,q_{\vlambda}(\vz^{(1:N)}) \, d\vz^{(1:N)}
    = \int \left(1 - \alpha\,(\vz_{t-1}, \vz^{(1:N)})\right) q_{\vlambda}(\vz^{(1:N)}) \, d\vz^{(1:N)}
  \end{align}
}%
is the probability of rejecting \textit{any} ensemble state and staying on \(\vz_{t-1}\).
\(r\,(\vz_{t-1}\mid \vz^{(1:N)})\) is the rejection rate given \(\vz^{(1:N)}\).
The acceptance ratio \(\alpha(\vz_{t-1}, \vz^{(1:N)})\) is known as~\citeauthor{barker_monte_1965}'s acceptance ratio~\citep{barker_monte_1965} and is a special case of the original~\cite{metropolis_equation_1953} acceptance ratio.

%% Now, the transition kernel can be denoted as
%% \begin{align}
%%   K(\vz_{t-1}, \vz) = \int K(\vz^{(1:N)}, \vz) \, \big( 1 - r\,(\vz_{t-1}\mid\vz^{(1:N)}) \big) \, q_{\vlambda}( \vz^{(1:N)} ) \, d\vz^{(1:N)}
%%   + r\,(\vz_{t-1}) \,\delta_{\vz_{t-1}}(\vz)
%% \end{align}

\subsection{Bias-Variance Tradeoff of Conditional Importance Sampling}\label{section:bias_variance}
\paragraph{Variance of Conditional Importance Sampling}
The IMH (or accept-reject) view in~\cref{section:cis_imh} now enables us to discuss the rejection rate of the CIS kernel.
As discussed in~\cref{section:msc_mcmc}, MSC obtains gradients using the conditional estimates of MCMC.
The variance of the conditional estimate is closely related to the rejection rate such that
\begin{align}
  \Vsub{q_{\vlambda}}{f \mid \vz_{t-1}} &= \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1},\,\vz^{(1:N)} } } + \underbrace{\Esub{q_{\vlambda}}{ \V{ f \mid \vz_{t-1},\,\vz^{(1:N)} } }}_{\text{Rao-Blackwellization gain}} \label{eq:total_variance} \\
  &\geq \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1}\,\vz^{(1:N)} } } \label{eq:rao_blackwell}\\
  &= \Vsub{q_{\vlambda}}{ \big(1 - r\,(\vz_{t-1}\mid\vz^{(1:N)})\big)\, f_{\mathrm{IS}}
    + r\,(\vz_{t-1}\mid\vz^{(1:N)})\,f(\vz_{t-1}) \,\middle\vert\, \vz_{t-1},\, \vz^{(1:N)} } \label{eq:exact_variance} \\
  &\text{where}\;\; f_{\mathrm{IS}} = {\sum_{i=1}^N w\,(\vz^{(i)}) f(\vz^{(i)})\,/\,\sum_{i=1}^N w\,(\vz^{(i)})}. \nonumber 
\end{align}
%
The bound in~\eqref{eq:rao_blackwell} is exact if we use Rao-Blackwellization (use the SNIS estimator formed by \(\widetilde{w}^{(0)}, \ldots, \widetilde{w}^{(N)}\}\) instead of multinomial resampling) as mentioned by~\citet{NEURIPS2020_b2070693}.
The expansion in~\eqref{eq:total_variance} follows from the Law of Total Variance.
The right-hand term is the variance reduction we gain from using Rao-Blackwellization~\citep{bernton_locally_2015}.

\vspace{-0.1in}
\paragraph{Low rejection rate means high conditional variance.}
Because of the dependence of \(r\,(\vz_{t-1}\mid\vz^{(1:N)})\) on \(\vz^{(1:N)}\), it is in general difficult to interpret the result of~\eqref{eq:exact_variance}.
Nonetheless, when \(w\,(\vz_{t-1}) \gg w\,(\vz^{(i)})\), \(r(\vz\mid\vz^{(1:N)})\) will be almost independent of \(q_{\vlambda}\) and close to 1.
Then, we perform the following approximation.
%
\input{variance_approx}
%
The statement of~\cref{thm:approx_var} is intuitive; if we reject all the states, there is obviously no conditional variance.
However, this obvious fact becomes more interesting combined with the results that will follow.

\vspace{-0.1in}
\paragraph{CIS has a high rejection rate until MSC converges.}
The following bound provides a condition for the rejection rate of a CIS kernel to be high.
%
\input{cis_bound}
%
The normalizing constant \(Z\) is, in general, tiny.
When \(w\,(\vz_{t-1})\) is large, the bound states that \(r(\vz_{t-1})\) will be close to one.
In this case, according to~\cref{thm:approx_var}, the conditional variance will be minimal.

The interesting fact emerges by the fact that \(w\,(\vz_{t-1})\) can be huge when the KL divergence is large.
%
\input{cis_bound_kl}
%
%% Since
%% \begin{align}
%%   \DKL{p}{q_{\vlambda}} = \int p\,(\vz\mid\vx) \log \frac{p\,(\vz\mid\vx)}{q_{\vlambda}(\vz)} \, d\vz
%%   \leq \log \Esub{p(\vz\mid\vx)}{\frac{p\,(\vz_t\mid\vx)}{q_{\vlambda}(\vz_t)}}
%% \end{align}
%% by Jensen's equality, 
%% \begin{align}
%%   \frac{1}{Z}\,\exp\big(\DKL{p}{q_{\vlambda}}\big) \leq \Esub{p(\vz_t\mid\vx)}{w\,(\vz_{t})}.\label{eq:kl_rej}
%% \end{align}
This result states that, in the ideal case that the Markov-chain has achieved stationarity and \(\vz_{t-1}\) closely follows \(p\,(\vz\mid\vx)\), the average rejection weight is bounded below exponentially by the KL divergence.
Thus, until MSC converges such that \(\DKL{p}{q_{\vlambda}}\) is small, the rejection rate will be very high.

This interesting interaction between the inclusive KL and the rejection rate cannot, in general, be mitigated by increasing \(N\) since
\begin{enumerate*}[label=(\roman*)]
  \item the iteration complexity also increases,
  \item and \(w(\vz_t)\) can easily be larger by many orders of magnitude.
\end{enumerate*}
\cref{thm:cis_bound_kl} explains why the Markov-chain in Figure~\ref{fig:motivating} does not move until MSC converges, and combined with~\cref{thm:approx_var}, explains why MSC works well with the CIS kernel.
These properties hold similarly in any IMH type kernel.

\vspace{-0.1in}
\paragraph{Is bias guaranteed to decrease?}
The bias of the conditional estimate is closely related to the total variation (TV) distance \(\DTV{K(\vz_{t-1}, \cdot)}{p(\cdot\mid\vx)}\).
For bounded functions, the TV distance provides an upper bound of the bias.
Unfortunately, it is in general difficult to talk about the TV distance (and hence the bias) with respect to \(p\) and \(q\).
Although,~\citet{wang_exact_2020} recently showed that the rejection rate is related with the TV distance such that \(r(\vz_{t-1}) \leq \DTV{K(\vz_{t-1}, \cdot)}{p(\cdot\mid\vx)}\).
Therefore, a low rejection rate is a necessary condition for the bias to decrease.

\vspace{-0.1in}
\paragraph{IMH type kernels automatic trade-off bias for variance.}
To summarize, IMH type kernels (including the CIS kernel) have an automatic bias-variance trade-off mechanism.
In the initial steps where the inclusive KL divergence is large, variance is suppressed by rejecting most of the proposals.
However, as MSC converges, the bound on the rejection rate becomes loose, admitting a lower rejection rate, which enables bias to decrease.
This mechanism provides an interesting case where rejections in MCMC can actually be beneficial.
Lastly, we note that the automatic trade-off mechanism is a unique property of IMH type kernels.
It does not exist in kernels with state-dependent proposals such as random-walk Metropolis-Hastings or HMC.

%% that the KL divergence is related to the maximum importance ratio \(w^* = \sup_{\vz} p(\vz\mid\vx) / q_{\vlambda}(\vz)\) such that
%% \begin{align}
%%   \DKL{p}{q_{\vlambda}} = \int p(\vz\mid\vx) \log \frac{p(\vz\mid\vx)}{q_{\vlambda}(\vz)} d\vz < \int p(\vz\mid\vx)  \log w^* d\vz = \log w^*.
%% \end{align}
%% \(\DKL{p}{q_{\vlambda}} < \infty\) is thus a necessary condition for \(w < \infty\) which is required for \(K\) to be geometrically ergodic~\citep{wang_exact_2020}.
%% Also, 

%% the TV distance is bounded 
%% This implies 
%% This is not only 
%As shown by~\citet{10.1214/17-STS611, chatterjee_sample_2018}, the number of particles for acheiving bounded error reduces exponentially with the KL divergence.

\begin{figure}
  \centering
\begin{minipage}[t]{0.45\textwidth}
  \small
  \begin{algorithm2e}[H]
    \DontPrintSemicolon
    \SetAlgoLined
    \KwIn{initial samples \(\vz_0^{(1)},\, \ldots,\, \vz_0^{(N)}\),
      initial parameter \(\vlambda_0\),
      number of iterations \(T\),
      stepsize schedule \(\gamma_t\)
    }
    \For{\textcolor{black}{\(t = 1, 2, \ldots, T\)}}{
      \textcolor{blue}{
        \For{\textcolor{blue}{\(i = 1, 2, \ldots, N\)}}{
          \(\vz^{(i)}_{t} \sim K(\vz^{(i)}_{t-1}, \cdot)\)
        }
      }
      \( s(\vz^{(i)}_t; \vlambda) = \nabla_{\vlambda} \log q_{\vlambda}(\vz_t^{(i)}) \)\;
      \( \vlambda_{t} = \vlambda_{t-1} + \gamma_t\,
      \textcolor{blue}{\frac{1}{N} \sum^{N}_{i=1} s\,(\vz_t^{(i)};\vlambda_{t-1})} \)\;
    }
    \caption{Markovian Score Climbing with Parallel Chains}\label{alg:pimh}
  \end{algorithm2e}
\end{minipage}
\hspace{0.2in}
\begin{minipage}[t]{0.39\textwidth}
  \small
  \begin{algorithm2e}[H]
    \DontPrintSemicolon
    \SetAlgoLined
    \KwIn{previous sample \(\vz_{t-1}\),
      previous parameter \(\vlambda_{t-1}\),
    }
    \(\vz^* \sim q_{\vlambda_{t-1}}(\vz)\)\;
    \(w(\vz) = p(\vz,\vx)/q_{\vlambda_{t-1}}(\vz) \)\;
    \(\alpha = \min\left( w\,(\vz^*)/w\,(\vz_{t-1}), 1\right)\)\;
    \(u \sim \mathrm{Uniform}(0, 1) \)\;
    \eIf{u < \(\alpha\)}
        {
          \(\vz_t \leftarrow \vz^*\)
        }
        {
          \(\vz_t \leftarrow \vz_{t-1}\)
        }
        \caption{\\ Independent Metropolis-Hastings}\label{alg:imh}
  \end{algorithm2e}
\end{minipage}
\vspace{-0.15in}
\end{figure}
%
\subsection{Reducing Variance with Parallel Independent Metropolis-Hastings Markov-Chains}\label{section:cis_bias}
Recall that the CIS kernel uses \(N\) multiple-try type proposals.
Thus, it would be natural to expect the variance to decrease as \(N\), the computational budget, increases.
On the contrary, under specific conditions, the variance actually \textit{increases} with \(N\).

\vspace{-0.1in}
\paragraph{The variance of CIS can increase with \(N\).}
The bound in~\cref{thm:cis_bound}, can be reinterpreted as a bound on the acceptance rate 
\begin{align}
  \alpha = 1 - r\,(\vz_{t-1}) \leq \frac{N\,Z}{ w\,(\vz_{t-1}) + N\,Z},
\end{align}
which in general is very tight.
More importantly, when \(w\,(\vz_{t-1}) \gg N\,Z\), the acceptance rate grows like \(\mathcal{O}(N)\).
On the other hand, the variance of a SNIS estimator is known to approximately decrease at a rate of \(\mathcal{O}(1/N)\)~\citep{kong_sequential_1994, robert_monte_2004, elvira_rethinking_2018}.
That is, 
\begin{align}
  \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1}\,\vz^{(1:N)} } } \approx \underbrace{{\big(1 - r\,(\vz_{t-1})\big)}^2}_{\text{approx.}\;\;\mathcal{O}(N^2)} \,
  \underbrace{\Vsub{q_{\vlambda}}{ f_{\text{IS}} }}_{\text{approx.}\;\;\mathcal{O}(1/N)}.\label{eq:cis_variance_incr}
\end{align}
Thus, when \(w\,(\vz_{t-1})\gg N\,Z\), the conditional variance of CIS approximately grows linearly with \(N\).

\vspace{-0.1in}
\paragraph{Bias and Variance Reduction with Paralel IMH Chains}
To resolve the aforementioned limitation of the CIS kernel, we propose a simple remedy: running \(N\) parallel IMH (PIMH) Markov-chains (\(\vz_t^{(1)}, \vz_t^{(2)}, \ldots, \vz_t^{(N)}\)).
Each of the chains performs a Metropolis-Hastings test with only a single proposal each.
The modified MSC algorithm incorporating parallel chains is shown in~\cref{alg:pimh}, (the modified parts are highlighted in \textcolor{blue}{blue}), while the pseudocode of the IMH kernel is shown in~\cref{alg:imh}.
Since the parallel chains generate an \textit{independent} conditional estimate each, the gradient estimate \(\nicefrac{1}{N} \sum^{N}_{i=1} s\,(\vz_t^{(i)}; \vlambda)\) is an average of \(N\) independent and identical estimators.
This trivially reduces the variance of a single conditional estimate in a rate of \(\mathcal{O}(1/N)\).
%Also, the estimates are now proper marginal esimates, which enjoy a tighter bound on the TV distance (and hence the bias).
%
%\input{bias_reduction}

\vspace{-0.1in}
\paragraph{Lower bound of rejection rate in IMH}
IMH also enjoys variance control properties similar to the CIS kernel.
That is, a lower bound similar to~\cref{thm:cis_bound} can be shown.
%
\input{imh_bound}
%
In this case, we can see that the bound does not depend on \(N\), unlike CIS.
This means the rejection rate will \textit{not} prematurely increase.
We thus obtain all the benefits of CIS except its limitations.
Using~\cref{thm:imh_bound}, a similar result to~\cref{thm:cis_bound_kl} can be obtained.

\vspace{-0.1in}
\paragraph{Computational Cost of PIMH}
The sampling cost of PIMH with \(N\) chains is exactly equal to a CIS kernel with \(N\) proposals.
However, we now need \(N\) evaluations of the score function.
This might seem expensive compared to the original MSC algorithm, which only needs 1 evaluation of \(s\) but is exactly equal to MSC with Rao-Blackwellization.
%% Meanwhile, unlike PIMH, using more expensive MCMC kernels such as HMC require many likelihood evaluations for generating \textit{a single sample}.
%% It is therefore likely that the cost of more advanced MCMC kernels (or even an independent sample from \(p(\vz\mid\vx)\)!) might not be worth their benefits.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
