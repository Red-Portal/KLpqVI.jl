
\section{Inclusive Variational Inference with Independent Metropolis-Hastings}

\subsection{Markov-Chain Monte Carlo in Markovian Score Climbing}\label{section:msc_mcmc}
\paragraph{MCMC uses marginal esimates.}
Before proceeding, we first discuss a subtle difference between the usual MCMC setting and the MSC setting.
In MCMC, we are interested in the estimate provided by the average of \textit{all} the states.
Thus, the marginal estimator of \(f\) is given as
\begin{align}
  \frac{1}{T} \sum^{T}_{t=1} f(\vz_t) = \E{f(\vz_t)} = \E{\Esub{\vz_t \sim K(\vz_{t-1},\cdot)}{ f(\vz_t) \mid \vz_{t-1}}}
\end{align}
where \(\Esub{K(\vz_{t-1},\cdot)}{ f \mid \vz_{t-1}}\) is the \textit{conditional estimate} of \(K\) given the previous state \(\vz_{t-1}\).

\paragraph{MSC uses conditional estimates.}
On the other hand, in standard MSC, the stochastic gradient is estimated only using a single state of the Markov-chain \(\vz_t \sim K(\vz_{t-1}, \cdot)\).
This means that we are estimating the gradients using the conditional estimate such as
\begin{align}
  \nabla_{\lambda} \DKL{q_{\lambda}}{p} \approx \Esub{\vz_t \sim K(\vz_{t-1},\cdot)}{s\,(\vz_t;\, \vlambda) \mid \vz_{t-1}}.
\end{align}
This seemingly subtle difference reveals many interesting facts about MSC.
\begin{enumerate*}[label=(\roman*)]
\item The usual central limit theorem (CLT) guarentees of MCMC do not apply.
\item Also, ergodicity guarentees work differently since, at each iteration, \(q_{\vlambda}\) is updated.
\end{enumerate*}
The largest difference however, is the way we should analyze the variance of the estimates.

%% \begin{wrapfigure}{r}{0.35\textwidth}
%%   \vspace{-0.2in}
%%   \begin{algorithm2e}[H]
%%     \DontPrintSemicolon
%%     \SetAlgoLined
%%     \KwIn{Proposal \(q_{\vlambda}\),
%%     Previous state \(\vz_{t-1}\)}
%%     \(\vz^* \sim q_{\vlambda}(\vz)\)\\
%%     \eIf{\(\vz^*\) is accepted}{
%%       \textit{(Accept)} \(\vz_t = \vz^*\)
%%     }
%%     {
%%       \textit{(Reject)} \(\vz_t = \vz_{t-1}\)
%%     }
%%     \caption{Generic IMH Kernel}\label{alg:imh}
%%   \end{algorithm2e}
%% \end{wrapfigure}
%
\subsection{Conditional Importance Sampling as Independent Metropolis Hastings}\label{section:cis_imh}
We now show that the CIS kernel proposed in~\citep{NEURIPS2020_b2070693} is an independent Metropolis-Hastings (IMH) kernel.
The IMH view will reveal many interesting properties of the CIS kernel and IMH kernels in general.

\paragraph{Independence Metropolis Hastings}
In IMH, a proposal \(\vz^*\) (or multiple proposals) is proposed from \(q_{\vlambda}\), which is independent of \(\vz_{t-1}\).
Then, this proposal is either accepted (\(\vz_{t} = \vz^*\)) or rejected (\(\vz_{t} = \vz_{t-1}\)).
Multiple consecutive rejections means the sampler gets stuck, which often results in bad statistical performance.
However, we will later show that this conventional knowledge does not hold in MSC.

\paragraph{Conditional Importance Sampling}
The CIS kernel is identical to the previously proposed \textit{ensemble MCMC sampler}~\citep{neal_mcmc_2011a, austad_parallel_2007} with independent proposals, which is a type of multiple-try MCMC~\citep{martino_review_2018}.
First, by defining \(\vz_t = \vz_{t-1}\) as ``reject'' and \(\vz_t \neq \vz_{t-1}\) as ``accept'', it is easy to notice that the CIS kernel can be understood as an accept-reject type kernel.
By denoting the \(N\) parallel proposals as an \textit{ensemble state} \(\vz^{(1:N)} = (\vz^{(1)}, \ldots, \vz^{(N)})\), the acceptance rate of CIS can be written as
\begin{align}
  \alpha(\vz_{t-1}, \vz^{(1:N)})
  &= \frac{\sum^{N}_{i=1} w\,(\vz^{(i)})}{w\,(\vz_{t-1}) + \sum^{N}_{i=1} w\,(\vz^{(i)})} \nonumber \\
  &= 1 - \frac{w\,(\vz_{t-1})}{w\,(\vz_{t-1}) + \sum^{N}_{i=1} w\,(\vz^{(i)})}
  = 1 - r\,(\vz_{t-1}\mid\vz^{(1:N)})\label{eq:acc}
\end{align}
where \(w(\vz) = \nicefrac{p(\vz,\vx)}{q_{\vlambda}(\vz)}\) is the unnormalized importance weight and \(r\,(\vz_{t-1}\mid \vz^{(1:N)})\) is the rejection rate given \(\vz^{(1:N)}\).
The the acceptance ratio in Equation~\eqref{eq:acc} is known as~\citeauthor{barker_monte_1965}'s acceptance ratio~\citep{barker_monte_1965}, and is a special case of the original Metropolis~\cite{metropolis_equation_1953} acceptance ratio.
If the ensemble proposal \(\vz^{(1:N)}\) is accepted, a single proposal is resampled proportionally to the normalized weight \(\tilde{w}^{(i)} = \nicefrac{w\,(\vz^{(i)})}{\sum_{i=1}^N w\,(\vz^{(i)}) }\).

%% Now, the transition kernel can be denoted as
%% \begin{align}
%%   K(\vz_{t-1}, \vz) = \int K(\vz^{(1:N)}, \vz) \, \big( 1 - r\,(\vz_{t-1}\mid\vz^{(1:N)}) \big) \, q_{\vlambda}( \vz^{(1:N)} ) \, d\vz^{(1:N)}
%%   + r\,(\vz_{t-1}) \,\delta_{\vz_{t-1}}(\vz)
%% \end{align}

\subsection{Bias-Variance Tradeoff of Conditional Importance Sampling}\label{section:bias_variance}
\paragraph{Variance of Conditional Importance Sampling}
The IMH (or accept-reject) view in Section~\ref{section:cis_imh} now enables us to discuss the rejection rate of the CIS kernel.
As discussed in Section~\ref{section:msc_mcmc}, MSC obtains gradients using the conditional estimates of MCMC.
The variance of the conditional estimate is closely related to the rejection rate such that
\begin{align}
  \Vsub{q_{\vlambda}}{f \mid \vz_{t-1}} &= \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1},\,\vz^{(1:N)} } } + \underbrace{\Esub{q_{\vlambda}}{ \V{ f \mid \vz_{t-1},\,\vz^{(1:N)} } }}_{\text{Rao-Blackwellization gain}} \label{eq:total_variance} \\
  &\geq \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1}\,\vz^{(1:N)} } } \\
  &= \Vsub{q_{\vlambda}}{ \big(1 - r\,(\vz_{t-1}\mid\vz^{(1:N)})\big)\, f_{\mathrm{IS}}
    + r\,(\vz_{t-1}\mid\vz^{(1:N)})\,f(\vz_{t-1}) \,\middle\vert\, \vz_{t-1},\, \vz^{(1:N)} } \label{eq:exact_variance} \\
  &\text{where}\;\; f_{\mathrm{IS}} = {\sum_{i=1}^N w\,(\vz^{(i)}) f(\vz^{(i)})\,/\,\sum_{i=1}^N w\,(\vz^{(i)})} \nonumber 
\end{align}
%
The expansion in Equation~\eqref{eq:total_variance} follows from the Law of Total Variance.
The right-hand term is the Rao-Blackwell variance reduction we gain from using the weighted estimator instead of multinomial resampling. (This was also originally mentioned by~\citet{NEURIPS2020_b2070693}.)

\paragraph{Low rejection rate means high conditional variance (which is bad).}
Because of the dependence of \(r\,(\vz_{t-1}\mid\vz^{(1:N)})\) on \(\vz^{(1:N)}\), it is in general difficult to interpret the result of Equation~\eqref{eq:exact_variance}.
However, since \(r(\vz\mid\vz^{(1:N)}) = \nicefrac{w(\vz_{t-1})}{w(\vz_{t-1}) + \sum_{i=1}^N w\,(\vz^{(i)})}\), when \(w(\vz_{t-1}) \gg w\,(\vz^{(i)})\), \(r(\vz\mid\vz^{(1:N)})\) will be almost independent of \(q_{\vlambda}\).
In this case, we obtain the approximation
\begin{align}
  \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1}, \vz^{(1:N)} } } \approx {\big(1 - r\,(\vz_{t-1})\big)}^2\,\Vsub{q_{\vlambda}}{ f_{\text{IS}} \,\middle\vert\, \vz_{t-1} }.\label{eq:approx_var}
\end{align}
The statement of Equation~\eqref{eq:approx_var} is intuitive; if we reject all the states there is obviously no variance.
However, it becomes more interesting combined with the results that will follow.

\paragraph{CIS has high rejection rate in the beginning.}
We now show that the CIS kernel (and IMH kernels in general) have high rejection rate in the initial steps of MSC.
%
\input{rejection_bound}
%
In the initial iterations of MSC, \(\vlambda\) and hence \(q_{\vlambda}\) change abrubtely because of the large stepsize \(\gamma_k\).
Then, for \(\vz_{t-1} \sim q_{\vlambda_{t-1}}(\vz)\), \(q_{\vlambda_t}(\vz_{t-1})\) can be very small.
Consequently, the importance weight of the rejection state \(w(\vz_t)\) will end up being extremely large.
This effect cannot in general be mitigated by increasing \(N\) since
\begin{enumerate*}[label=(\roman*)]
  \item the iteration complexity also increases,
  \item and \(w(\vz_t)\) can be easily be large by many orders of magnitude.
\end{enumerate*}
As a result, Proposition~\ref{thm:rej_bound} explains why the Markov-chain in Figure~\ref{fig:motivating} does not move until \(\vlambda\) converges.
This property also holds similarly in any IMH type kernel.

\paragraph{Rejection rate decreases as MSC converges.}
The rejection rate itself can be shown to automatically decrease as MSC converges.
In particular,~\citet[Theorem 1]{neklyudov_metropolishastings_2019} show that the marginal acceptance rate \(\alpha = \int \int \alpha(\vz^*,\vz)\,q(\vz^*)\,p(\vz\mid\vx)\,d\vz^*\,d\vz \) of an IMH sampler is related with \(\DKL{p}{q_{\vlambda}}\) such that
\begin{align}
    \alpha = 1 - r \geq 1 - \sqrt{
      \frac{1}{2} \Big(
      \DKL{q_{\vlambda}}{p} + \DKL{p}{q_{\vlambda}}
      \Big)
    }.
\end{align}
By the non-negativity of the KL divergence, the bound on the \textit{marginal rejection rate} \(r=1 - \alpha\) 
\begin{align}
  r
  \leq 
  \sqrt{ \frac{1}{2}
    \DKL{p}{q_{\vlambda}}
  }\label{eq:rej_bound}
\end{align}
directly follows.

\paragraph{IMH kernels automatic trade-off bias for variance.}
Equation~\eqref{eq:rej_bound} guarentees that the rejection rate of IMH automatically decreases as MSC converges.
Thus, as shown in Equation~\eqref{eq:approx_var}, when using an IMH type kernel in MSC, variance is suppressed by rejecting most of the proposals.
While this would come at the cost of having high bias, as we start to converge, the rejection rate start to decrease, allowing bias to be reduced.
This automatic trade-off mechanism is a unique property of IMH type kernels, and is not true for state-dependent proposals such as in random-walk Metropolis-Hastings or HMC.

\paragraph{Is bias guarenteed to decrease?}
The bias of the conditional estimate is closely related to the total variation (TV) distance \(\DTV{K(\vz_{t-1}, \cdot)}{p}\).
For bounded functions, the TV distance provides an upper-bound of the bias.
Unfortunately, it is in general difficult to talk about the TV distance (and hence the bias) with respect to \(p\) and \(q\).
Although,~\citet{wang_exact_2020} recently showed that the rejection rate is related with the TV distance such that \(r(\vz_{t-1}) \leq \DTV{K(\vz_{t-1}, \cdot)}{p(\cdot\mid\vx)}\).
Therefore, a low rejection rate is a necessary condition for the bias to decrease.

%% that the KL divergence is related to the maximum importance ratio \(w^* = \sup_{\vz} p(\vz\mid\vx) / q_{\vlambda}(\vz)\) such that
%% \begin{align}
%%   \DKL{p}{q_{\vlambda}} = \int p(\vz\mid\vx) \log \frac{p(\vz\mid\vx)}{q_{\vlambda}(\vz)} d\vz < \int p(\vz\mid\vx)  \log w^* d\vz = \log w^*.
%% \end{align}
%% \(\DKL{p}{q_{\vlambda}} < \infty\) is thus a necessary condition for \(w < \infty\) which is required for \(K\) to be geometrically ergodic~\citep{wang_exact_2020}.
%% Also, 

%% the TV distance is bounded 
%% This implies 
%% This is not only 
%As shown by~\citet{10.1214/17-STS611, chatterjee_sample_2018}, the number of particles for acheiving bounded error reduces exponentially with the KL divergence.

\subsection{Reducing Variance with Parallel Indpendent Metropolis-Hastings Markov Chains}\label{section:cis_bias}
Recall that the CIS kernel uses \(N\) multiple-try type proposals.
Thus, it would be natural to expect the variance to decrease as \(N\), the computational budget, increases.
Instead, under specific conditions, the variance \textit{increases} with \(N\).

\paragraph{The variance of CIS can increase with \(N\).}
The bound in Proposition~\ref{thm:rej_bound}, can be reinterpreted as a bound on the acceptance rate 
\begin{align}
  1 - r\,(\vz_{t-1}) \leq \frac{N}{ w\,(\vz_{t-1}) + N},
\end{align}
which in general is very tight.
More importantly however, when \(w\,(\vz_{t-1})\) is very large, the acceptance rate grows like \(\mathcal{O}(N)\).
On the other hand, the variance of a SNIS estimator is known to approximately decrease in a rate of \(\mathcal{O}(1/N)\)~\citep{kong_sequential_1994, robert_monte_2004, elvira_rethinking_2018}.
That is, 
\begin{align}
  \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1}\,\vz^{(1:N)} } } \approx \underbrace{{\big(1 - r\,(\vz_{t-1})\big)}^2}_{\text{approx.}\;\;\mathcal{O}(N^2)} \,
  \underbrace{\Vsub{q_{\vlambda}}{ f_{\text{IS}} }}_{\text{approx.}\;\;\mathcal{O}(1/N)}.
\end{align}
Thus, when \(w\,(\vz_{t-1})\) is very large, the conditional variance of CIS approximately grows linearly with \(N\).

\paragraph{Variance Reduction with Paralel IMH Chains}
To resolve the aforementioned limitation of the CIS kernel, we propose a simple remedy: running \(N\) parallel IMH (PIMH) Markov-chains \(\vz_t^{(1)}, \vz_t^{(2)}, \ldots, \vz_t^{(N)}\) using only a single proposal each.
The gradient is then estimated as
\begin{align}
  \Esub{p(\vz\mid\vx)}{ s\,(\vz;\,\vlambda) } \approx \frac{1}{N} \sum_{i=1}^N s\,(\vz_t^{(i)};\,\vlambda).\label{eq:pimh}
\end{align}
By ensuring that the initial states \(\vz_{0}^{(i)}\) are sampled independently, the estimate of Equation~\eqref{eq:pimh} is an average of \(N\) independent estimators.
Thus, the variance linearly decreases in an order of \(\mathcal{O}(1/N)\).

\paragraph{Computational Cost of PIMH}
The sampling cost of PIMH with \(N\) chains is exactly equal to a CIS kernel with \(N\) proposals.
However, we now need \(N\) evaluations of the score function for computing~\eqref{eq:pimh}.
This might seem expensive compared to the original MSC algorithm which only needs 1 evaluation of \(s\).
On the other hand, the cost of PIMH is exactly equal to MSC if we use Rao-Blackwellization.
Notice that, unlike PIMH, using more expensive MCMC kernels such as HMC require many likelihood evaluations for generating \textit{a single sample}.
It is therefore likely that the cost of more advanced MCMC kernels (or even an independent sample from \(p(\vz\mid\vx)\)!) might not be worth their benefits.

\paragraph{Convergence Diagnostics}
A practical consideration in VI recently highlighted by~\citet{NEURIPS2020_7cac11e2} is to diagnose convergence of SGD.
\citeauthor{NEURIPS2020_7cac11e2} proposed to compuate the \(\widehat{R}\) diagnostics popularly used in MCMC~\citep{gelman_inference_1992, vehtari_ranknormalization_2020} from the sequence of the variational parameters \(\vlambda_t\).
Computing \(\widehat{R}\) from \(\vlambda_t\) (denoted as \(\widehat{R}_{\vlambda}\)) is requires running multiple VI runs concurrently, which inconvinent and computationally expansive.
While~\citeauthor{NEURIPS2020_7cac11e2} makes the remark that the split-\(\widehat{R}\) diagnostic~\citep{gelman_bayesian_2014} can be used only a single sequence of \(\vlambda_t\) (by splitting the sequence in half and treating them two), we later show that this does not work well in practice.

In contrast, our PIMH kernel enables the use of convergence diagnostics only from a single VI run.
\begin{enumerate*}[label=\textbf{(\roman*)}]
  \item \textbf{(\(r\)-diagnostic)} First, since we apply the same IMH kernel (internally using \(q_{\vlambda}\)) \(N\) times, we obtain \(N\) unbiased estimates of the marginal rejection rate \(r\).
    As highlighted in Equation~\eqref{eq:rej_bound}, the marginal rejection rate provides a lower bound of the KL divergence.
  \item \textbf{(\(\widehat{R}_{\vz}\)-diagnostic)} Second, since we operate \(N\) parallel chains, we can use the \(\widehat{R}\) diagnostics on the sequence of \(\vz_t^{(i)}\) (denoted as \(\widehat{R}_{\vz}\)).
\end{enumerate*}
Unlike \(\widehat{R}_{\vlambda}\), which directly diagnoses the convergence of \(\vlambda\), \(\widehat{R}_{\vz}\) is more indirect; it diagnoses the convergence of \(\vz\), which does not necessary imply convergence of \(\vlambda\).
The convergence of \(\vz\) would imply that the stochastic gradients are reliable, which is convenient in the context of Polyak-Rupert averaging~\citep{ruppert_efficient_1988, polyak_acceleration_1992} suggested for VI by~\citet{NEURIPS2020_7cac11e2}.
In terms of usability, the \(r\)-diagnostic tend to be highly noisy, which complicates the use of arbitrary threshold.
On the contrary, \(\widehat{R}_{\vz}\) is relatively well understood, and has time-tested fixed thresholds for detecting convergence.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
