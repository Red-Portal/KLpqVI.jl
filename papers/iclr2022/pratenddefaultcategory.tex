
\prAtEndRestateii*
\label{proofsection:prAtEndii}\begin{proof}[Proof of \autoref{thm:prAtEndii}]\phantomsection\label{proof:prAtEndii}We employ a similar proof strategy with the works of~\citet [Theorem 4]{jiang_mcmc_2021}. \par Let us first denote the empirical distribution of the Markov-chain states at iteration \(t\) as \begin {align} \eta _{\mathrm {seq.},\, t}(\vz ) = \frac {1}{N} \sum _{i=1}^N K^{N (t-1) + i}(\vz _0, \vz ), \end {align} and consequently, \begin {align} g_{\mathrm {seq}, t}(\vlambda ) = \int s\left (\vz ; \vlambda \right ) \, \eta _{\mathrm {seq.},\, t}(\vz ) \, d\vz . \end {align} Now, \begin {align} \DTV { \eta _{seq.,\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} &= \DTV {\frac {1}{N} \sum _{i=1}^N K^{N (t-1) + i}(\vz _0, \cdot )}{p\left (\cdot \mid \vx \right )} \\ &\leq \frac {1}{N} \sum _{i=1}^N \DTV {K^{N (t-1) + i}(\vz _0, \cdot )}{p\left (\cdot \mid \vx \right )} &\text { (Triangle inequality)} \end {align} For an IMH kernel with \(w^* < \infty \), the geometric ergodicity of the IMH kernel \citep [Theorem 2.1]{10.2307/2242610} gives the bound \begin {align} \DTV {K^t(\vz _{0}, \cdot )}{p(\cdot \mid \vx )} \leq {\left (1 - \frac {1}{w^*}\right )}^t. \end {align} But, since in our case \(w^*\) is dependent on \(\vlambda \), our bound is given as \begin {align} \DTV {K^t(\vz _{0}, \cdot )}{p(\cdot \mid \vx )} \leq \prod _{\tau =1}^t {\left (1 - \frac {1}{w^*(\vlambda _{\tau })}\right )}. \end {align} Thus, \begin {align} \DTV { \eta _{\mathrm {seq.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} &\leq \prod _{\tau =1}^{t-1} {\left ( 1 - \frac {1}{w^*(\vlambda _{\tau })} \right )}^N \frac {1}{N} \sum _{i=1}^N {\left ( 1 - \frac {1}{w^*(\vlambda _{t})} \right )}^i \\ &\leq \prod _{\tau =1}^{t-1} {\left ( 1 - \frac {1}{w^*} \right )}^N \frac {1}{N} \sum _{i=1}^N {\left ( 1 - \frac {1}{w^*} \right )}^i \\ &\leq C^{N (t-1)} \frac {1}{N} \sum _{i=1}^N C^i \\ &\leq C^{N t} \end {align} where \(C = 1 - \frac {1}{w^*} \). \par Finally, by the definition of the total-variation distance, \begin {align} \mathrm {bias}\left [ g_{\mathrm {seq., t}} \right ] &\leq \DTV {\eta _{seq.,\, t}(\cdot )}{p(\cdot \mid \vx )} \\ &\leq \sup _{h : \mathcal {Z} \rightarrow \left [ \text {-}\nicefrac {L}{2}, \nicefrac {L}{2} \right ]} \left |\, \Esub {\eta _{\mathrm {seq.},\, t}(\cdot )}{h} - \Esub {p(\cdot \mid \vx )}{h} \,\right | \\ &= L \DTV { \eta _{\mathrm {seq.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} \\ &\leq L \, C_1^{N t}. \end {align}\end{proof}
\prAtEndRestateiii*
\label{proofsection:prAtEndiii}\begin{proof}[Proof of \autoref{thm:prAtEndiii}]\phantomsection\label{proof:prAtEndiii}We denote the empirical distribution of the Markov-chain states at iteration \(t\) as \begin {align} \eta _{\mathrm {par.},\, t}(\vz ) = \frac {1}{N} \sum _{i=1}^N K^{t}\left (\vz _0^{(i)}, \vz \right ). \end {align} and consequently, \begin {align} g_{\mathrm {par.}, t}(\vlambda ) = \int s\left (\vz ; \vlambda \right ) \, \eta _{\mathrm {par.},\, t}(\vz ) \, d\vz . \end {align} Similarly with~\cref {thm:bias_seq}, \begin {align} \DTV { \eta _{\mathrm {par.},\, t}(\vz ) }{p\left (\cdot \mid \vx \right )} &= \DTV {\frac {1}{N} \sum _{i=1}^N K^{t}\left (\vz _0^{(i)}, \vz \right )}{p\left (\cdot \mid \vx \right )} \\ &\leq \frac {1}{N} \sum _{i=1}^N \DTV {K^t(\vz _0^{(i)}, \cdot )}{p\left (\cdot \mid \vx \right )} &\text {(Triangle inequality)} \\ &\leq \DTV {K^t(\vz _0, \cdot )}{p\left (\cdot \mid \vx \right )} &\text {(Independence)} \\ &\leq \prod _{\tau =1}^t {\left (1 - \frac {1}{w^*(\vlambda _{\tau })}\right )} \\ &\leq \prod _{\tau =1}^t {\left (1 - \frac {1}{w^*}\right )} \\ &\leq C^t \end {align} where \(w^*(\vlambda _{\tau }) = \sup _{\vz } \nicefrac {p\left (\vz \mid \vx \right )}{q_{\vlambda _{\tau }}\left (\vz \right )} \) and \(C = 1 - \nicefrac {1}{w^*}\). And, finally the bias is given as \begin {align} \mathrm {bias}\left [ g_{\mathrm {par., t}} \right ] &\leq L \DTV {\eta _{\mathrm {par.},\, t}(\cdot )}{p(\cdot \mid \vx )} \\ &\leq L \sup _{h : \mathcal {Z} \rightarrow \left [ \text {-}\nicefrac {L}{2}, \nicefrac {L}{2} \right ]} \left |\, \Esub {\eta _{\mathrm {par.},\, t}(\cdot )}{h} - \Esub {p(\cdot \mid \vx )}{h} \,\right | \\ &= L \DTV { \eta _{\mathrm {par.},\, t}(\cdot ) }{p\left (\cdot \mid \vx \right )} \\ &\leq L \, C^t. \end {align}\end{proof}
\prAtEndRestateiv*
\label{proofsection:prAtEndiv}\begin{proof}[Proof of \autoref{thm:prAtEndiv}]\phantomsection\label{proof:prAtEndiv}\begin {align} \V {g_{\mathrm {seq., t}}} &= \V { \frac {1}{N} \sum _{i=1}^{N} f\left (\vz _{T + i}\right ) } \\ &= \frac {1}{N^2} \sum _{i=1}^{N} \V { f\left (\vz _{T+i}\right ) } + \frac {1}{N^2} \sum _{i=1,\, i \leq j}^{N} \Cov { f\left (\vz _{T+i}\right ), f\left (\vz _{T+j}\right ) } \\ &= \frac {1}{N} \V { f\left (\vz \right ) } + \frac {1}{N^2} \sum _{i=1,\, i \leq j}^{N} \Cov { f\left (\vz _{T+i}\right ), f\left (\vz _{T+j}\right ) } &\text {(By stationarity)} \\ \end {align}\end{proof}
\prAtEndRestatev*
\label{proofsection:prAtEndv}\begin{proof}[Proof of \autoref{thm:prAtEndv}]\phantomsection\label{proof:prAtEndv}By the law of total variance, \begin {align} \Vsub {q_{\vlambda }}{g_{\mathrm {single}}} \geq \Vsub {q_{\vlambda }}{g_{\mathrm {single}} \mid \vz _{t-1}}. \end {align} \par Write \begin {align} \mathbb {V}_{q_\lambda }[f|\vz _{t-1}] = \mathbb {V}_{q_\lambda }\left [\frac {\sum _{i=1}^N w(\vz ^{(i)})}{\sum _{i=0}^N w(\vz ^{(i)})} \frac {\sum _{i=1}^N w(\vz ^{(i)})f(\vz ^{(i)})}{\sum _{i=1}^N w(\vz ^{(i)})} + \left .\frac {w(\vz _{t-1})}{\sum _{i=0}^N w(\vz ^{(i)})} f(\vz _{t-1}) \right \vert \vz _{t-1} \right ]. \end {align} Note that if $a>0$, then we can approximate the function $\sum _{i=1}^N x_i/(a+\sum _{i=1}^N x_i)$ using the first-order Taylor series expansion about $(Z,\dots ,Z)$ by $$ \frac {\sum _{i=1}^N x_i}{a+\sum _{i=1}^N x_i} \approx \frac {NZ}{a+NZ}+\sum _{i=1}^N \frac {a}{(a+NZ)^2} (x_i -Z). $$ Hence, given $\vz _{t-1}$, we approximate $\sum _{i=1}^N w(\vz ^{(i)})/\sum _{i=0}^N w(\vz ^{(i)})$ by \begin {align} \frac {\sum _{i=1}^N w(\vz ^{(i)})}{\sum _{i=0}^N w(\vz ^{(i)})} &\approx \frac {NZ}{w(\vz _{t-1})+NZ} + \sum _{i=1}^N \frac {w(\vz _{t-1})}{(w(\vz _{t-1})+ NZ)^2} (w(\vz ^{(i)})-Z) \\ & = \frac {N^2 Z^2 + w(\vz _{t-1}) \sum _{i=1}^N w(\vz ^{(i)})}{(w(\vz _{t-1})+NZ)^2} \end {align} so that \begin {align} \mathbb {V}_{q_\lambda }[f|\vz _{t-1}] \approx \mathbb {V}_{q_\lambda } \left [\frac {N^2Z^2}{(w(\vz _{t-1})+NZ)^2} f_{IS} + \frac {w(\vz _{t-1})}{(w(\vz _{t-1})+NZ)^2}\sum _{i=1}^N w(\vz ^{(i)})f(\vz ^{(i)})\right . \\ \left . + \left . \frac {w(\vz _{t-1})}{\sum _{i=0}^N w(\vz ^{(i)})} f(\vz _{t-1}) \right \vert \vz _{t-1} \right ]. \end {align} Observe that $\sum _{i=1}^N w(\vz ^{(i)}) f(\vz ^{(i)}) = O(N)$ since $\{\vz ^{(1)}, \dots , \vz ^{(N)}\}$ are independent and identically distributed and that $w(\vz _{t-1})f(\vz _{t-1})/\sum _{i=0}^N w(\vz ^{(i)})=o(N)$. Combining these, we obtain \begin {align} \Vsub {q_{\vlambda }}{ f \mid \vz _{t-1}, \vz ^{(1:N)} } \approx \frac {N^4 Z^4}{(w(\vz _{t-1}) + NZ)^4} \Vsub {q_{\vlambda }}{ f_{\text {IS}} \,\middle \vert \, \vz _{t-1} }, \end {align} as was to be shown.\end{proof}
\prAtEndRestatevi*
\label{proofsection:prAtEndvi}\begin{proof}[Proof of \autoref{thm:prAtEndvi}]\phantomsection\label{proof:prAtEndvi}The rejection rate \(r(\vz _{t-1})\) is given by \begin {align} r(\vz _{t-1}) &= \Esub {q_{\vlambda }}{ \frac {w(\vz _{t-1})}{ \sum ^{N}_{k=1} w(\vz ^{(k)}) + w(\vz _{t-1})} } \\ &= \Esub {q_{\vlambda }}{ {\left ( \frac {\sum ^{N}_{k=1} w(\vz ^{(k)}) }{ w(\vz _{t-1}) } + 1 \right )}^{-1} }. \\ \intertext {At this point, we apply Jensen's inequality subject to the convex function \(f(x) = \nicefrac {1}{(1 + x)}\),} &\geq \frac {1}{1 + \Esub {q_{\vlambda }}{ \frac {\sum ^{N}_{k=1} w(\vz ^{(k)})}{w(\vz _{t-1})} } } \\ &= \frac {1}{1 + \frac {1}{w(\vz _{t-1})} \Esub {q_{\vlambda }}{ \sum ^{N}_{k=1} w(\vz ^{(k)}) } }. \\ \intertext {From the independence of the \(N\) proposals, we obtain} &= \frac {1}{1 + \frac {1}{w(\vz _{t-1})} N \Esub {q_{\vlambda }}{ w(\vz ) } } \\ &= \frac {1}{1 + \frac {1}{w(\vz _{t-1})}\,N\,Z }.\end {align}\end{proof}
\prAtEndRestatevii*
\label{proofsection:prAtEndvii}\begin{proof}[Proof of \autoref{thm:prAtEndvii}]\phantomsection\label{proof:prAtEndvii}We first show a simple Lemma that relates the rejection weight \(w\,(\vz _{t-1})\) with the KL divergence. \begin {framedlemma}\label {thm:rej_kl_bound} The average unnormalized weight of the rejection states is bounded below by the KL divergence such as \[ Z\,\exp \big (\DKL {p}{q_{\vlambda }}\big ) \leq \Esub {p(\vz _{t-1}\mid \vx )}{w\,(\vz _{t-1})}. \] \begin {proof} By the definition of the inclusive KL divergence, \begin {align} \DKL {p}{q_{\vlambda }} = \int p\,(\vz \mid \vx ) \log \frac {p\,(\vz \mid \vx )}{q_{\vlambda }(\vz )} \, d\vz &\leq \log \Esub {p(\vz \mid \vx )}{\frac {p\,(\vz \mid \vx )}{q_{\vlambda }(\vz )}} \\ &= \log \Esub {p(\vz \mid \vx )}{ \frac {w\,(\vz )}{Z} } \label {eq:weight_bound_kl} \end {align} where the right-hand side follows from Jensen's inequality. By a simple change of notation, we relate~\eqref {eq:weight_bound_kl} with the rejection states \(\vz _{t-1}\) such as \begin {align} \DKL {p}{q_{\vlambda }} &\leq \log \Esub {p(\vz _{t-1}\mid \vx )}{\frac {w\,(\vz _{t-1})}{Z}}. \end {align} Then, \begin {align} \exp \big (\DKL {p}{q_{\vlambda }}\big ) &\leq \Esub {p(\vz _{t-1}\mid \vx )}{ \frac {w\,(\vz _{t-1})}{Z}} \\ Z \exp \big (\DKL {p}{q_{\vlambda }}\big ) &\leq \Esub {p(\vz _{t-1}\mid \vx )}{w\,(\vz _{t-1})}. \end {align} \end {proof} \end {framedlemma} Now, from the result of~\cref {thm:cis_bound}, \begin {equation} \Esub {p(\vz _{t-1}\mid \vx )}{ r(\vz _{t-1}) } \geq \Esub {p(\vz _{t-1}\mid \vx )}{ \frac {w(\vz _{t-1})}{w(\vz _{t-1}) + N\,Z}} = \Esub {p(\vz _{t-1}\mid \vx )}{ \varphi \left ( w(\vz _{t-1}) \right ) },\label {eq:mean_bound} \end {equation} where \(\varphi (x) = \nicefrac {x}{(x + N\,Z)}\). The lower bound has the following relationship \begin {equation} \varphi \left (\, \Esub {p(\vz _{t-1}\mid \vx )}{ w(\vz _{t-1}) } \,\right ) \geq \Esub {p(\vz _{t-1}\mid \vx )}{ \varphi \,\left ( w\,(\vz _{t-1}) \right ) } \end {equation} by the concavity of \(\varphi \) and Jensen's inequality. From this, we denote the \textit {Jensen gap} \begin {equation} \delta = \varphi \left (\, \Esub {p(\vz _{t-1}\mid \vx )}{ w(\vz _{t-1}) } \,\right ) - \Esub {p(\vz _{t-1}\mid \vx )}{ \varphi \,\big ( w(\vz _{t-1}) \big ) },\label {eq:jensen_gap} \end {equation} where \(\delta \geq 0\). Then, by applying~\eqref {eq:jensen_gap} to~\eqref {eq:mean_bound}, \begin {align} \Esub {p(\vz _{t-1}\mid \vx )}{ r(\vz _{t-1}) } &\geq \Esub {p(\vz _{t-1}\mid \vx )}{ \varphi \,\big ( w\,(\vz _{t-1}) \big ) } \\ &= \varphi \left (\, \Esub {p(\vz _{t-1}\mid \vx )}{ w(\vz _{t-1}) } \,\right ) - \delta , \\ \intertext {and by the monotonicity of \(\varphi \) and~\cref {thm:rej_kl_bound},} &\geq \varphi \left (\, Z\,\exp \big (\DKL {p}{q_{\vlambda }}\big ) \,\right ) - \delta \\ &= \frac { Z\,\exp \big (\DKL {p}{q_{\vlambda }}\big ) }{ Z\,\exp \big (\DKL {p}{q_{\vlambda }}\big ) + N\,Z } - \delta \\ &= \frac { \exp \big (\DKL {p}{q_{\vlambda }}\big ) }{ \exp \big (\DKL {p}{q_{\vlambda }}\big ) + N } - \delta \\ &= \frac {1}{ 1 + \frac {N}{ \exp \big (\DKL {p}{q_{\vlambda }}\big ) } } - \delta . \end {align} Now we discuss the Jensen gap \(\delta \), which directly gives the sharpness of our lower bound. \citet [Theorem 1]{liao_sharpening_2019} have shown that, for a random variable \(X\) satisfying \(P(X \in (a, b))=1\), where \(-\infty \leq a < b \leq \infty \), and a differentiable function \(\widetilde {\varphi }\,(x)\), the following inequality holds: \begin {align} \inf _{x \in (a,b)} h\,(x; \mu ) \, \sigma ^2 \leq \E {\widetilde {\varphi }\,(X)} - \widetilde {\varphi }\,(\E {X}), \quad \text {where}\quad h\,(x; \nu ) = \frac {\widetilde {\varphi }\,(x) - \widetilde {\varphi }\,(\nu )}{{(x - \nu )}^2} - \frac { \widetilde {\varphi }\,\prime (\nu ) }{ x - \nu },\label {eq:jensen_gap_bound} \end {align} \(\mu \) and \(\sigma ^2\) are the mean and variance of \(X\), respectively. Also, \citet [Lemma 1]{liao_sharpening_2019} have shown that, if \(\widetilde {\varphi }\prime \,(x)\) is convex, then \(\inf _{x \in (a,b)} h\,(x; \mu ) = \lim _{x \rightarrow a} h\,(x; \mu ) \). \par In our case, the domain is \((a,b) = (0, \infty )\) since \(w\,(\vz _{t-1}) > 0\). Since \(\varphi \,\prime (x) = \nicefrac {N\,Z}{{(x + N\,Z)}^2}\) is convex, we have \begin {align} \lim _{x \rightarrow 0} h\,(x; \mu ) &= \lim _{x \rightarrow 0} \; \frac {1}{{(x - \mu )}^2} \left ( \varphi \,(x) - \varphi \,(\mu ) \right ) - \frac {1}{ x - \mu } \varphi \,\prime (\mu ) \\ &= \lim _{x \rightarrow 0} \frac {1}{{(x - \mu )}^2} \left ( \frac {x}{x + N\,Z} - \frac {\mu }{\mu + N\,Z} \right ) - \frac {1}{x - \mu } \left ( \frac {N\,Z}{ {(\mu + N\,Z)}^2 } \right ) \\ &= -\frac {1}{\mu ^2} \left ( \frac {\mu }{\mu + N\,Z} \right ) + \frac {1}{\mu } \left ( \frac {N\,Z}{{(\mu + N\,Z)}^2} \right ) \\ &= -\frac {1}{ \mu \, (\mu + N\,Z) } + \frac {N\,Z}{ \mu \, {(\mu + N\,Z)}^2} \\ &>- \frac {1}{\mu ^2}. \end {align} Notice that in the context of the original problem, \(\mu = \Esub {p(\vz _{t-1}\mid \vx )}{ w\,(\vz _{t-1}) }\). \par We finally discuss the variance term \(\sigma ^2\) in~\eqref {eq:jensen_gap_bound}. Since we assume \(\sup \nicefrac {p\,(\vz \mid \vx )}{q_{\vlambda }(\vz )} = M < \infty \), \( 0 < \frac {p\,(\vz \mid \vx )}{q_{\vlambda }(\vz )} < M \) for all \(\vz \in \mathcal {Z}\). Then, \begin {align} \sigma ^2 &= \E {w^2\,(\vz )} - {\E {w\,(\vz )}}^2 \\ &= \E { {\left ( \frac {Z\,p\,(\vz \mid \vx )}{q_{\vlambda }(\vz ) } \right )}^2 } - {\E {\frac {Z\,p\,(\vz \mid \vx )}{q_{\vlambda }(\vz ) }}}^2 \\ &= Z^2 \left (\; \E { {\left ( \frac {p\,(\vz \mid \vx )}{q_{\vlambda }(\vz ) } \right )}^2 } - {\E {\frac {p\,(\vz \mid \vx )}{q_{\vlambda }(\vz ) }}}^2 \;\right ) \\ &= Z^2 \, \V { \frac {p\,(\vz \mid \vx )}{q_{\vlambda }(\vz ) } } , \end {align} and by~\citet {bhatia_better_2000}'s inequality, \begin {equation} 0 \leq \sigma ^2 = Z^2 \, \V { \frac {p\,(\vz \mid \vx )}{q_{\vlambda }(\vz ) } } \leq Z^2 \, (M - \mu )\,\mu . \end {equation} \par By combining the results, we obtain \begin {align} 0 \leq \delta \leq -\inf _{x \in (a,b)} h\,(x; \mu ) \, \sigma ^2 = - \sigma ^2 \lim _{x \rightarrow 0} h\,(x; \mu ) < \frac {\sigma ^2}{\mu ^2} < \frac {Z^2 M }{\mu ^2} = \frac { Z^2 \, M } { {\Esub {p(\vz _{t-1}\mid \vx )}{ w\,(\vz _{t-1}) }}^2 }\;, \end {align} and by~\cref {thm:rej_kl_bound}, \begin {equation} 0 \leq \delta < \frac { M }{ \exp ^2\,\big ( \DKL {p}{q_{\vlambda }} \big ) }. \end {equation}\end{proof}
\prAtEndRestateviii*
\label{proofsection:prAtEndviii}\begin{proof}[Proof of \autoref{thm:prAtEndviii}]\phantomsection\label{proof:prAtEndviii}The rejection rate \(r(\vz _{t-1})\) is given by \begin {align} r\,(\vz _{t-1}) = 1 - \int \alpha \left (\vz , \vz _{t-1} \right ) \, q_{\vlambda }(\vz ) \, d\vz . \end {align} For an IMH sampler with the Metropolis-Hastings acceptance function and independent proposals, the rejection rate is bounded such that \begin {align} r\,(\vz _{t-1}) &= 1 - \int \min \left (\frac {w\,(\vz )}{w\,(\vz _{t-1})}, 1 \right ) \, q_{\vlambda }(\vz )\,d\vz \\ &= 1 - \frac {1}{w\,(\vz _{t-1})} \int \min \Big (w\,(\vz ), w\,(\vz _{t-1})\Big ) \, q_{\vlambda }(\vz )\,d\vz \\ &= 1 - \frac {1}{w\,(\vz _{t-1})} \int \min \left (\frac {p\,(\vz ,\vx )}{q_{\vlambda }(\vz )}, w\,(\vz _{t-1})\right ) \, q_{\vlambda }(\vz )\,d\vz \\ &= 1 - \frac {1}{w\,(\vz _{t-1})} \int \min \Big (p\,(\vz ,\vx ), w\,(\vz _{t-1})\,q_{\vlambda }(\vz )\Big ) \, d\vz \\ &\geq 1 - \frac {1}{w\,(\vz _{t-1})} \int p\,(\vz ,\vx ) \, d\vz \label {eq:imh_min_comp} \\ &= 1 - \frac {Z}{w\,(\vz _{t-1})} \end {align} The inequality in Equation~\eqref {eq:imh_min_comp} follows from \(\min \big (p\,(\vz ,\vx ), \,\cdot \, \big ) \leq p\,(\vz ,\vx ) \) for \(\forall \vz \in \mathcal {Z}\).\end{proof}
