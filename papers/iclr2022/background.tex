
\section{Background and Motivation}
\subsection{Inclusive Variational Inference Until Now}\label{section:ivi_previous}
A typical way to perform VI is to use stochastic gradient descent (SGD,~\citealt{robbins_stochastic_1951, bottou_online_1999}), which requires unbiased gradient estimates of the optimization target.
In the case of inclusive variational inference, this corresponds to estimating
%
\begin{align}
  \nabla_{\vlambda} \DKL{p}{q_{\lambda}}
  = \Esub{p(\vz\mid\vx)}{ - \nabla_{\vlambda} \log q_{\vlambda}(\vz) }
  = - \Esub{p(\vz\mid\vx)}{ s\,(\vz; \vlambda) }
\end{align}
where \(s\,(\vz; \vlambda) = \nabla_{\vlambda} \log q_{\vlambda}(\vz)\) is known as the \textit{score function}.
Evidently, estimating \(\nabla_{\vlambda} \DKL{p}{q_{\lambda}}\) requires integrating the score function over \(p(\vz\mid\vx)\), which is prohibitive.

\paragraph{Importance Sampling}
When it is easy to sample from the variational approximation \(q_{\lambda}(\vz)\), one can use importance sampling (IS, \citealt{robert_monte_2004, mcbook}) since 
\begin{align}
  \Esub{p(\vz\mid\vx)}{ s\,(\vz; \vlambda) }
  \propto \Esub{q_{\vlambda}}{ w\,(\vz) \, s\,(\vz; \vlambda) }
  \approx \frac{1}{N} \sum^{N}_{i=1} w\,(\vz^{(i)}) \, s\,(\vz^{(i)}; \vlambda)
\end{align}
where \(w\,(\vz) = p\,(\vz,\vx) / q_{\vlambda}(\vz)\) is known as the \textit{importance weight}, and \(\vz^{(1)},\, \ldots,\, \vz^{(N)}\) are \(N\) independent samples from \(q_{\vlambda}(\vz)\).
This scheme is equivalent to adaptive IS methods~\citep{cappe_adaptive_2008, bugallo_adaptive_2017} since the IS proposal \(q_{\vlambda}(\vz)\) is iteratively optimized based on the current samples.
Though IS is unbiased, it is highly unstable in practice.
A more stable alternative is to use the \textit{normalized weight} \(\widetilde{w}^{(i)} = \nicefrac{w\,(\vz^{(i)})}{\sum_{i=1}^N w\,(\vz^{(i)}) }\), which is known as the self-normalized IS (SNIS) approximation.
Unfortunately, SNIS still fails to converge even on moderate dimensional objectives and unlike IS, it is no longer unbiased~\citep{robert_monte_2004, mcbook}.

%
  %% \begin{minipage}[l]{0.45\linewidth}
  %%   \small
  %%   \centering
  %%   \begin{algorithm2e}[H]
  %%     \DontPrintSemicolon
  %%     \SetAlgoLined
  %%     \KwIn{MCMC kernel \(K(\vz,\cdot)\),
  %%       initial sample \(\vz_0\),
  %%       initial parameter \(\vlambda_0\),
  %%       number of iterations \(T\),
  %%       stepsize schedule \(\gamma_t\)}
  %%     \For{\textcolor{black}{\(t = 1, 2, \ldots, T\)}}{
  %%       \textit{Sample} \hspace{0.035in} \( \vz_{t} \sim K(\vz_{t-1}, \cdot) \)\;
  %%       \textit{Estimate} \( s(\vz; \vlambda) = \nabla_{\vlambda} \log q_{\vlambda}(\vz_t) \)\;
  %%       \textit{Update} \hspace{0.03in} \( \vlambda_{t} = \vlambda_{t-1} + \gamma_t\, s\,(\vz_t;\vlambda_{t-1}) \)\;
  %%     }
  %%     \caption{Markovian Score Climbing}\label{alg:msc}
  %%   \end{algorithm2e}
  %%   \vspace{-0.1in}
  %% \end{minipage}
  %% \qquad
  %% \begin{minipage}[r]{0.5\linewidth}
  %%   \vspace{-0.1in}
  %%   \begin{figure}[H]
  %%     \centering
  %%     \input{figures/trace.tex}
  %%     \caption{KL divergence and trace of \(\vz_t\) of MSC with a CIS kernel.
  %%       \(\vz_t\) barely moves until \(t=250\) around which \(\DKL{p}{q_{\vlambda}}\) starts to converge.}\label{fig:motivating}
  %%   \end{figure}
  %%   \vspace{-0.1in}
  %% \end{minipage}
%
\subsection{Stochastic Approximation with Markov-Chain Monte Carlo}\label{section:msc}
%
%\vspace{-0.1in}
\paragraph{MSC and JSA}
Recently,~\citeauthor{NEURIPS2020_b2070693} and~\citeauthor{pmlr-v124-ou20a} proposed two similar but independent methods for performing inclusive variational inference.
Both methods operate a Markov-chain in parallel with the VI optimization sequence, but the formulation is slightly different.
\citeauthor{NEURIPS2020_b2070693} proposed \textit{Markovian score climbing} (MSC) which operates a MCMC kernel leaving \(p\left(\vz \mid \vx \right)\) invariant.
Also, for the MCMC kernel, they propose conditional importance sampling (CIS), which is inspired by the particle MCMC method by~\cite{andrieu_particle_2010}.

On the other hand,~\cite{pmlr-v124-ou20a} propose \textit{joint stochastic approximation} (JSA) which operats a MCMC kernel leaving the likelihoods of the independent data points \(p\left(\vz_i \mid \vx_i \right)\) invariant.
For the MCMC kernel, unlike~\citeauthor{NEURIPS2020_b2070693}, they use the independence Metropolis Hastings (IMH) sampler.
Also, they operate multiple 


They showed that MSC achieves better and more robust performance compared to methods such as SNIS and expectation propagation (EP, \citealt{10.5555/2074022.2074067}).
MSC is described in~\cref{alg:msc}.
It obtains stochastic gradients from a Markov-chain \(\{\,\vz_1, \vz_2, \ldots \vz_T\,\}\) generated from a \(\pi\)-invariant transition operator \(K(\vz, \cdot)\) running in parallel with the VI optimizer (represented by the sequence \(\{\,\vlambda_1, \vlambda_2, \ldots, \vlambda_T\,\}\)).
\citeauthor{NEURIPS2020_b2070693} specifically proposed to use the \textit{conditional importance sampling} (CIS) kernel for MSC.

%If \(K(\vz, \cdot)\) is a proper Markov-chain Monte Carlo (MCMC) kernel,~\citeauthor{NEURIPS2020_b2070693} note that the gradient estimates become asymptotically unbiased with \(t \rightarrow \infty\).

%% \vspace{-0.1in}
%% \paragraph{A Motivating Mystery}
%% Even though~\citeauthor{NEURIPS2020_b2070693} suggested that \textit{any} good MCMC kernel would work, we present a counterexample showing that the CIS kernel may operate unusually when used for MSC.
%% In fact, \textit{MSC makes the most progress when the CIS-generated Markov-chain is the least effective}.
%% In~\cref{fig:motivating}, we show the trace plot of \(\vz_t\) (\textcolor{blue}{blue line}) and \(\DKL{p}{q_{\vlambda}}\) (\textcolor{red}{red line}).
%% Notice that \(\vz_t\) barely moves until iteration 250, around which \(\DKL{p}{q_{\vlambda}}\) starts to converge.
%% In a traditional accept-reject MCMC kernel view, this implies that most of the samples are rejected, making the kernel no longer statistically effective.
%% However, our analysis will show that this is actually a \textit{positive feature} of the CIS kernel, contributing to the empirical success of MSC.


%%% Local Variables:
%%% TeX-master: "master"
%%% End:
