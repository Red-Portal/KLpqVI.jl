
%\vspace{-0.05in}
\section{Inclusive Variational Inference with Independent Metropolis-Hastings}
%\vspace{-0.05in}
\subsection{Overview of Markov-Chain Monte Carlo Schemes}\label{section:msc_mcmc}
%\vspace{-0.05in}

\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.25\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures/diagram_1.png}
        \caption{Single mode}\label{fig:single}
    \end{subfigure}
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures/diagram_2.png}
        \caption{Sequential mode}\label{fig:seq}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[scale=0.25]{figures/diagram_3.png}
        \caption{Parallel mode (proposed)}\label{fig:par}
    \end{subfigure}
    \caption{Visualization of different ways of combining MCMC with stochastic approximation variational inference.
    The index \(t\) denotes the stochastic approximation iteration.
    The dark circles denote the MCMC samples used for estimating the score gradient for \(t=2\).
    }\label{fig:overview}
\end{figure*}
%
\input{table_cost}
%
First, the different modes of operating Markov-chains are illustrated in~\cref{fig:overview}.
The three different methods all have a similar computational cost, but their statistical performances are very different.
The computational costs of each scemes are organized in~\cref{table:cost}.
%
%\input{pseudocode}

\input{bias_seq}
\input{bias_par}
\input{var_seq}

\paragraph{Single Mode}
First, the ``single mode'' (see \cref{fig:single}) used by \citet{NEURIPS2020_b2070693} performs a ``single'' Markov-chain transition at each stochastic approximation step, and estimates the score using the single state such as \(g\left(\vlambda\right) = \nabla \log q_{\vlambda}\left(\vz_t\right)\).
The single state is generated by a computationally expensive MCMC kernel such as the CIS kernel proposed by~\citeauthor{NEURIPS2020_b2070693}.

\paragraph{Sequential Mode}
On the other hand, \citep{pmlr-v124-ou20a} perform \textit{multiple} ``sequental'' state transitions such that \(\{\vz_i, \vz_{i+1},\ldots, \vz_{i+K}\}\) at each stochastic approximation step.
The score is estimated by averaging the multiple intermediate states such that \(\frac{1}{K} \sum_{k=1}^K \nabla \log q_{\vlambda}\left(\vz_{t+k}\right)\).
When using the independent Metropolis Hastings (IMH) kernel, the computational cost is comparable to the single mode with the CIS kernel.
We call this scheme the ``sequential mode'' in contrast with the parallel mode described below.

\paragraph{Parallel Mode}
The ``parallel mode'' shown in~\cref{fig:par} is another possible scheme that has been underappreciated.
Unlike both the single and sequential modes that operate only a single Markov-chain, the parallel mode operates \textit{multiple chains} in parallel.
This scheme differs from the traditional way of running MCMC, but has multiple benefits for the specific purpose of VI.


\paragraph{MCMC uses marginal estimates.}
To explain the counterintuitive observation in~\cref{fig:motivating}, we first discuss a subtle difference between the usual MCMC setting and the MSC setting.
In MCMC, what matters is the estimate provided by the average of \textit{all} the states.
This \textit{marginal} estimate of \(f\) is 
\begin{align}
  \frac{1}{T} \sum^{T}_{t=1} f(\vz_t) %= \E{f(\vz_t)}
  = \E{\Esub{\vz_t \sim K(\vz_{t-1},\cdot)}{ f(\vz_t) \mid \vz_{t-1}}}
\end{align}
where \(\Esub{\vz_t \sim K(\vz_{t-1},\cdot)}{ f(\vz_t) \mid \vz_{t-1}}\) is the \textit{conditional estimate} of \(K(\vz_{t-1},\cdot)\) given the previous state \(\vz_{t-1}\).



\paragraph{MSC uses conditional estimates.}
On the other hand, in MSC, the stochastic gradient is estimated using a \textit{single} state of the Markov-chain denoted as \(\vz_t \sim K(\vz_{t-1}, \cdot)\).
This means that one needs to estimate the gradients using the conditional estimate such as
\begin{align}
  \nabla_{\lambda} \DKL{p}{q_{\lambda}} \approx \Esub{\vz_t \sim K(\vz_{t-1},\cdot)}{s\,(\vz_t;\, \vlambda) \mid \vz_{t-1}}.
\end{align}
This seemingly subtle difference of using either the marginal or conditional estimate reveals the following facts about MSC:
\begin{enumerate*}[label=(\roman*)]
\item The usual central limit theorem guarantees of MCMC do not apply to MSC.
\item In addition, ergodicity guarantees work differently since \(q_{\vlambda}(\vz)\) is updated at each iteration.
\end{enumerate*}
The most important difference is, however, the way we analyze the variance of the estimates.

\subsection{Interpreting Conditional Importance Sampling as Independent Metropolis-Hastings}\label{section:cis_imh}
Many popular MCMC kernels are based on the Metropolis-Hastings test where a random proposal \(\vz^*\) is either accepted into the Markov-chain (\(\vz_{t} = \vz^*\)) or rejected (\(\vz_t = \vz_{t-1}\)).
Among these, independent Metropolis-Hastings (IMH) kernels generate proposals independently of \(\vz_{t-1}\) such as \(\vz^* \sim q\,(\vz)\) instead of \(\vz^* \sim q\,(\vz\mid\vz_{t-1})\).% for \textit{state-dependent}.
%This constrasts with other kernels that generate  proposals such as .
We show that the CIS kernel proposed by~\citet{NEURIPS2020_b2070693} turns out to be a type of IMH kernel that uses Barker's acceptance ratio~\citep{barker_monte_1965} for the Metropolis-Hastings test.
This interpretation enables the analysis of the \textit{rejection rate} of the CIS kernel.

\begin{figure}[H]
  \vspace{-0.1in}
  \small
  \begin{algorithm2e}[H]
    \DontPrintSemicolon
    \SetAlgoLined
    \KwIn{previous sample \(\vz_{t-1}\),
      previous parameter \(\vlambda_{t-1}\),
      number of proposals \(N\)
    }
    \(\vz^{(0)} = \vz_{t-1}\) \;
    \textit{Propose} \(\;\vz^{(i)} \sim q_{\vlambda_{t-1}}(\vz)\) for \(i = 1, 2,\ldots, N\) \;
    \textit{Weight} \(\;w(\vz^{(i)}) = p(\vz^{(i)},\vx) \,/\, q_{\vlambda_{t-1}}(\vz^{(i)}) \) for \(i = 0, 1,\ldots, N\)\;
    \textit{Normalize} \(\;\widetilde{w}^{(i)} = \nicefrac{w(\vz^{(i)})}{ \sum^{N}_{i=0} w(\vz^{(i)}) }\) for \(i = 0, 1,\ldots, N\)\;
    \(\vz_{t} \sim \mathrm{Multinomial}(\widetilde{w}^{(0)}, \widetilde{w}^{(1)}, \ldots, \widetilde{w}^{(N)}) \)\;
    \caption{Conditional Importance Sampling}\label{alg:cis}
  \end{algorithm2e}
  \vspace{-0.2in}
\end{figure}
%
\vspace{-0.1in}
\paragraph{Conditional Importance Sampling}
A pseudocode of the CIS kernel is shown in~\cref{alg:cis}.
The original algorithmic description of CIS is to
\begin{enumerate*}[label=(\roman*)]
  \item obtain \(N\) samples from \(q_{\vlambda}(\vz)\),
  \item compute the importance weight including the previous Markov-chain state \(\vz_{t-1}\), and 
  \item resample \(\vz_{t-1}\) from the multinomial distribution of \(N+1\) proposals.
\end{enumerate*}
%
While particle MCMC~\citep{andrieu_particle_2010} originally inspired the CIS kernel, it is possible to find connections in multiple-try MCMC methods~\citep{martino_review_2018}.
In particular, the CIS kernel is identical to the previously proposed \textit{ensemble MCMC sampler}~\citep{austad_parallel_2007, neal_mcmc_2011a} with independent proposals, which is an instance of multiple-try MCMC~\citep[Table 12]{martino_review_2018}.

\vspace{-0.1in}
\paragraph{CIS as a Metropolis-Hastings Kernel}
Now we show that the CIS kernel is an accept-reject type kernel with Barker's acceptance ratio.
First, by defining \(\vz_t = \vz_{t-1}\) as ``reject'' and \(\vz_t \neq \vz_{t-1}\) as ``accept'', the CIS kernel can be understood as an accept-reject type kernel.
By denoting the \(N\) parallel proposals as an \textit{ensemble state} \(\vz^{(1:N)} = (\vz^{(1)}, \ldots, \vz^{(N)})\)~\citep{neal_mcmc_2011a}, the CIS kernel conditional estimate can be written as
%
{%\small
  \begin{align}
    \Esub{K(\vz_{t-1}, \vz_t)}{f(\vz_t)\mid\vz_{t-1}}  = 
    \Esub{q_{\vlambda}}{
      \alpha\,(\vz_{t-1}, \vz^{(1:N)}) \,
      \frac{
        \sum^{N}_{i=1} w\,(\vz^{(i)})\,f(\vz^{(i)})
      }
           {
             \sum^{N}_{i=1} w\,(\vz^{(i)})
           }
    }
    + r\,(\vz_{t-1}) \, f(\vz_{t-1})\label{eq:cis_kernel}
  \end{align}
}
where \(q_{\vlambda}(\vz^{(1:N)}) = \prod^N_{i=1} q_{\vlambda}(\vz^{(i)}) \),
the acceptance ratio
\(
  \alpha(\vz_{t-1}, \vz^{(1:N)})
  = \nicefrac{\sum^{N}_{i=1} w\,(\vz^{(i)})}{\sum^{N}_{i=0} w\,(\vz^{(i)})}
\)
is the probability of accepting the ensemble state \(\vz^{(1:N)}\), and
\begin{align}
  r\,(\vz_{t-1}) = \Esub{q_{\vlambda}(\vz^{(1:N)})}{
    r\,(\vz_{t-1} \mid \vz^{(1:N)})
  }
  = \Esub{q_{\vlambda}(\vz^{(1:N)})}{
    \left(1 - \alpha\,(\vz_{t-1}, \vz^{(1:N)})\right)
  }
\end{align}
is the probability of staying on \(\vz_{t-1}\) by rejecting \textit{any} ensemble state, \(r\,(\vz_{t-1}\mid \vz^{(1:N)})\) is the rejection rate given \(\vz^{(1:N)}\).
The expression of \(\alpha\,(\vz_{t-1}, \vz^{(1:N)})\) is known as Barker's acceptance ratio~\citep{barker_monte_1965}, which is a special case of the original Metropolis ratio~\citep{metropolis_equation_1953}.
(A detailed derivation is in the \textit{supplementary material}.)

%% Now, the transition kernel can be denoted as
%% \begin{align}
%%   K(\vz_{t-1}, \vz) = \int K(\vz^{(1:N)}, \vz) \, \big( 1 - r\,(\vz_{t-1}\mid\vz^{(1:N)}) \big) \, q_{\vlambda}( \vz^{(1:N)} ) \, d\vz^{(1:N)}
%%   + r\,(\vz_{t-1}) \,\delta_{\vz_{t-1}}(\vz)
%% \end{align}

\subsection{Bias-Variance Tradeoff of Conditional Importance Sampling}\label{section:bias_variance}
\paragraph{Variance of Conditional Importance Sampling}
The IMH (or accept-reject) view in~\cref{section:cis_imh} now enables us to discuss the rejection rate of the CIS kernel.
As discussed in~\cref{section:msc_mcmc}, MSC obtains gradients using the conditional estimates of MCMC.
The variance of the conditional estimate is closely related to the rejection rate such as
\begin{align}
  \Vsub{K(\vz_{t-1},\cdot)}{f \mid \vz_{t-1}} 
  &=\Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1},\,\vz^{(1:N)} } } + \underbrace{\Esub{q_{\vlambda}}{ \V{ f \mid \vz_{t-1},\,\vz^{(1:N)} } }}_{\text{Rao-Blackwellization gain}} \label{eq:total_variance} \\
  &\geq \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1},\,\vz^{(1:N)} } } \label{eq:rao_blackwell}\\
  &= \Vsub{q_{\vlambda}}{ \big(1 - r\,(\vz_{t-1}\mid\vz^{(1:N)})\big)\, f_{\mathrm{IS}}
    + r\,(\vz_{t-1}\mid\vz^{(1:N)})\,f(\vz_{t-1}) \;\middle\vert\; \vz_{t-1} } \label{eq:exact_variance} \\
  &\text{where}\;\; f_{\mathrm{IS}} = {\sum_{i=1}^N w\,(\vz^{(i)}) f(\vz^{(i)})\,/\,\sum_{i=1}^N w\,(\vz^{(i)})}. \nonumber 
\end{align}
%
The bound in~\eqref{eq:rao_blackwell} becomes exact if we use Rao-Blackwellization (that is, if we use the SNIS estimator \(\sum^{N}_{i=1} \widetilde{w}^{(i)}\,f\,(\vz^{(i)})\) instead of the resampled \(\vz_t\)) as mentioned by~\citet{NEURIPS2020_b2070693}.
The expansion in~\eqref{eq:total_variance} follows from the law of total variance where the right-hand term is the variance reduction we gain from using Rao-Blackwellization~\citep{bernton_locally_2015}, and the equality in~\eqref{eq:exact_variance} follows from~\eqref{eq:cis_kernel}.

\vspace{-0.1in}
\paragraph{Low rejection rate means high conditional variance.}
Because of the dependence of \(r\,(\vz_{t-1}\mid\vz^{(1:N)})\) on \(\vz^{(1:N)}\), it is in general difficult to interpret the result of~\eqref{eq:exact_variance}.
Nonetheless, when \(w\,(\vz_{t-1}) \gg w\,(\vz^{(i)})\), \(r(\vz\mid\vz^{(1:N)})\) is close to 1 almost independently of \(q_{\vlambda}(\vz)\).
The intuition is that if the \textit{rejection weight} \(w\,(\vz_{t-1})\) is large, most proposals will be rejected regardless of their values.
By translating this intuition into an approximation, we obtain the following result.
%
\input{variance_approx}
%
The statement of~\cref{thm:approx_var} is intuitive; if we reject all the states, there is no conditional variance.
However, this obvious fact becomes more interesting combined with the followings.

\vspace{-0.1in}
\paragraph{CIS has a high rejection rate until MSC converges.}
The following bound provides a condition for the rejection rate of a CIS kernel to be high.
%
\input{cis_bound}
%
When \(w\,(\vz_{t-1})\) is large, the lower bound of \(r(\vz_{t-1})\) becomes close to one.
In this case, according to~\cref{thm:approx_var}, the conditional variance becomes minimal.

In the context of VI, the following shows that \(r\,(\vz_{t-1})\) is huge when the KL divergence is large.
%
\input{cis_bound_kl}
%
This result states that in the ideal case when the Markov-chain has achieved stationarity and \(\vz_{t-1}\) closely follows \(p\,(\vz\mid\vx)\), the average rejection weight is bounded below exponentially by the KL divergence.
The bound is tight as long as \(\DKL{p}{q_{\vlambda}}\) is large.
In practical conditions, the rejection rate cannot be improved by increasing \(N\) since
\begin{enumerate*}[label=(\roman*)]
  \item the iteration complexity also increases, and
  \item \(w\,(\vz_t)\) can easily be larger by many orders of magnitude.
\end{enumerate*}

The results of \cref{thm:cis_bound_kl} signify that, until MSC converges such that \(\DKL{p}{q_{\vlambda}}\) is small, the rejection rate will be very high.
And by~\cref{thm:approx_var}, the variance will be small, improving the convergence of SGD.
This explains why the Markov-chain in~\cref{fig:motivating} does not move until MSC converges and why MSC works well with the CIS kernel.
Note that these properties hold similarly in any IMH type kernels.


\vspace{-0.1in}
\paragraph{Is bias guaranteed to decrease?}
The bias of the conditional estimate is closely related to the total variation (TV) distance \(\DTV{K(\vz_{t-1}, \cdot)}{p(\cdot\mid\vx)}\).
For bounded functions, the TV distance provides an upper bound of the bias.
Unfortunately, it is in general difficult to specify the TV distance (and hence the bias) with respect to \(p\,(\vz\mid\vx)\) and \(q_{\vlambda}(\vz)\).
Neverthless,~\citet{wang_exact_2020} recently showed that the rejection rate is related with the TV distance such that \(r\,(\vz_{t-1}) \leq \DTV{K(\vz_{t-1}, \cdot)}{p\,(\cdot\mid\vx)}\).
Therefore, a low rejection rate is \textit{necessary} for the bias to decrease.

\vspace{-0.1in}
\paragraph{Automatic Bias-Variance Tradeoff of IMH Type Kernels}
To summarize, IMH type kernels (including the CIS kernel) have an automatic bias-variance tradeoff mechanism.
In the initial steps where the inclusive KL divergence is large, variance is suppressed by rejecting most proposals.
However, as MSC converges, the bound on the rejection rate becomes loose, admitting a lower rejection rate, which enables bias to decrease.
This mechanism provides an interesting case where rejections in MCMC can actually be beneficial.
Lastly, we note that the automatic tradeoff mechanism is a unique property of IMH type kernels; it does not exist in kernels with state-dependent proposals such as random-walk Metropolis-Hastings or HMC.

%% that the KL divergence is related to the maximum importance ratio \(w^* = \sup_{\vz} p(\vz\mid\vx) / q_{\vlambda}(\vz)\) such that
%% \begin{align}
%%   \DKL{p}{q_{\vlambda}} = \int p(\vz\mid\vx) \log \frac{p(\vz\mid\vx)}{q_{\vlambda}(\vz)} d\vz < \int p(\vz\mid\vx)  \log w^* d\vz = \log w^*.
%% \end{align}
%% \(\DKL{p}{q_{\vlambda}} < \infty\) is thus a necessary condition for \(w < \infty\) which is required for \(K\) to be geometrically ergodic~\citep{wang_exact_2020}.
%% Also, 

%% the TV distance is bounded 
%% This implies 
%% This is not only 
%As shown by~\citet{10.1214/17-STS611, chatterjee_sample_2018}, the number of particles for acheiving bounded error reduces exponentially with the KL divergence.

\subsection{Reducing Variance with Parallel Independent Metropolis-Hastings Markov-Chains}\label{section:cis_bias}
Recall that the CIS kernel uses \(N\) multiple-try type proposals.
Thus, it is natural to expect the variance to decrease as the (per-transition) computational budget \(N\) increases.
However, under specific conditions, we find that the variance actually \textit{increases} with \(N\).

\vspace{-0.1in}
\paragraph{The variance of CIS can increase with \(N\).}
The bound in~\cref{thm:cis_bound} can be reinterpreted as a bound on the acceptance rate 
\begin{align}
  1 - r\,(\vz_{t-1}) \leq \frac{N\,Z}{ w\,(\vz_{t-1}) + N\,Z}\;,
\end{align}
which is, in general, very tight.
More importantly, when \(w\,(\vz_{t-1}) \gg N\,Z\), the acceptance rate grows like \(\mathcal{O}(N)\).
On the other hand, the variance of an SNIS estimator is known to decrease approximately at a rate of \(\mathcal{O}(1/N)\)~\citep{kong_sequential_1994, robert_monte_2004, elvira_rethinking_2018}.
That is, 
\begin{align}
  \Vsub{q_{\vlambda}}{ \E{ f \mid \vz_{t-1},\,\vz^{(1:N)} } } \approx \underbrace{{\big(1 - r\,(\vz_{t-1})\big)}^2}_{\text{approx.}\;\;\mathcal{O}(N^2)} \,
  \underbrace{\Vsub{q_{\vlambda}}{ f_{\text{IS}} }}_{\text{approx.}\;\;\mathcal{O}(1/N)}.\label{eq:cis_variance_incr}
\end{align}
Thus, when \(w\,(\vz_{t-1})\gg N\,Z\), the conditional variance of CIS approximately grows as \(\mathcal{O}(N)\).
We provide numerical simulations that support our analysis in the \textit{supplementary material}.

\vspace{-0.02in}
\input{pimh_algorithm}

\vspace{-0.1in}
\paragraph{Variance Reduction with Paralel IMH Chains}
To resolve the aforementioned limitation of the CIS kernel, we propose a simple but effective remedy: running \(N\) parallel IMH (PIMH) Markov-chains \( \big\{\,\vz_t^{(1)}\big\}, \big\{\,\vz_t^{(2)}\big\}, \ldots, \big\{\,\vz_t^{(N)}\big\}\) where each of the chains performs a Metropolis-Hastings test with only a \textit{single} proposal each.
The modified MSC algorithm incorporating parallel chains is shown in~\cref{alg:pimh} (the modified parts are highlighted in \textcolor{blue}{blue} and \textcolor{purple}{purple}), while the IMH kernel is described in~\cref{alg:imh}.
Since the parallel chains generate an \textit{independent} conditional estimate each, the gradient estimate \(\nicefrac{1}{N} \sum^{N}_{i=1} s\,(\vz_t^{(i)}; \vlambda)\) is an average of \(N\) independent and identical estimators.
This obviously reduces the variance of a single conditional estimate as \(\mathcal{O}(1/N)\).
In the best case, relative to CIS, PIMH will have a variance reduction close to \(\mathcal{O}(1/N^2)\).
%Also, the estimates are now proper marginal esimates, which enjoy a tighter bound on the TV distance (and hence the bias).
%
%\input{bias_reduction}

\vspace{-0.1in}
\paragraph{Lower bound of rejection rate in IMH}
IMH also enjoys variance control properties similar to the CIS kernel.
That is, a lower bound similar to~\cref{thm:cis_bound} can be shown.
%
\input{imh_bound}
%
Unlike CIS, we can see that the bound does not depend on \(N\).
This means the rejection rate will \textit{not} prematurely increase.
We thus obtain all the benefits of CIS except its limitations.
%Using~\cref{thm:imh_bound}, a similar result to~\cref{thm:cis_bound_kl} can be obtained.

\vspace{-0.1in}
\paragraph{Costs and Limitation of PIMH}
The computational cost of sampling \(\vz^{(i)}_{t}\) (\textcolor{blue}{blue} region in \cref{alg:pimh}) for \(N\) chains is equal to a CIS kernel with \(N\) proposals.
On the other hand, the cost of estimating the stochastic gradient (\textcolor{purple}{purple} region in \cref{alg:pimh}) is now \(\mathcal{O}(N)\) instead of \(\mathcal{O}(1)\) of the CIS kernel.
This cost is, however, also imposed on the CIS kernel if we use Rao-Blackwellization.
Thus, the overall computational cost of PIMH is more or less equal to that of the CIS kernel.
The only downside of PIMH is that the Metropolis-Hastings acceptance ratio that we use has a slightly larger acceptance rate than Barker's~\citep{peskun_optimum_1973, minh_understanding_2015}.
In the small \(N\) regime, PIMH has a slightly larger conditional variance, but it can be easily fixed by using Barker's ratio.

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
