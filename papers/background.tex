
\section{Background}
\vspace{-0.05in}
\subsection{Inclusive Variational Inference Until Now}\label{section:ivi_previous}
\vspace{-0.05in}
A typical way to perform VI is to use stochastic gradient descent (SGD,~\citealt{robbins_stochastic_1951, bottou_online_1999}), which requires unbiased gradient estimates of the optimization target.
In the case of inclusive variational inference, this corresponds to estimating
%
\begin{align}
  \nabla_{\vlambda} \DKL{p}{q_{\lambda}}
  = \Esub{p(\vz\mid\vx)}{ - \nabla_{\vlambda} \log q_{\vlambda}(\vz) }
  = - \Esub{p(\vz\mid\vx)}{ s\,(\vz; \vlambda) }
\end{align}
where \(s\,(\vz; \vlambda) = \nabla_{\vlambda} \log q_{\vlambda}(\vz)\) is known as the \textit{score function}.
Evidently, estimating \(\nabla_{\vlambda} \DKL{p}{q_{\lambda}}\) requires integrating the score function over \(p(\vz\mid\vx)\), which is prohibitive.

\vspace{-0.1in}
\paragraph{Importance Sampling}
When it is easy to sample from the variational approximation \(q_{\lambda}(\vz)\), one can use importance sampling (IS, \citealt{robert_monte_2004, mcbook}) since 
\begin{align}
  \Esub{p(\vz\mid\vx)}{ s\,(\vz; \vlambda) }
  \propto \Esub{q_{\vlambda}}{ w\,(\vz) \, s\,(\vz; \vlambda) }
  \approx \frac{1}{N} \sum^{N}_{i=1} w\,(\vz^{(i)}) \, s\,(\vz^{(i)}; \vlambda)
\end{align}
where \(w\,(\vz) = p\,(\vz,\vx) / q_{\vlambda}(\vz)\) is known as the \textit{importance weight}, and \(\vz^{(1)},\, \ldots,\, \vz^{(N)}\) are \(N\) independent samples from \(q_{\vlambda}(\vz)\).
This scheme is equivalent to adaptive IS methods~\citep{cappe_adaptive_2008, bugallo_adaptive_2017} since the IS proposal \(q_{\vlambda}(\vz)\) is iteratively optimized based on the current samples.
Though IS is unbiased, it is highly unstable in practice.
A more stable alternative is to use the \textit{normalized weight} \(\widetilde{w}^{(i)} = \nicefrac{w\,(\vz^{(i)})}{\sum_{i=1}^N w\,(\vz^{(i)}) }\), resulting in the self-normalized IS (SNIS) approximation.
Unfortunately, SNIS still fails to converge even on moderate dimensional objectives.
Furthermore, unlike IS, SNIS is no longer unbiased~\citep{robert_monte_2004, mcbook}.

\vspace{-0.1in}
%
  \begin{minipage}[l]{0.45\linewidth}
    \small
    \centering
    \begin{algorithm2e}[H]
      \DontPrintSemicolon
      \SetAlgoLined
      \KwIn{MCMC kernel \(K(\vz,\cdot)\),
        initial sample \(\vz_0\),
        initial parameter \(\vlambda_0\),
        number of iterations \(T\),
        stepsize schedule \(\gamma_t\)}
      \For{\textcolor{black}{\(t = 1, 2, \ldots, T\)}}{
        \textit{Sample} \hspace{0.035in} \( \vz_{t} \sim K(\vz_{t-1}, \cdot) \)\;
        \textit{Estimate} \( s(\vz; \vlambda) = \nabla_{\vlambda} \log q_{\vlambda}(\vz_t) \)\;
        \textit{Update} \hspace{0.03in} \( \vlambda_{t} = \vlambda_{t-1} + \gamma_t\, s\,(\vz_t;\vlambda_{t-1}) \)\;
      }
      \caption{Markovian Score Climbing}\label{alg:msc}
    \end{algorithm2e}
    \vspace{-0.1in}
  \end{minipage}
  \qquad
  \begin{minipage}[r]{0.5\linewidth}
    \vspace{-0.1in}
    \begin{figure}[H]
      \centering
      \input{figures/trace.tex}
      \caption{KL divergence and trace of \(\vz_t\) of MSC with a CIS kernel.
        \(\vz_t\) barely moves until \(t=300\) around which \(\DKL{p}{q_{\vlambda}}\) starts to converge.}\label{fig:motivating}
    \end{figure}
    \vspace{-0.1in}
  \end{minipage}
%
\subsection{Markovian Score Climbing}\label{section:msc}
%
%\vspace{-0.1in}
\paragraph{Description of MSC}
Very recently,~\citet{NEURIPS2020_b2070693} proposed \textit{Markovian score climbing} (MSC) for performing inclusive VI.
They showed that MSC achieves better and more robust performance compared to methods such as SNIS and expectation propagation (EP, \citealt{10.5555/2074022.2074067}).
MSC is described in~\cref{alg:msc}.
It obtains stochastic gradients from a Markov-chain \(\{\,\vz_1, \vz_2, \ldots \vz_T\,\}\) generated from a \(\pi\)-invariant transition operator \(K(\vz, \cdot)\) running in parallel with the VI optimizer (represented by the sequence \(\{\,\vlambda_1, \vlambda_2, \ldots, \vlambda_T\,\}\)).
\citeauthor{NEURIPS2020_b2070693} specifically proposed to use the \textit{conditional importance sampling} (CIS) kernel for MSC.

%If \(K(\vz, \cdot)\) is a proper Markov-chain Monte Carlo (MCMC) kernel,~\citeauthor{NEURIPS2020_b2070693} note that the gradient estimates become asymptotically unbiased with \(t \rightarrow \infty\).

\vspace{-0.1in}
\paragraph{A Motivating Mystery}
Even though~\citeauthor{NEURIPS2020_b2070693} suggested that \textit{any} good MCMC kernel would work, we present a counterexample showing that the CIS kernel may operate unusually when used for MSC.
In fact, \textit{MSC makes the most progress when the CIS-generated Markov-chain is the least effective}.
In~\cref{fig:motivating}, we show the trace plot of \(\vz_t\) (\textcolor{blue}{blue line}) and noisy estimates of \(\DKL{p}{q_{\vlambda}}\) (\textcolor{red}{red line}).
Notice that \(\vz_t\) barely moves until iteration 300, around which \(\DKL{p}{q_{\vlambda}}\) starts to converge.
In a traditional accept-reject MCMC kernel view, this implies that most of the samples are rejected, making the kernel no longer statistically effective.
However, our analysis will show that this is actually a \textit{positive feature} of the CIS kernel, contributing to the empirical success of MSC.

%Our work investigates this phenomenon
%We later show that this phenomenon is actually 
%Thus, until we achieve convergence, the samples from \(K(\vz_{t-1}, \cdot)\) are the most biased towards the previous state \(\vz_{t-1}\).
%In this work, we show that this counter-intuitive behavior of the CIS kernel is what makes MSC work well in practice.

%

%%% Local Variables:
%%% TeX-master: "master"
%%% End:
